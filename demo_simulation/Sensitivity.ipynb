{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c300e7-1e3e-4e87-985b-faea868c6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from collections import OrderedDict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##################################################################\n",
    "#                       Data preprocessing                       #\n",
    "##################################################################\n",
    "def sim_data(n, dim, Type):\n",
    "\tif Type == 'A':\n",
    "\t\tX = torch.rand((n,2))\n",
    "\t\ty = torch.exp(2*torch.sin(X[:,0]*0.5*torch.pi)+ 0.5*torch.cos(X[:,1]*2.5*torch.pi))\n",
    "\t\ty = y.reshape(-1,1)\n",
    "\t\ty = y.float()\n",
    "\t\t\n",
    "\telif Type == 'B':\n",
    "\t\tX = torch.rand((n, dim))\n",
    "\t\ty = 1\n",
    "\t\tfor d in range(dim):\n",
    "\t\t\ta = (d+1)/2\n",
    "\t\t\ty *= ((torch.abs(4*X[:,d]-2)+a)/(1+a))\n",
    "\t\ty = y.reshape(-1,1)\n",
    "\t\ty = y.float()\n",
    "\telse:\n",
    "\t\tpass\n",
    "\t\n",
    "\treturn X, y\n",
    "\n",
    "def diag_mat_weights(dimp, type = 'first'):\n",
    "\tif type == 'first':\n",
    "\t\tdg = np.zeros((dimp-1, dimp))\n",
    "\t\tfor i in range(dimp-1):\n",
    "\t\t\tdg[i,i] = -1\n",
    "\t\t\tdg[i,i+1]= 1\n",
    "\telif type == 'second':\n",
    "\t\tdg = np.zeros((dimp-2, dimp))\n",
    "\t\tfor i in range(dimp-2):\n",
    "\t\t\tdg[i,i] = -1\n",
    "\t\t\tdg[i,i+1]= 2\n",
    "\t\t\tdg[i,i+2]= -1\n",
    "\telse:\n",
    "\t\tpass\n",
    "\treturn torch.Tensor(dg)\n",
    "\n",
    "def num_para(model):\n",
    "\ttp = 0\n",
    "\tfor param in model.parameters():\n",
    "\t\ttp += param.numel()\n",
    "\treturn tp\n",
    "    \n",
    "##################################################################\n",
    "#                          DPS Modeling                          #\n",
    "##################################################################\n",
    "\n",
    "class BSL(nn.Module):\n",
    "    def __init__(self, degree, num_knots, num_neurons, bias = True):\n",
    "        super(BSL, self).__init__()\n",
    "        self.degree = degree\n",
    "        self.num_knots = num_knots\n",
    "        self.num_neurons = num_neurons\n",
    "        self.control_p = nn.Parameter(torch.randn(self.num_knots, self.num_neurons))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(self.num_neurons))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.inter = {}\n",
    "    \n",
    "    def basis_function(self, x, i, k, t):\n",
    "    \n",
    "        # Base case: degree 0 spline\n",
    "        if k == 0:\n",
    "            return ((t[i] <= x) & (x < t[i + 1])).float()\n",
    "    \n",
    "        # Recursive case\n",
    "        denom1 = t[i + k] - t[i]\n",
    "        denom2 = t[i + k + 1] - t[i + 1]\n",
    "    \n",
    "        term1 = 0\n",
    "        if denom1 != 0:\n",
    "            term1 = (x - t[i]) / denom1 * self.basis_function(x, i, k - 1, t)\n",
    "    \n",
    "        term2 = 0\n",
    "        if denom2 != 0:\n",
    "            term2 = (t[i + k + 1] - x) / denom2 * self.basis_function(x, i + 1, k - 1, t)\n",
    "    \n",
    "        return term1 + term2\n",
    "\n",
    "    def knots_distribution(self, dg, nk):\n",
    "\n",
    "        knots = torch.cat([torch.linspace(-0.002, -0.001, steps=dg),            # Add repeated values at the start for clamping\n",
    "            torch.linspace(0, 1, nk-2*dg-2),  # Uniform knot spacing in the middle\n",
    "            torch.linspace(1.001, 1.002, steps=dg)           # Add repeated values at the end for clamping\n",
    "            ]).view(-1,1)\n",
    "     \n",
    "        return knots\n",
    "    \n",
    "    def basis_function2(self, x, spl):\n",
    "        basis_output = spl.fit_transform(x.cpu().numpy())\n",
    "        return basis_output\n",
    "            \n",
    "    def forward(self, x):\n",
    "        batch_size, num_features = x.size()\n",
    "        device = x.device\n",
    "        \n",
    "        # Create knot vector and apply B-spline basis functions for each feature\n",
    "    \n",
    "        basises = []\n",
    "        knots = self.knots_distribution(self.degree, self.num_knots)\n",
    "        #knots = knots.to(device)\n",
    "        spl = SplineTransformer(n_knots=self.num_knots, degree=self.degree, knots = knots)\n",
    "\n",
    "        \n",
    "        for feature in range(num_features):\n",
    "            # Calculate B-spline basis functions for this feature\n",
    "            \n",
    "            basis = self.basis_function2(x[:, feature].reshape(-1,1), spl)\n",
    "            basis = torch.Tensor(basis).to(device)\n",
    "            basises.append(basis)\n",
    "        \n",
    "        self.inter['basic'] = torch.reshape(torch.stack(basises, dim = 1), (batch_size, self.num_knots * self.num_neurons)).T\n",
    "        basises = torch.stack(basises)\n",
    "        tout = basises.permute(1,2,0) * self.control_p\n",
    "        tout = tout.sum(dim =1)\n",
    "        if self.bias is not None:\n",
    "            tout += self.bias        \n",
    "            \n",
    "        return tout\n",
    "\n",
    "\n",
    "class NormLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NormLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        min_val = torch.min(x, axis = 1).values.reshape(-1,1)\n",
    "        max_val = torch.max(x, axis = 1).values.reshape(-1,1)\n",
    "\n",
    "        x = (x - min_val)/(max_val - min_val)  # Rescale to [0, 1]\n",
    "        return x.detach()\n",
    "        \n",
    "class BSpline_block(nn.Module):\n",
    "    def __init__(self, degree, num_knots, num_neurons, dropout = 0.0, bias = True):\n",
    "        super(BSpline_block, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(OrderedDict([\n",
    "            ('norm', NormLayer()),\n",
    "            ('BSL', BSL(degree = degree, num_knots = num_knots, num_neurons = num_neurons, bias = bias)),\n",
    "            ('drop', nn.Dropout(dropout)),\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "        \n",
    "class StackBS_block(nn.Module):\n",
    "    def __init__(self, block, degree, num_knots, num_neurons, num_blocks, dropout = 0.0, bias = True):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleDict({\n",
    "            f'block_{i}': block(degree = degree, num_knots = num_knots, num_neurons = num_neurons)\n",
    "            for i in range(num_blocks)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        for name, block in self.model.items():\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DPS(nn.Module):\n",
    "    def __init__(self, input_dim, degree, num_knots, num_neurons, num_bsl, dropout, output_dim, bias):\n",
    "        super(DPS, self).__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_knots = num_knots\n",
    "        self.ln1 = nn.Linear(input_dim, num_neurons)\n",
    "        self.Spline_block = StackBS_block(BSpline_block, degree = degree, num_knots = num_knots, num_neurons = num_neurons, num_blocks = num_bsl, dropout = dropout)\n",
    "        self.ln2 = nn.Linear(num_neurons, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.ln1(x)\n",
    "        spout = self.Spline_block(x)\n",
    "        output = self.ln2(spout)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def get_para_ecm(self, x):\n",
    "\n",
    "        '''\n",
    "        ecm_para: A dictionary that collects the parameter we need to the following ECM algorithm.\n",
    "        ecm_para.basic: Store the output of each B-Spline block; Dimension = [n_sample, n_neurons]\n",
    "        ecm_para.ebasic Store the weight matrix of each B-Spline expansion; Dimension = [n_knots * n_neurons, n_sample]\n",
    "\n",
    "        '''\n",
    "        ecm_para = {}\n",
    "        bs_block_out = {}\n",
    "        bs_spline_weight = {}\n",
    "        bs_spline_value = {}\n",
    "        bs_spline_bias = {}\n",
    "\n",
    "        _ = self(x)\n",
    "        \n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                bs_block_out[name] = output.detach()\n",
    "            return hook\n",
    "\n",
    "        handles = []\n",
    "        for name, layer in self.named_modules():\n",
    "            if 'block.drop' in name:\n",
    "                handles.append(layer.register_forward_hook(get_activation(name)))\n",
    "            elif 'block.BSL' in name:\n",
    "                bs_spline_value[name] = layer.inter['basic'].detach()\n",
    "                bs_spline_weight[name] = layer.control_p.detach()\n",
    "                bs_spline_bias[name] = layer.bias.detach()\n",
    "        # Run forward pass (triggers hooks)\n",
    "        _ = self(x)\n",
    "\n",
    "        # Clean up hooks\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "            \n",
    "        ecm_para['basic'] = torch.stack(list(bs_block_out.values()), dim=0)\n",
    "        ecm_para['ebasic'] = torch.stack(list(bs_spline_value.values()), dim=0)\n",
    "        ecm_para['wbasic'] = torch.stack(list(bs_spline_weight.values()), dim=0)\n",
    "        ecm_para['bbasic'] = torch.stack(list(bs_spline_bias.values()), dim=0)\n",
    "        del bs_block_out, bs_spline_weight, bs_spline_value, bs_spline_bias, _\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return ecm_para\n",
    "\n",
    "    def fit(self, x):\n",
    "        return 0\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss  # because we want to minimize val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "        \n",
    "def train_one_epoch(model, x, y, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "                \n",
    "def validate(model, x, y, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    return loss\n",
    "    \n",
    "##################################################################\n",
    "#                          ECM Iteration                         #\n",
    "##################################################################\n",
    "\n",
    "def ECM(par, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4):\n",
    "    lambdab = initial_lambda\n",
    "    sigma = initial_sigma\n",
    "    xi = initial_xi\n",
    "    \n",
    "    n_block, num_knots, num_neurons = par['wbasic'].size()\n",
    "    ls_lambda = torch.empty(n_block)\n",
    "    \n",
    "    for l in range(n_block):\n",
    "        B = par['ebasic'][l]\n",
    "        By = par['basic'][l]\n",
    "        WB = par['wbasic'][l]\n",
    "        \n",
    "        DB = diag_mat_weights(WB.size()[0]).to(device)\n",
    "        size = B.size()[1]\n",
    "        S = DB.T @ DB\n",
    "        Cov_a = (xi**2)* torch.linalg.pinv(S)\n",
    "        Cov_a.to(device)\n",
    "        Cov_e = (torch.eye(size*num_neurons)* sigma).to(device)\n",
    "        \n",
    "        block_y = torch.reshape(By, (-1,1))\n",
    "        flatB = B.view(num_neurons, num_knots, size)\n",
    "            \n",
    "        sqr_xi= 0\n",
    "        sqr_sig = 0\n",
    "\n",
    "        for i in range(num_neurons):\n",
    "            Ncov = (Cov_a -(Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)]) @ flatB[i].T @ Cov_a))\n",
    "            Nmu = (Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)])) @ By[:,i].reshape(-1,1)\n",
    "            \n",
    "            first_xi = S @ Ncov\n",
    "            second_xi = (Nmu.T @ S @ Nmu)\n",
    "            sqr_xi += torch.trace(first_xi) + second_xi\n",
    "                \n",
    "            first_sig = torch.norm(By[:,i])\n",
    "            second_sig = 2 * (By[:,i] @ flatB[i].T) @ Nmu \n",
    "            third_sig = torch.trace((flatB[i] @ flatB[i].T) @ Ncov)\n",
    "            four_sig = (Nmu.T @ flatB[i] @ flatB[i].T @ Nmu)\n",
    "            \n",
    "            sqr_sig += (first_sig + second_sig + third_sig + four_sig)\n",
    "            \n",
    "            del first_xi, second_xi, first_sig, second_sig, third_sig, four_sig\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        sqr_xi /= num_neurons\n",
    "        sqr_sig /= (num_neurons*size)\n",
    "\n",
    "        ls_lambda[l] = (sqr_sig/sqr_xi).item()\n",
    "        \n",
    "        del Cov_a, Cov_e, flatB\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return ls_lambda\n",
    "    \n",
    "def ECM_layersise_update(model, par, Lambda, x, y):\n",
    "\n",
    "    model.eval()\n",
    "    device = x.device\n",
    "    \n",
    "    B_out, B_in, B_w, B_b = par['basic'], par['ebasic'], par['wbasic'], par['bbasic']\n",
    "    n_layer, nk, nm = B_w.size()\n",
    "    DB = diag_mat_weights(B_w[0].size()[0], 'second').to(device)\n",
    "\n",
    "    Project_matrix = (torch.linalg.pinv(B_in[-1].T @ B_in[-1]) @ B_in[-1].T @ B_in[-1])\n",
    "    Size = [b.size()[1] for b in B_in]\n",
    "\n",
    "    B_in = B_in.view(n_layer, nm, nk, Size[0])\n",
    "\n",
    "    for l in range(n_layer):    \n",
    "        NW = torch.empty((nk, nm)).to(device)\n",
    "        NB = torch.empty((nm)).to(device)\n",
    "        \n",
    "        for i in range(nm):\n",
    "            B1y = B_out[l][:,i] - B_b[l][i]\n",
    "            BB = B_in[l][i].T\n",
    "    \n",
    "            # Update the weights and bias\n",
    "            NW[:, i] = (torch.inverse(BB.T @ BB + (Lambda[l]/Size[l]) * (DB.T @ DB)) @ BB.T @ B1y)\n",
    "            NB[i] = torch.mean(B_out[l][:,i] - (NW[:,i] @ BB.T))\n",
    "                \n",
    "        # update the weight\n",
    "        block = getattr(model.Spline_block.model, f'block_{l}')\n",
    "        getattr(block.block.BSL, 'control_p').data = NW\n",
    "        getattr(block.block.BSL, 'bias').data = NB\n",
    "\n",
    "        del NW, NB, B1y, BB, block\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        DPSy = model(x)\n",
    "        Update_Train_Loss = np.round(criterion(y, DPSy.detach()).item(), 5)\n",
    "        GCV = np.round((torch.norm(y - DPSy)/(Size[-1]-torch.trace(Project_matrix))).item(), 5)\n",
    "    \n",
    "    return model, GCV\n",
    "\n",
    "def ECM_update(model, max_iter, x, y):\n",
    "    BestGCV = prev = 9999\n",
    "    patient = 10\n",
    "    pcount = 0\n",
    "    for i in range(max_iter):\n",
    "        _ = model(X_train)\n",
    "        ECM_para = model.get_para_ecm(x)\n",
    "        ECM_Lambda = ECM(ECM_para, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4)\n",
    "\n",
    "        model, GCV = ECM_layersise_update(model, ECM_para, ECM_Lambda, x, y)\n",
    "\n",
    "        print(np.abs(prev - GCV))\n",
    "        if np.abs(prev - GCV) < 1e-5:\n",
    "            Iter = i+1\n",
    "            print('GCV Converge at ',i+1,' iteration')\n",
    "            break\n",
    "            \n",
    "        if GCV < BestGCV:\n",
    "            BestLambda = ECM_Lambda\n",
    "            BestGCV = GCV\n",
    "            pcount = 0\n",
    "        else:\n",
    "            pcount += 1\n",
    "\n",
    "        if pcount == patient:\n",
    "            Iter = i+1\n",
    "            print('GCV Converge at ',i,' iteration')\n",
    "            break\n",
    "\n",
    "        prev = GCV\n",
    "        Iter = i+1\n",
    "        \n",
    "        del ECM_para, ECM_Lambda\n",
    "    \n",
    "    return BestLambda, Iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a8aafe-74db-4fc5-a7f2-295f38adc51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  1\n",
      "Epoch 01 | Train Loss: 29.6453 | Val Loss: 27.0359\n",
      "Epoch 11 | Train Loss: 4.5901 | Val Loss: 5.1676\n",
      "Epoch 21 | Train Loss: 1.2740 | Val Loss: 1.2008\n",
      "Epoch 31 | Train Loss: 0.8275 | Val Loss: 0.7424\n",
      "Epoch 41 | Train Loss: 0.5851 | Val Loss: 0.5640\n",
      "Epoch 51 | Train Loss: 0.4700 | Val Loss: 0.4598\n",
      "Epoch 61 | Train Loss: 0.3937 | Val Loss: 0.3816\n",
      "Epoch 71 | Train Loss: 0.3036 | Val Loss: 0.2974\n",
      "Epoch 81 | Train Loss: 0.2408 | Val Loss: 0.2425\n",
      "Epoch 91 | Train Loss: 0.1938 | Val Loss: 0.2024\n",
      "Epoch 101 | Train Loss: 0.1646 | Val Loss: 0.1765\n",
      "Epoch 111 | Train Loss: 0.1475 | Val Loss: 0.1608\n",
      "Epoch 121 | Train Loss: 0.1372 | Val Loss: 0.1511\n",
      "Epoch 131 | Train Loss: 0.1301 | Val Loss: 0.1446\n",
      "Epoch 141 | Train Loss: 0.1245 | Val Loss: 0.1397\n",
      "Epoch 151 | Train Loss: 0.1198 | Val Loss: 0.1357\n",
      "Epoch 161 | Train Loss: 0.1156 | Val Loss: 0.1320\n",
      "Epoch 171 | Train Loss: 0.1119 | Val Loss: 0.1287\n",
      "Epoch 181 | Train Loss: 0.1086 | Val Loss: 0.1256\n",
      "Epoch 191 | Train Loss: 0.1056 | Val Loss: 0.1228\n",
      "Epoch 201 | Train Loss: 0.1028 | Val Loss: 0.1202\n",
      "Epoch 211 | Train Loss: 0.1003 | Val Loss: 0.1178\n",
      "Epoch 221 | Train Loss: 0.0979 | Val Loss: 0.1155\n",
      "Epoch 231 | Train Loss: 0.0957 | Val Loss: 0.1133\n",
      "Epoch 241 | Train Loss: 0.0936 | Val Loss: 0.1112\n",
      "Epoch 251 | Train Loss: 0.0916 | Val Loss: 0.1091\n",
      "Epoch 261 | Train Loss: 0.0897 | Val Loss: 0.1071\n",
      "Epoch 271 | Train Loss: 0.0878 | Val Loss: 0.1051\n",
      "Epoch 281 | Train Loss: 0.0861 | Val Loss: 0.1032\n",
      "Epoch 291 | Train Loss: 0.0843 | Val Loss: 0.1013\n",
      "Epoch 301 | Train Loss: 0.0826 | Val Loss: 0.0994\n",
      "Epoch 311 | Train Loss: 0.0810 | Val Loss: 0.0975\n",
      "Epoch 321 | Train Loss: 0.0794 | Val Loss: 0.0956\n",
      "Epoch 331 | Train Loss: 0.0778 | Val Loss: 0.0938\n",
      "Epoch 341 | Train Loss: 0.0763 | Val Loss: 0.0920\n",
      "Epoch 351 | Train Loss: 0.0748 | Val Loss: 0.0902\n",
      "Epoch 361 | Train Loss: 0.0733 | Val Loss: 0.0884\n",
      "Epoch 371 | Train Loss: 0.0719 | Val Loss: 0.0867\n",
      "Epoch 381 | Train Loss: 0.0705 | Val Loss: 0.0849\n",
      "Epoch 391 | Train Loss: 0.0691 | Val Loss: 0.0832\n",
      "Epoch 401 | Train Loss: 0.0677 | Val Loss: 0.0816\n",
      "Epoch 411 | Train Loss: 0.0664 | Val Loss: 0.0799\n",
      "Epoch 421 | Train Loss: 0.0652 | Val Loss: 0.0783\n",
      "Epoch 431 | Train Loss: 0.0639 | Val Loss: 0.0767\n",
      "Epoch 441 | Train Loss: 0.0627 | Val Loss: 0.0752\n",
      "Epoch 451 | Train Loss: 0.0615 | Val Loss: 0.0736\n",
      "Epoch 461 | Train Loss: 0.0604 | Val Loss: 0.0722\n",
      "Epoch 471 | Train Loss: 0.0592 | Val Loss: 0.0707\n",
      "Epoch 481 | Train Loss: 0.0582 | Val Loss: 0.0693\n",
      "Epoch 491 | Train Loss: 0.0571 | Val Loss: 0.0679\n",
      "Epoch 501 | Train Loss: 0.0561 | Val Loss: 0.0666\n",
      "Epoch 511 | Train Loss: 0.0551 | Val Loss: 0.0653\n",
      "Epoch 521 | Train Loss: 0.0545 | Val Loss: 0.0640\n",
      "Epoch 531 | Train Loss: 0.4156 | Val Loss: 0.4136\n",
      "Epoch 541 | Train Loss: 0.1124 | Val Loss: 0.1243\n",
      "Epoch 551 | Train Loss: 0.0904 | Val Loss: 0.1013\n",
      "Early stopping triggered. Restoring best model...\n",
      "9998.99123\n",
      "7.000000000000062e-05\n",
      "4.99999999999997e-05\n",
      "3.0000000000000512e-05\n",
      "3.999999999999837e-05\n",
      "3.0000000000000512e-05\n",
      "3.0000000000000512e-05\n",
      "3.0000000000000512e-05\n",
      "2.9999999999998778e-05\n",
      "3.0000000000000512e-05\n"
     ]
    }
   ],
   "source": [
    "ntrain = 800\n",
    "ntest = 400\n",
    "Dtype = 'A'\n",
    "ndim = 2\n",
    "learning_rate = 1e-1\n",
    "ndf = 1\n",
    "nl = 1\n",
    "nm = 50\n",
    "nk = 20    \n",
    "Fout = 1\n",
    "nepoch = 10000\n",
    "data = {}\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for d in range(ndf):\n",
    "    torch.manual_seed(d)\n",
    "\n",
    "    X_train, y_train = sim_data(ntrain, ndim, Dtype)\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device) \n",
    "    X_val, y_val = sim_data(ntrain, ndim, Dtype)\n",
    "    X_val, y_val = X_val.to(device), y_val.to(device) \n",
    "    X_test, y_test = sim_data(ntest, ndim, Dtype)\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device) \n",
    "\n",
    "    epstrain = torch.normal(0, torch.var(y_train)*0.01, size=y_train.size()).to(device)\n",
    "    epstest = torch.normal(0, torch.var(y_test)*0.01, size=y_test.size()).to(device)\n",
    "    epsval = torch.normal(0, torch.var(y_val)*0.01, size=y_val.size()).to(device) \n",
    "\n",
    "    y_train, y_val, y_test = y_train + epstrain, y_val + epsval, y_test + epstest\n",
    "    data[str(d+1)] = {'TrainX': X_train, 'Trainy': y_train, 'ValX': X_val, 'Valy': y_val, 'TestX': X_test, 'Testy': y_test}\n",
    "\n",
    "\n",
    "result = {}\n",
    "Lambdalist = {}\n",
    "Bres = np.zeros((ndf))\n",
    "Pres = np.zeros((ndf))\n",
    "Dres = np.zeros((ndf))\n",
    "Iterlist = np.zeros((ndf))\n",
    "\n",
    "for d in range(ndf):\n",
    "    print('dataset: ', str(d+1))\n",
    "    X_train = data[str(d+1)]['TrainX']; X_test = data[str(d+1)]['TestX']; X_val = data[str(d+1)]['ValX']\n",
    "    y_train = data[str(d+1)]['Trainy']; y_test = data[str(d+1)]['Testy']; y_val = data[str(d+1)]['Valy']\n",
    "\n",
    "    best_model_path = \"./best_DBS_model_d\" + str(d+1)+ \".pt\"\n",
    "    early_stopping = EarlyStopping(patience=30, verbose=False, delta=1e-3, path= best_model_path)\n",
    "    \n",
    "    DeepBS = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = nl, dropout = 0.0, output_dim = Fout, bias = True).to(device)\n",
    "    optimizer = torch.optim.Adam(DeepBS.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(nepoch):\n",
    "        optimizer.zero_grad()\n",
    "        DeepBS.train()\n",
    "\n",
    "        # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "        '''\n",
    "        train_loss = train_one_epoch(DeepBS, X_train, y_train, criterion, optimizer, device)\n",
    "        val_loss = validate(DeepBS, X_train, y_train, criterion, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | \" f\"Val Loss: {val_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "        early_stopping(val_loss, DeepBS)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered. Restoring best model...\")        \n",
    "            break\n",
    "\n",
    "        '''\n",
    "\n",
    "        output = DeepBS(X_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        val_loss = validate(DeepBS, X_val, y_val, criterion, device)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:02d} | Train Loss: {loss:.4f} | \" f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        early_stopping(val_loss, DeepBS)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered. Restoring best model...\")        \n",
    "            break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    del DeepBS, output, loss, val_loss\n",
    "    del X_val, y_val    \n",
    "    \n",
    "    ## ECM -> Find optimal Lambda\n",
    "    with torch.no_grad():\n",
    "        model = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = nl, dropout = 0.0, output_dim = Fout, bias = True).to(device)\n",
    "        model.load_state_dict(torch.load(\"./best_DBS_model_d\" + str(d+1)+ \".pt\", weights_only = True))\n",
    "        BMSPE = criterion(y_test, model(X_test).detach()).item()\n",
    "        Bres[d] = BMSPE\n",
    "        BestLambda, Iter = ECM_update(model, 10, X_train, y_train)\n",
    "        Iterlist[d] = Iter\n",
    "        Lambdalist[str(d+1)] = BestLambda\n",
    "        \n",
    "        del model, X_train, y_train, X_test, y_test       \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "result['Iteration'] = Iterlist\n",
    "result['DeepBS'] = Bres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57efc81-0849-44d2-a64a-dd50c9792ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33c03a0e-da53-4de4-87f0-7afbbc98b296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  1\n",
      "Epoch 01 | Train Loss: 33.8801 | Val Loss: 30.9913\n",
      "9998.89369\n",
      "0.0\n",
      "GCV Converge at  2  iteration\n",
      "Epoch 01 | Train Loss: 35.5681 | Val Loss: 32.8449\n",
      "9998.88318\n",
      "0.0\n",
      "GCV Converge at  2  iteration\n",
      "Epoch 01 | Train Loss: 26.5353 | Val Loss: 24.1090\n",
      "9998.87179\n",
      "0.0\n",
      "GCV Converge at  2  iteration\n",
      "Epoch 01 | Train Loss: 30.7099 | Val Loss: 28.1174\n",
      "9998.88595\n",
      "0.0\n",
      "GCV Converge at  2  iteration\n",
      "Epoch 01 | Train Loss: 31.1598 | Val Loss: 28.4400\n",
      "9998.88595\n",
      "0.0\n",
      "GCV Converge at  2  iteration\n",
      "Epoch 01 | Train Loss: 47.7382 | Val Loss: 44.5691\n",
      "9998.86888\n",
      "0.0\n",
      "GCV Converge at  2  iteration\n",
      "Epoch 01 | Train Loss: 48.8471 | Val Loss: 45.4929\n",
      "9998.8864\n",
      "0.0\n",
      "GCV Converge at  2  iteration\n",
      "Epoch 01 | Train Loss: 32.3550 | Val Loss: 29.7879\n",
      "9998.88563\n",
      "0.0\n",
      "GCV Converge at  2  iteration\n",
      "Epoch 01 | Train Loss: 36.3039 | Val Loss: 33.2604\n",
      "9998.87384\n",
      "0.0\n",
      "GCV Converge at  2  iteration\n"
     ]
    }
   ],
   "source": [
    "neuron_list = [45, 50, 55]\n",
    "knot_list = [15, 20, 25]\n",
    "Lambda_list = torch.zeros((len(neuron_list), len(knot_list)))\n",
    "Lambdadict = {}\n",
    "nl = 1\n",
    "\n",
    "for d in range(ndf):\n",
    "    print('dataset: ', str(d+1))\n",
    "    X_train = data[str(d+1)]['TrainX']; X_test = data[str(d+1)]['TestX']; X_val = data[str(d+1)]['ValX']\n",
    "    y_train = data[str(d+1)]['Trainy']; y_test = data[str(d+1)]['Testy']; y_val = data[str(d+1)]['Valy']\n",
    "    \n",
    "\n",
    "    for i in range(len(neuron_list)):\n",
    "        for j in range(len(knot_list)):\n",
    "            best_model_path = \"./best_DBS_model_d\" + str(d+1)+'n'+str(neuron_list[i])+'k'+str(knot_list[j])+ \".pt\"\n",
    "            early_stopping = EarlyStopping(patience=30, verbose=False, delta=1e-3, path= best_model_path)\n",
    "    \n",
    "            cand_model = DPS(input_dim = ndim, degree = 3, num_knots = knot_list[j], num_neurons = neuron_list[i], num_bsl = nl, dropout = 0.0, output_dim = Fout, bias = True).to(device)\n",
    "            optimizer = torch.optim.Adam(cand_model.parameters(), lr=learning_rate)\n",
    "\n",
    "            for epoch in range(nepoch):\n",
    "                optimizer.zero_grad()\n",
    "                DeepBS.train()\n",
    "        \n",
    "                output = cand_model(X_train)\n",
    "                loss = criterion(output, y_train)\n",
    "                val_loss = validate(cand_model, X_val, y_val, criterion, device)\n",
    "        \n",
    "                if epoch % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1:02d} | Train Loss: {loss:.4f} | \" f\"Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "                early_stopping(val_loss, cand_model)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Early stopping triggered. Restoring best model...\")        \n",
    "                    break\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "            \n",
    "            del cand_model, output, loss, val_loss\n",
    "    \n",
    "            ## ECM -> Find optimal Lambda\n",
    "            with torch.no_grad():\n",
    "                model = DPS(input_dim = ndim, degree = 3, num_knots = knot_list[j], num_neurons = neuron_list[i], num_bsl = nl, dropout = 0.0, output_dim = Fout, bias = True).to(device)\n",
    "                model.load_state_dict(torch.load(\"./best_DBS_model_d\" + str(d+1)+'n'+str(neuron_list[i])+'k'+str(knot_list[j])+ \".pt\", weights_only = True))\n",
    "                BMSPE = criterion(y_test, model(X_test).detach()).item()\n",
    "                Bres[d] = BMSPE\n",
    "                BestLambda, _ = ECM_update(model, 10, X_train, y_train)\n",
    "                Lambda_list[i, j] = BestLambda\n",
    "                \n",
    "                del model \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    Lambdadict[str(d+1)] = Lambda_list\n",
    "    del X_train, y_train, X_test, y_test, X_val, y_val\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da5285a-d382-4f60-aa94-aec18f2a3524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea2eb56-7481-49ef-8804-79310a184afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb0dbc-1516-483f-be89-06c8ef0b3e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cb25b3d-75a8-46b6-ba32-f402b60e5062",
   "metadata": {},
   "source": [
    "## Convergence Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8856a253-f293-4c59-9803-104e254e98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepBS = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = nl, dropout = 0.0, output_dim = Fout, bias = True).to(device)\n",
    "DeepBS.load_state_dict(torch.load(\"./best_DBS_model_d\" + str(d+1)+ \".pt\", weights_only = True))\n",
    "\n",
    "X_train = data[str(d+1)]['TrainX']; X_test = data[str(d+1)]['TestX']; X_val = data[str(d+1)]['ValX']\n",
    "par = DeepBS.get_para_ecm(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "20a20d83-ccdc-4022-a67c-a4af7ef4b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import cholesky, solve_triangular\n",
    "\n",
    "def multivariate_log_likelihood(X, mu, Sigma):\n",
    "    \"\"\"\n",
    "    X: Data matrix (n x 20)\n",
    "    mu: Mean vector (20,)\n",
    "    Sigma: Covariance matrix (20x20)\n",
    "    \"\"\"\n",
    "    d, n = X.shape\n",
    "    # Center data\n",
    "    R = X.T - mu.T\n",
    "    # Cholesky decomposition (lower triangular)\n",
    "    L = cholesky(Sigma, lower=True)\n",
    "    # Solve L @ z = R.T for z\n",
    "    z = solve_triangular(L, R.T, lower=True)\n",
    "    # Quadratic form (Mahalanobis distance)\n",
    "    maha = np.sum(z**2)\n",
    "    # Log-determinant from Cholesky\n",
    "    log_det = 2 * np.sum(np.log(np.diag(L)))\n",
    "    # Log-likelihood calculation\n",
    "    return -0.5 * (n * d * np.log(2 * np.pi) + n * log_det + maha)\n",
    "\n",
    "\n",
    "n_block, num_knots, num_neurons = par['wbasic'].size()\n",
    "ls_lambda = torch.empty(n_block)\n",
    "xi = 1; sigma = 1\n",
    "\n",
    "for l in range(n_block):\n",
    "    B = par['ebasic'][l]\n",
    "    By = par['basic'][l]\n",
    "    WB = par['wbasic'][l]\n",
    "    \n",
    "    DB = diag_mat_weights(WB.size()[0]).to(device)\n",
    "    size = B.size()[1]\n",
    "    S = DB.T @ DB\n",
    "    Cov_a = (xi**2)* torch.linalg.pinv(S)\n",
    "    Cov_a.to(device)\n",
    "    Cov_e = (torch.eye(size*num_neurons)* sigma).to(device)\n",
    "    \n",
    "    block_y = torch.reshape(By, (-1,1))\n",
    "    flatB = B.view(num_neurons, num_knots, size)\n",
    "        \n",
    "    sqr_xi= 0\n",
    "    sqr_sig = 0\n",
    "\n",
    "    llg_sum = 0\n",
    "    for i in range(num_neurons):\n",
    "        Ncov = (Cov_a -(Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)]) @ flatB[i].T @ Cov_a))\n",
    "        Nmu = (Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)])) @ By[:,i].reshape(-1,1)\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e401635-6485-41f6-8c5f-4588bf1ca5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377d91a-d4ce-45ad-9915-9c1afd529fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24ed95-945a-40ab-8aaf-dc1a5431b1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e96ae-0f43-4884-b195-7eef835fefe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTORCH",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
