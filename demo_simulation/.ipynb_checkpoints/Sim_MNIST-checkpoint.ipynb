{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78638a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import glob\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce2513e-3f5f-4b62-9dcb-d786cf7a0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag_mat_weights(dimp, type = 'first'):\n",
    "    if type == 'first':\n",
    "        dg = np.zeros((dimp-1, dimp))\n",
    "        for i in range(dimp-1):\n",
    "            dg[i,i] = -1\n",
    "            dg[i,i+1]= 1\n",
    "    elif type == 'second':\n",
    "        dg = np.zeros((dimp-2, dimp))\n",
    "        for i in range(dimp-2):\n",
    "            dg[i,i] = -1\n",
    "            dg[i,i+1]= 2\n",
    "            dg[i,i+2]= -1\n",
    "    else:\n",
    "        pass\n",
    "    return torch.Tensor(dg)\n",
    "    \n",
    "\n",
    "class BSL(nn.Module):\n",
    "    def __init__(self, degree, num_knots, num_neurons, bias = True):\n",
    "        super(BSL, self).__init__()\n",
    "        self.degree = degree\n",
    "        self.num_knots = num_knots\n",
    "        self.num_neurons = num_neurons\n",
    "        self.control_p = nn.Parameter(torch.randn(self.num_knots, self.num_neurons))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(self.num_neurons))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.inter = {}\n",
    "    \n",
    "    def basis_function(self, x, i, k, t):\n",
    "    \n",
    "        # Base case: degree 0 spline\n",
    "        if k == 0:\n",
    "            return ((t[i] <= x) & (x < t[i + 1])).float()\n",
    "    \n",
    "        # Recursive case\n",
    "        denom1 = t[i + k] - t[i]\n",
    "        denom2 = t[i + k + 1] - t[i + 1]\n",
    "    \n",
    "        term1 = 0\n",
    "        if denom1 != 0:\n",
    "            term1 = (x - t[i]) / denom1 * self.basis_function(x, i, k - 1, t)\n",
    "    \n",
    "        term2 = 0\n",
    "        if denom2 != 0:\n",
    "            term2 = (t[i + k + 1] - x) / denom2 * self.basis_function(x, i + 1, k - 1, t)\n",
    "    \n",
    "        return term1 + term2\n",
    "\n",
    "    def knots_distribution(self, dg, nk):\n",
    "\n",
    "        knots = torch.cat([torch.linspace(-0.002, -0.001, steps=dg),            # Add repeated values at the start for clamping\n",
    "            torch.linspace(0, 1, nk-2*dg-2),  # Uniform knot spacing in the middle\n",
    "            torch.linspace(1.001, 1.002, steps=dg)           # Add repeated values at the end for clamping\n",
    "            ]).view(-1,1)\n",
    "        \n",
    "        knots = torch.cat([torch.linspace(0, 1, nk-2)          # Add repeated values at the end for clamping\n",
    "            ]).view(-1,1)\n",
    "\n",
    "        return knots\n",
    "    \n",
    "    def basis_function2(self, x, spl):\n",
    "        basis_output = spl.fit_transform(x.cpu().numpy())\n",
    "        return basis_output\n",
    "            \n",
    "    def forward(self, x):\n",
    "        batch_size, num_features = x.size()\n",
    "        device = x.device\n",
    "        \n",
    "        # Create knot vector and apply B-spline basis functions for each feature\n",
    "    \n",
    "        basises = []\n",
    "        knots = self.knots_distribution(self.degree, self.num_knots)\n",
    "        spl = SplineTransformer(n_knots=self.num_knots, degree=self.degree, knots = knots)\n",
    "\n",
    "        \n",
    "        for feature in range(num_features):\n",
    "            # Calculate B-spline basis functions for this feature\n",
    "            \n",
    "            basis = self.basis_function2(x[:, feature].reshape(-1,1), spl)\n",
    "            basis = torch.Tensor(basis).to(device)\n",
    "            basises.append(basis)\n",
    "        \n",
    "        if num_features == 1:\n",
    "            tout = basises[0] @ self.control_p\n",
    "            self.inter['basic'] = basises[0].T\n",
    "        else:\n",
    "            self.inter['basic'] = torch.reshape(torch.stack(basises, dim = 1), (batch_size, self.num_knots * self.num_neurons)).T\n",
    "            basises = torch.stack(basises)\n",
    "            tout = basises.permute(1,2,0) * self.control_p\n",
    "            tout = tout.sum(dim =1)\n",
    "                \n",
    "        if self.bias is not None:\n",
    "            tout += self.bias        \n",
    "            \n",
    "        return tout\n",
    "\n",
    "\n",
    "class NormLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NormLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        min_val = torch.min(x, axis = 1).values.reshape(-1,1)\n",
    "        max_val = torch.max(x, axis = 1).values.reshape(-1,1)\n",
    "\n",
    "        x = (x - min_val)/(max_val - min_val)  # Rescale to [0, 1]\n",
    "        return x.detach()\n",
    "    \n",
    "class BSpline_block(nn.Module):\n",
    "    def __init__(self, degree, num_knots, num_neurons, dropout = 0.0, bias = True):\n",
    "        super(BSpline_block, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(OrderedDict([\n",
    "            ('norm', NormLayer()),\n",
    "            ('BSL', BSL(degree = degree, num_knots = num_knots, num_neurons = num_neurons, bias = bias)),\n",
    "            ('drop', nn.Dropout(dropout)),\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "        \n",
    "class StackBS_block(nn.Module):\n",
    "    def __init__(self, block, degree, num_knots, num_neurons, num_blocks, dropout = 0.0, bias = True):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleDict({\n",
    "            f'block_{i}': block(degree = degree, num_knots = num_knots, num_neurons = num_neurons)\n",
    "            for i in range(num_blocks)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        for name, block in self.model.items():\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3231c48-f7b3-4619-8cf8-4a7bdd4cf4be",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c7eef1-9f10-48e6-a6ec-fc1df9ac3a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        scheduler.step()\n",
    "        \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            output = torch.log_softmax(output, dim=1)\n",
    "            _, pred = torch.max(output, dim = 1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def extract_n_samples(data, targets, digit, n):\n",
    "    indices = (targets == digit).nonzero().squeeze()\n",
    "    selected_indices = indices[:n]\n",
    "    return data[selected_indices], targets[selected_indices]\n",
    "    \n",
    "def data_preprocessing(MNIST_Dataset, extract_num, size, Type):\n",
    "    datas = MNIST_Dataset.data\n",
    "    targets = MNIST_Dataset.targets\n",
    "\n",
    "    data_array = torch.empty(0, datas.size()[1], datas.size()[2]).to(device)\n",
    "    targets_array = torch.empty(0).to(device)\n",
    "\n",
    "    for num in extract_num:\n",
    "        data, label = extract_n_samples(datas, targets, digit = num, n = size)\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        data_array = torch.cat([data_array, data], dim = 0).to(device)\n",
    "        targets_array = torch.cat([targets_array, label], dim = 0).to(device)\n",
    "\n",
    "    data_array = data_array.unsqueeze(1).float() / 255.0\n",
    "    data_array = (data_array - 0.1307) / 0.3081\n",
    "\n",
    "    if Type == 'train':\n",
    "        perm = torch.randperm(data_array.size(0))\n",
    "        data_array = data_array[perm]\n",
    "        targets_array = targets_array[perm]\n",
    "\n",
    "    if len(extract_num) > 2:\n",
    "        # Multi-label\n",
    "        targets_array = targets_array.long()  # ensure correct dtype\n",
    "        \n",
    "    return data_array, targets_array\n",
    "\n",
    "def accuracy(prediction, target):\n",
    "    \n",
    "    return ((torch.argmax(prediction, axis = 1) == target).sum()/ target.size(0)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a8080c9-d5e2-4858-84f1-d493051e8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps:0\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Download the MNIST dataset\n",
    "train_dataset_full = datasets.MNIST(root='./data', train=True, download=True)\n",
    "test_dataset_full = datasets.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "target_num = [0, 1, 2, 3]\n",
    "train_size = 200\n",
    "test_size = 200\n",
    "X_train, y_train = data_preprocessing(train_dataset_full, target_num, train_size, 'train')\n",
    "X_test, y_test = data_preprocessing(test_dataset_full, target_num, test_size, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f21edc-3c3d-4c7b-95ec-e8f0e5bbabfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45c9015-c08b-4a47-8fea-ef000648222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self, dg, nk, nm, nbl, dropout, Fout, bias):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.gap = nn.Flatten()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1568, nm),\n",
    "            StackBS_block(BSpline_block, degree = dg, num_knots = nk, num_neurons = nm, num_blocks = nbl, dropout = dropout),\n",
    "            nn.Linear(nm, Fout))\n",
    "        self.sm = nn.Softmax(dim = 1)\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.classifier(x)\n",
    "        x = self.sm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_para_ecm(self, x):\n",
    "\n",
    "        '''\n",
    "        ecm_para: A dictionary that collects the parameter we need to the following ECM algorithm.\n",
    "        ecm_para.basic: Store the output of each B-Spline block; Dimension = [n_sample, n_neurons]\n",
    "        ecm_para.ebasic Store the weight matrix of each B-Spline expansion; Dimension = [n_knots * n_neurons, n_sample]\n",
    "\n",
    "        '''\n",
    "        ecm_para = {}\n",
    "        bs_block_out = {}\n",
    "        bs_spline_weight = {}\n",
    "        bs_spline_value = {}\n",
    "        bs_spline_bias = {}\n",
    "\n",
    "        _ = self(x)\n",
    "        \n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                bs_block_out[name] = output.detach()\n",
    "            return hook\n",
    "\n",
    "        handles = []\n",
    "        for name, layer in self.named_modules():\n",
    "            if 'block.drop' in name:\n",
    "                handles.append(layer.register_forward_hook(get_activation(name)))\n",
    "            elif 'block.BSL' in name:\n",
    "                bs_spline_value[name] = layer.inter['basic'].detach()\n",
    "                bs_spline_weight[name] = layer.control_p.detach()\n",
    "                bs_spline_bias[name] = layer.bias.detach()\n",
    "        # Run forward pass (triggers hooks)\n",
    "        _ = self(x)\n",
    "\n",
    "        # Clean up hooks\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "            \n",
    "        ecm_para['basic'] = torch.stack(list(bs_block_out.values()), dim=0)\n",
    "        ecm_para['ebasic'] = torch.stack(list(bs_spline_value.values()), dim=0)\n",
    "        ecm_para['wbasic'] = torch.stack(list(bs_spline_weight.values()), dim=0)\n",
    "        ecm_para['bbasic'] = torch.stack(list(bs_spline_bias.values()), dim=0)\n",
    "        del bs_block_out, bs_spline_weight, bs_spline_value, bs_spline_bias\n",
    "        \n",
    "        return ecm_para\n",
    "\n",
    "nm = 50; nk = 15; dg = 3; nl = 1; Iteration = 300\n",
    "mnist_DeepBS = MNISTClassifier(dg = dg, nk = nk, nm = nm, nbl = nl, dropout = 0.0, Fout = len(target_num), bias = True).to(device)\n",
    "learning_r = 1e-2\n",
    "optimizer = torch.optim.Adam(mnist_DeepBS.parameters(), lr=learning_r)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd18a6-5a45-4860-98a7-2bf0e99b63ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "615a9942-375c-4965-a78e-d4bfcb5fd617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch:  1 / 300  | Loss:  1.0957  | Acc:  0.3616666793823242\n",
      "tensor(0.3700, device='mps:0')\n",
      "| Epoch:  11 / 300  | Loss:  0.9287  | Acc:  0.7799999713897705\n",
      "| Epoch:  21 / 300  | Loss:  0.7923  | Acc:  0.8566666841506958\n",
      "| Epoch:  31 / 300  | Loss:  0.7063  | Acc:  0.8983333110809326\n",
      "| Epoch:  41 / 300  | Loss:  0.6595  | Acc:  0.9233333468437195\n",
      "| Epoch:  51 / 300  | Loss:  0.6326  | Acc:  0.9399999976158142\n",
      "| Epoch:  61 / 300  | Loss:  0.6152  | Acc:  0.9566666483879089\n",
      "| Epoch:  71 / 300  | Loss:  0.6036  | Acc:  0.9666666388511658\n",
      "| Epoch:  81 / 300  | Loss:  0.5959  | Acc:  0.9683333039283752\n",
      "| Epoch:  91 / 300  | Loss:  0.5905  | Acc:  0.971666693687439\n",
      "| Epoch:  101 / 300  | Loss:  0.5862  | Acc:  0.9733333587646484\n",
      "tensor(0.9417, device='mps:0')\n",
      "| Epoch:  111 / 300  | Loss:  0.5828  | Acc:  0.9766666889190674\n",
      "| Epoch:  121 / 300  | Loss:  0.5796  | Acc:  0.9783333539962769\n",
      "| Epoch:  131 / 300  | Loss:  0.5764  | Acc:  0.9816666841506958\n",
      "| Epoch:  141 / 300  | Loss:  0.5731  | Acc:  0.9866666793823242\n",
      "| Epoch:  151 / 300  | Loss:  0.5706  | Acc:  0.9866666793823242\n",
      "| Epoch:  161 / 300  | Loss:  0.5686  | Acc:  0.9883333444595337\n",
      "| Epoch:  171 / 300  | Loss:  0.567  | Acc:  0.9883333444595337\n",
      "| Epoch:  181 / 300  | Loss:  0.565  | Acc:  0.9916666746139526\n",
      "| Epoch:  191 / 300  | Loss:  0.5635  | Acc:  0.9933333396911621\n",
      "| Epoch:  201 / 300  | Loss:  0.5624  | Acc:  0.9933333396911621\n",
      "tensor(0.9383, device='mps:0')\n",
      "| Epoch:  211 / 300  | Loss:  0.5617  | Acc:  0.9933333396911621\n",
      "| Epoch:  221 / 300  | Loss:  0.5612  | Acc:  0.9933333396911621\n",
      "| Epoch:  231 / 300  | Loss:  0.5609  | Acc:  0.9933333396911621\n",
      "| Epoch:  241 / 300  | Loss:  0.5605  | Acc:  0.9933333396911621\n",
      "| Epoch:  251 / 300  | Loss:  0.5603  | Acc:  0.9933333396911621\n",
      "| Epoch:  261 / 300  | Loss:  0.5601  | Acc:  0.9933333396911621\n",
      "| Epoch:  271 / 300  | Loss:  0.5599  | Acc:  0.9933333396911621\n",
      "| Epoch:  281 / 300  | Loss:  0.5597  | Acc:  0.9933333396911621\n",
      "| Epoch:  291 / 300  | Loss:  0.5596  | Acc:  0.9933333396911621\n"
     ]
    }
   ],
   "source": [
    "for t in range(Iteration):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "    pyb_af = mnist_DeepBS(X_train)\n",
    "    loss = criterion(pyb_af, y_train)\n",
    "    \n",
    "    prediction = torch.argmax(pyb_af, axis = 1)\n",
    "    acc = (torch.argmax(pyb_af, axis = 1) == y_train).sum()/len(y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(t % 10 == 0):\n",
    "        print('| Epoch: ',t+1,'/',str(Iteration),' | Loss: ', np.round(loss.item(), 4),' | Acc: ', acc.item())\n",
    "        if(t % 100 == 0):\n",
    "            with torch.no_grad():\n",
    "                print((torch.argmax(mnist_DeepBS(X_test).detach(), axis = 1) == y_test).sum()/len(y_test))\n",
    "\n",
    "torch.save(mnist_DeepBS.state_dict(), './MNIST'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e363d-c692-4f18-ae8f-d8168cdcf13d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2a1697c-2a51-44e0-b467-cd4aecd081d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target digits: [0, 1, 2]\n",
      "| Training Accuracy: 99.33% | Testing Accuracy: 94.50% |\n"
     ]
    }
   ],
   "source": [
    "eval_model = MNISTClassifier(dg = dg, nk = nk, nm = nm, nbl = nl, dropout = 0.0, Fout = len(target_num), bias = True).to(device)\n",
    "eval_model.load_state_dict(torch.load('./MNIST'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk), weights_only = True))\n",
    "eval_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_pred_postecm = eval_model(X_train)\n",
    "    train_acc = accuracy(train_pred_postecm, y_train)\n",
    "    test_pred_postecm = eval_model(X_test)\n",
    "    test_acc = accuracy(test_pred_postecm, y_test)\n",
    "    print(f\"Target digits: {target_num}\")\n",
    "    print(f\"| Training Accuracy: {train_acc*100:.2f}% | Testing Accuracy: {test_acc*100:.2f}% |\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194fcad-c101-430f-b75f-ac9f933f05ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82dfaedd-becf-4ab8-9419-809e72c05d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ECM(par, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4):\n",
    "    lambdab = initial_lambda\n",
    "    sigma = initial_sigma\n",
    "    xi = initial_xi\n",
    "    \n",
    "    n_block, num_knots, num_neurons = par['wbasic'].size()\n",
    "    ls_lambda = torch.empty(n_block)\n",
    "    \n",
    "    for l in range(n_block):\n",
    "        B = par['ebasic'][l]\n",
    "        By = par['basic'][l]\n",
    "        WB = par['wbasic'][l]\n",
    "        \n",
    "        DB = diag_mat_weights(WB.size()[0]).to(device)\n",
    "        size = B.size()[1]\n",
    "        S = DB.T @ DB\n",
    "        Cov_a = (xi**2)* torch.linalg.pinv(S)\n",
    "        Cov_a.to(device)\n",
    "        Cov_e = (torch.eye(size*num_neurons)* sigma).to(device)\n",
    "        \n",
    "        block_y = torch.reshape(By, (-1,1))\n",
    "        flatB = B.view(num_neurons, num_knots, size)\n",
    "            \n",
    "        sqr_xi= 0\n",
    "        sqr_sig = 0\n",
    "\n",
    "        for i in range(num_neurons):\n",
    "            Ncov = (Cov_a -(Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)]) @ flatB[i].T @ Cov_a))\n",
    "            Nmu = (Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)])) @ By[:,i].reshape(-1,1)\n",
    "            \n",
    "            first_xi = S @ Ncov\n",
    "            second_xi = (Nmu.T @ S @ Nmu)\n",
    "            sqr_xi += torch.trace(first_xi) + second_xi\n",
    "                \n",
    "            first_sig = torch.norm(By[:,i])\n",
    "            second_sig = 2 * (By[:,i] @ flatB[i].T) @ Nmu \n",
    "            third_sig = torch.trace((flatB[i] @ flatB[i].T) @ Ncov)\n",
    "            four_sig = (Nmu.T @ flatB[i] @ flatB[i].T @ Nmu)\n",
    "            \n",
    "            sqr_sig += (first_sig + second_sig + third_sig + four_sig)\n",
    "            \n",
    "            del first_xi, second_xi, first_sig, second_sig, third_sig, four_sig\n",
    "\n",
    "        sqr_xi /= num_neurons\n",
    "        sqr_sig /= (num_neurons*size)\n",
    "\n",
    "        ls_lambda[l] = (sqr_sig/sqr_xi).item()\n",
    "        \n",
    "        del Cov_a, Cov_e, flatB, B, By, WB\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return ls_lambda\n",
    "    \n",
    "def ECM_layersise_update(model, par, Lambda, x, y):\n",
    "\n",
    "    model.eval()\n",
    "    device = x.device\n",
    "    \n",
    "    B_out, B_in, B_w, B_b = par['basic'], par['ebasic'], par['wbasic'], par['bbasic']\n",
    "    n_layer, nk, nm = B_w.size()\n",
    "    DB = diag_mat_weights(B_w[0].size()[0], 'second').to(device)\n",
    "\n",
    "    Project_matrix = (torch.linalg.pinv(B_in[-1].T @ B_in[-1]) @ B_in[-1].T @ B_in[-1])\n",
    "    Size = [b.size()[1] for b in B_in]\n",
    "\n",
    "    B_in = B_in.view(n_layer, nm, nk, Size[0])\n",
    "\n",
    "    for l in range(n_layer):    \n",
    "        NW = torch.empty((nk, nm)).to(device)\n",
    "        NB = torch.empty((nm)).to(device)\n",
    "        \n",
    "        for i in range(nm):\n",
    "            B1y = B_out[l][:,i] - B_b[l][i]\n",
    "            BB = B_in[l][i].T\n",
    "    \n",
    "            # Update the weights and bias\n",
    "            NW[:, i] = (torch.inverse(BB.T @ BB + (Lambda[l]/Size[l]) * (DB.T @ DB)) @ BB.T @ B1y)\n",
    "            NB[i] = torch.mean(B_out[l][:,i] - (NW[:,i] @ BB.T))\n",
    "                \n",
    "        # update the weight\n",
    "        block = getattr(model.classifier[1].model, f'block_{l}')\n",
    "        getattr(block.block.BSL, 'control_p').data = NW\n",
    "        getattr(block.block.BSL, 'bias').data = NB\n",
    "\n",
    "        del NW, NB, B1y, BB, block\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        DPSy = model(x)\n",
    "        CLoss = criterion(DPSy.detach(), y)\n",
    "        GCV = CLoss/(Size[-1]-torch.trace(Project_matrix))\n",
    "    \n",
    "    return model, GCV\n",
    "\n",
    "def ECM_update(model, max_iter, x, y):\n",
    "    BestGCV = prev = 9999\n",
    "    patient = 10\n",
    "    pcount = 0\n",
    "    for i in range(max_iter):\n",
    "        _ = model(X_train)\n",
    "        ECM_para = model.get_para_ecm(x)\n",
    "        ECM_Lambda = ECM(ECM_para, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4)\n",
    "\n",
    "        model, GCV = ECM_layersise_update(model, ECM_para, ECM_Lambda, x, y)\n",
    "        if np.abs(prev - GCV.cpu().detach().numpy()) < 5e-5:\n",
    "            print('GCV Converge at ',i+1,' iteration')\n",
    "            break\n",
    "            \n",
    "        if GCV < BestGCV:\n",
    "            BestLambda = ECM_Lambda\n",
    "            BestGCV = GCV\n",
    "            pcount = 0\n",
    "        else:\n",
    "            pcount += 1\n",
    "\n",
    "        if pcount == patient:\n",
    "            print('GCV Converge at ',i,' iteration')\n",
    "            break\n",
    "\n",
    "        prev = GCV.cpu().detach().numpy()\n",
    "\n",
    "        del ECM_para, ECM_Lambda\n",
    "        \n",
    "    del model\n",
    "    \n",
    "    return BestLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6fe32-ee90-42e9-851d-917c8e56cfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "338dd381-a752-4830-b5bf-93fc66068870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCV Converge at  2  iteration\n"
     ]
    }
   ],
   "source": [
    "eval_model = MNISTClassifier(dg = dg, nk = nk, nm = nm, nbl = nl, dropout = 0.0, Fout = len(target_num), bias = True).to(device)\n",
    "eval_model.load_state_dict(torch.load('./MNIST'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk), weights_only = True))\n",
    "eval_model.eval()\n",
    "with torch.no_grad():\n",
    "    BestLambda = ECM_update(eval_model, 10, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f20ef1a-c2f0-4e8a-984c-d0d016afc0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch:  1 / 1000  | Loss:  0.5738351345062256  | Acc:  0.99333\n",
      "tensor(0.9383, device='mps:0')\n",
      "| Epoch:  11 / 1000  | Loss:  0.5831608176231384  | Acc:  0.97833\n",
      "| Epoch:  21 / 1000  | Loss:  0.5711502432823181  | Acc:  0.99167\n",
      "| Epoch:  31 / 1000  | Loss:  0.5682769417762756  | Acc:  0.99333\n",
      "| Epoch:  41 / 1000  | Loss:  0.5676297545433044  | Acc:  0.99333\n",
      "| Epoch:  51 / 1000  | Loss:  0.5669155716896057  | Acc:  0.99333\n",
      "| Epoch:  61 / 1000  | Loss:  0.5662226676940918  | Acc:  0.99333\n",
      "| Epoch:  71 / 1000  | Loss:  0.5656009316444397  | Acc:  0.99333\n",
      "| Epoch:  81 / 1000  | Loss:  0.5650939345359802  | Acc:  0.99333\n",
      "| Epoch:  91 / 1000  | Loss:  0.5646349787712097  | Acc:  0.99333\n",
      "| Epoch:  101 / 1000  | Loss:  0.5642228126525879  | Acc:  0.99333\n",
      "tensor(0.9533, device='mps:0')\n",
      "| Epoch:  111 / 1000  | Loss:  0.5638472437858582  | Acc:  0.99333\n",
      "| Epoch:  121 / 1000  | Loss:  0.5631074905395508  | Acc:  0.99333\n",
      "| Epoch:  131 / 1000  | Loss:  0.5606483817100525  | Acc:  0.99667\n",
      "| Epoch:  141 / 1000  | Loss:  0.559965193271637  | Acc:  0.99667\n",
      "| Epoch:  151 / 1000  | Loss:  0.5595739483833313  | Acc:  0.99667\n",
      "| Epoch:  161 / 1000  | Loss:  0.5592502355575562  | Acc:  0.99667\n",
      "| Epoch:  171 / 1000  | Loss:  0.5589936375617981  | Acc:  0.99667\n",
      "| Epoch:  181 / 1000  | Loss:  0.5587762594223022  | Acc:  0.99667\n",
      "| Epoch:  191 / 1000  | Loss:  0.5585918426513672  | Acc:  0.99667\n",
      "| Epoch:  201 / 1000  | Loss:  0.5584274530410767  | Acc:  0.99667\n",
      "tensor(0.9600, device='mps:0')\n",
      "| Epoch:  211 / 1000  | Loss:  0.5582818388938904  | Acc:  0.99667\n",
      "| Epoch:  221 / 1000  | Loss:  0.5581513047218323  | Acc:  0.99667\n",
      "| Epoch:  231 / 1000  | Loss:  0.5580341219902039  | Acc:  0.99667\n",
      "| Epoch:  241 / 1000  | Loss:  0.5579285621643066  | Acc:  0.99667\n",
      "| Epoch:  251 / 1000  | Loss:  0.5578330159187317  | Acc:  0.99667\n",
      "| Epoch:  261 / 1000  | Loss:  0.5577463507652283  | Acc:  0.99667\n",
      "| Epoch:  271 / 1000  | Loss:  0.5576671957969666  | Acc:  0.99667\n",
      "| Epoch:  281 / 1000  | Loss:  0.5575942993164062  | Acc:  0.99667\n",
      "| Epoch:  291 / 1000  | Loss:  0.5575269460678101  | Acc:  0.99667\n",
      "| Epoch:  301 / 1000  | Loss:  0.5574642419815063  | Acc:  0.99667\n",
      "tensor(0.9683, device='mps:0')\n",
      "| Epoch:  311 / 1000  | Loss:  0.5574054718017578  | Acc:  0.99667\n",
      "| Epoch:  321 / 1000  | Loss:  0.5573500394821167  | Acc:  0.99667\n",
      "| Epoch:  331 / 1000  | Loss:  0.5572974681854248  | Acc:  0.99667\n",
      "| Epoch:  341 / 1000  | Loss:  0.5572473406791687  | Acc:  0.99667\n",
      "| Epoch:  351 / 1000  | Loss:  0.5571991801261902  | Acc:  0.99667\n",
      "| Epoch:  361 / 1000  | Loss:  0.5571506023406982  | Acc:  0.99667\n",
      "| Epoch:  371 / 1000  | Loss:  0.5566774606704712  | Acc:  0.99667\n",
      "| Epoch:  381 / 1000  | Loss:  0.5558933019638062  | Acc:  0.99833\n",
      "| Epoch:  391 / 1000  | Loss:  0.5557306408882141  | Acc:  0.99833\n",
      "| Epoch:  401 / 1000  | Loss:  0.5556069016456604  | Acc:  0.99833\n",
      "tensor(0.9733, device='mps:0')\n",
      "| Epoch:  411 / 1000  | Loss:  0.5555328130722046  | Acc:  0.99833\n",
      "| Epoch:  421 / 1000  | Loss:  0.5554840564727783  | Acc:  0.99833\n",
      "| Epoch:  431 / 1000  | Loss:  0.555444061756134  | Acc:  0.99833\n",
      "| Epoch:  441 / 1000  | Loss:  0.5554075837135315  | Acc:  0.99833\n",
      "| Epoch:  451 / 1000  | Loss:  0.5553737282752991  | Acc:  0.99833\n",
      "| Epoch:  461 / 1000  | Loss:  0.5553416609764099  | Acc:  0.99833\n",
      "| Epoch:  471 / 1000  | Loss:  0.5553109645843506  | Acc:  0.99833\n",
      "| Epoch:  481 / 1000  | Loss:  0.555281400680542  | Acc:  0.99833\n",
      "| Epoch:  491 / 1000  | Loss:  0.5552529692649841  | Acc:  0.99833\n",
      "| Epoch:  501 / 1000  | Loss:  0.555225670337677  | Acc:  0.99833\n",
      "tensor(0.9717, device='mps:0')\n",
      "| Epoch:  511 / 1000  | Loss:  0.5551992058753967  | Acc:  0.99833\n",
      "| Epoch:  521 / 1000  | Loss:  0.5551738142967224  | Acc:  0.99833\n",
      "| Epoch:  531 / 1000  | Loss:  0.5551491975784302  | Acc:  0.99833\n",
      "| Epoch:  541 / 1000  | Loss:  0.5551252961158752  | Acc:  0.99833\n",
      "| Epoch:  551 / 1000  | Loss:  0.5551023483276367  | Acc:  0.99833\n",
      "| Epoch:  561 / 1000  | Loss:  0.555079996585846  | Acc:  0.99833\n",
      "| Epoch:  571 / 1000  | Loss:  0.5550584197044373  | Acc:  0.99833\n",
      "| Epoch:  581 / 1000  | Loss:  0.5550373196601868  | Acc:  0.99833\n",
      "| Epoch:  591 / 1000  | Loss:  0.5550169348716736  | Acc:  0.99833\n",
      "| Epoch:  601 / 1000  | Loss:  0.5549973845481873  | Acc:  0.99833\n",
      "tensor(0.9717, device='mps:0')\n",
      "| Epoch:  611 / 1000  | Loss:  0.55497807264328  | Acc:  0.99833\n",
      "| Epoch:  621 / 1000  | Loss:  0.5549594163894653  | Acc:  0.99833\n",
      "| Epoch:  631 / 1000  | Loss:  0.5549414753913879  | Acc:  0.99833\n",
      "| Epoch:  641 / 1000  | Loss:  0.5549237728118896  | Acc:  0.99833\n",
      "| Epoch:  651 / 1000  | Loss:  0.5549066662788391  | Acc:  0.99833\n",
      "| Epoch:  661 / 1000  | Loss:  0.5548901557922363  | Acc:  0.99833\n",
      "| Epoch:  671 / 1000  | Loss:  0.5548738837242126  | Acc:  0.99833\n",
      "| Epoch:  681 / 1000  | Loss:  0.5548580884933472  | Acc:  0.99833\n",
      "| Epoch:  691 / 1000  | Loss:  0.5548427700996399  | Acc:  0.99833\n",
      "| Epoch:  701 / 1000  | Loss:  0.5548278093338013  | Acc:  0.99833\n",
      "tensor(0.9700, device='mps:0')\n",
      "| Epoch:  711 / 1000  | Loss:  0.5548133254051208  | Acc:  0.99833\n",
      "| Epoch:  721 / 1000  | Loss:  0.5547991394996643  | Acc:  0.99833\n",
      "| Epoch:  731 / 1000  | Loss:  0.5547852516174316  | Acc:  0.99833\n",
      "| Epoch:  741 / 1000  | Loss:  0.5547717213630676  | Acc:  0.99833\n",
      "| Epoch:  751 / 1000  | Loss:  0.5547584295272827  | Acc:  0.99833\n",
      "| Epoch:  761 / 1000  | Loss:  0.5547455549240112  | Acc:  0.99833\n",
      "| Epoch:  771 / 1000  | Loss:  0.5547329187393188  | Acc:  0.99833\n",
      "| Epoch:  781 / 1000  | Loss:  0.5547208189964294  | Acc:  0.99833\n",
      "| Epoch:  791 / 1000  | Loss:  0.5547086596488953  | Acc:  0.99833\n",
      "| Epoch:  801 / 1000  | Loss:  0.554696798324585  | Acc:  0.99833\n",
      "tensor(0.9700, device='mps:0')\n",
      "| Epoch:  811 / 1000  | Loss:  0.554685115814209  | Acc:  0.99833\n",
      "| Epoch:  821 / 1000  | Loss:  0.5546742081642151  | Acc:  0.99833\n",
      "| Epoch:  831 / 1000  | Loss:  0.5546630024909973  | Acc:  0.99833\n",
      "| Epoch:  841 / 1000  | Loss:  0.5546520948410034  | Acc:  0.99833\n",
      "| Epoch:  851 / 1000  | Loss:  0.5546413660049438  | Acc:  0.99833\n",
      "| Epoch:  861 / 1000  | Loss:  0.5546309351921082  | Acc:  0.99833\n",
      "| Epoch:  871 / 1000  | Loss:  0.5546208620071411  | Acc:  0.99833\n",
      "| Epoch:  881 / 1000  | Loss:  0.5546106696128845  | Acc:  0.99833\n",
      "| Epoch:  891 / 1000  | Loss:  0.5546017289161682  | Acc:  0.99833\n",
      "| Epoch:  901 / 1000  | Loss:  0.5545914173126221  | Acc:  0.99833\n",
      "tensor(0.9717, device='mps:0')\n",
      "| Epoch:  911 / 1000  | Loss:  0.5545821189880371  | Acc:  0.99833\n",
      "| Epoch:  921 / 1000  | Loss:  0.5545726418495178  | Acc:  0.99833\n",
      "| Epoch:  931 / 1000  | Loss:  0.554563581943512  | Acc:  0.99833\n",
      "| Epoch:  941 / 1000  | Loss:  0.5545545816421509  | Acc:  0.99833\n",
      "| Epoch:  951 / 1000  | Loss:  0.5545458793640137  | Acc:  0.99833\n",
      "| Epoch:  961 / 1000  | Loss:  0.5545374155044556  | Acc:  0.99833\n",
      "| Epoch:  971 / 1000  | Loss:  0.5545288324356079  | Acc:  0.99833\n",
      "| Epoch:  981 / 1000  | Loss:  0.5545203685760498  | Acc:  0.99833\n",
      "| Epoch:  991 / 1000  | Loss:  0.5545123219490051  | Acc:  0.99833\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "`fast_epoch`: number of epoch to run the fast tuning\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "fast_epoch = 1000\n",
    "DPS = MNISTClassifier(dg = dg, nk = nk, nm = nm, nbl = nl, dropout = 0.0, Fout = len(target_num), bias = True).to(device)\n",
    "DPS.load_state_dict(torch.load('./MNIST'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk), weights_only = True))\n",
    "\n",
    "lr_ft = 1e-1\n",
    "optimizer = torch.optim.Adam(DPS.parameters(), lr=lr_ft)\n",
    "\n",
    "for t in range(fast_epoch):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "    pyb_af = DPS(X_train)\n",
    "    loss = criterion(pyb_af, y_train)\n",
    "    \n",
    "    for l in range(nl):\n",
    "        block = getattr(DPS.classifier[1].model, f'block_{l}')\n",
    "        W = getattr(block.block.BSL, 'control_p')\n",
    "        D = diag_mat_weights(W.size()[0]).to(device)\n",
    "        loss += BestLambda[l].to(device)/X_train.size()[0] * torch.norm(D@W)\n",
    "    \n",
    "    prediction = torch.argmax(pyb_af, axis = 1)\n",
    "    acc = (torch.argmax(pyb_af, axis = 1) == y_train).sum()/len(y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        print('| Epoch: ',t+1,'/',str(fast_epoch),' | Loss: ', loss.item(),' | Acc: ', np.round(acc.item(), 5))\n",
    "        if t % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                print((torch.argmax(DPS(X_test).detach(), axis = 1) == y_test).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd8e860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e2a6ffe-271b-47db-bc60-378ae3188a13",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2004a12-fe71-40a5-a3ce-517f35e8b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze all layers (optional, you can fine-tune entire network if needed)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(target_num))\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# --- Loss and Optimizer ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_ft.fc.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b947d78-f6fc-4faf-8eae-57f1c80a88ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch:  1 / 300  | Loss:  1.4675  | Acc:  0.23375000059604645\n",
      "tensor(0.2500, device='mps:0')\n",
      "| Epoch:  11 / 300  | Loss:  32.0567  | Acc:  0.2537499964237213\n",
      "| Epoch:  21 / 300  | Loss:  5.7897  | Acc:  0.6812499761581421\n",
      "| Epoch:  31 / 300  | Loss:  3.6778  | Acc:  0.7612500190734863\n",
      "| Epoch:  41 / 300  | Loss:  1.7413  | Acc:  0.8337500095367432\n",
      "| Epoch:  51 / 300  | Loss:  0.4275  | Acc:  0.9399999976158142\n",
      "| Epoch:  61 / 300  | Loss:  0.1492  | Acc:  0.9649999737739563\n",
      "| Epoch:  71 / 300  | Loss:  0.0214  | Acc:  0.9912499785423279\n",
      "| Epoch:  81 / 300  | Loss:  0.0033  | Acc:  1.0\n",
      "| Epoch:  91 / 300  | Loss:  0.0016  | Acc:  1.0\n",
      "| Epoch:  101 / 300  | Loss:  0.001  | Acc:  1.0\n",
      "tensor(0.6400, device='mps:0')\n",
      "| Epoch:  111 / 300  | Loss:  0.0009  | Acc:  1.0\n",
      "| Epoch:  121 / 300  | Loss:  0.0007  | Acc:  1.0\n",
      "| Epoch:  131 / 300  | Loss:  0.0007  | Acc:  1.0\n",
      "| Epoch:  141 / 300  | Loss:  0.0006  | Acc:  1.0\n",
      "| Epoch:  151 / 300  | Loss:  0.0006  | Acc:  1.0\n",
      "| Epoch:  161 / 300  | Loss:  0.0005  | Acc:  1.0\n",
      "| Epoch:  171 / 300  | Loss:  0.0005  | Acc:  1.0\n",
      "| Epoch:  181 / 300  | Loss:  0.0005  | Acc:  1.0\n",
      "| Epoch:  191 / 300  | Loss:  0.0004  | Acc:  1.0\n",
      "| Epoch:  201 / 300  | Loss:  0.0004  | Acc:  1.0\n",
      "tensor(0.6413, device='mps:0')\n",
      "| Epoch:  211 / 300  | Loss:  0.0004  | Acc:  1.0\n",
      "| Epoch:  221 / 300  | Loss:  0.0004  | Acc:  1.0\n",
      "| Epoch:  231 / 300  | Loss:  0.0004  | Acc:  1.0\n",
      "| Epoch:  241 / 300  | Loss:  0.0003  | Acc:  1.0\n",
      "| Epoch:  251 / 300  | Loss:  0.0003  | Acc:  1.0\n",
      "| Epoch:  261 / 300  | Loss:  0.0003  | Acc:  1.0\n",
      "| Epoch:  271 / 300  | Loss:  0.0003  | Acc:  1.0\n",
      "| Epoch:  281 / 300  | Loss:  0.0003  | Acc:  1.0\n",
      "| Epoch:  291 / 300  | Loss:  0.0003  | Acc:  1.0\n"
     ]
    }
   ],
   "source": [
    "for t in range(Iteration):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "    pyb_af = model_ft(X_train)\n",
    "    loss = criterion(pyb_af, y_train)\n",
    "    \n",
    "    prediction = torch.argmax(pyb_af, axis = 1)\n",
    "    acc = (torch.argmax(pyb_af, axis = 1) == y_train).sum()/len(y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(t % 10 == 0):\n",
    "        print('| Epoch: ',t+1,'/',str(Iteration),' | Loss: ', np.round(loss.item(), 4),' | Acc: ', acc.item())\n",
    "        if(t % 100 == 0):\n",
    "            with torch.no_grad():\n",
    "                print((torch.argmax(model_ft(X_test).detach(), axis = 1) == y_test).sum()/len(y_test))\n",
    "\n",
    "#torch.save(mnist_DeepBS.state_dict(), './MNIST'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0f57512-5a70-4666-a6b2-3fd4916e6ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6400, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print((torch.argmax(model_ft(X_test).detach(), axis = 1) == y_test).sum()/len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77707dfe-9f7f-4341-b372-b4e027e7d791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 2, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0,\n",
       "        3, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 2, 3, 0, 0, 3, 0, 2, 0, 0,\n",
       "        3, 0, 1, 0, 2, 0, 0, 2, 0, 3, 0, 2, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0,\n",
       "        0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 3, 0, 2, 0, 2, 0,\n",
       "        2, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 3, 2, 2, 0, 2, 0, 0, 0, 2, 0, 2, 3, 0,\n",
       "        0, 0, 1, 0, 3, 0, 0, 0, 0, 2, 1, 3, 2, 0, 0, 2, 1, 2, 2, 3, 0, 0, 0, 2,\n",
       "        0, 0, 0, 0, 0, 3, 0, 2, 0, 2, 0, 1, 3, 0, 0, 2, 0, 2, 0, 0, 3, 0, 2, 0,\n",
       "        2, 2, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 2,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1,\n",
       "        1, 1, 2, 1, 3, 1, 1, 0, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
       "        1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 3, 3, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1,\n",
       "        1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 3, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 0, 1, 0, 1, 2, 1, 3, 3, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 3,\n",
       "        1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 0, 1, 0, 2, 2, 3,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2, 2,\n",
       "        3, 2, 0, 3, 2, 0, 3, 2, 2, 2, 0, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2,\n",
       "        3, 2, 2, 2, 2, 0, 2, 0, 3, 2, 3, 2, 3, 2, 2, 2, 1, 2, 2, 3, 0, 0, 0, 3,\n",
       "        0, 1, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 0, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2,\n",
       "        2, 3, 2, 3, 2, 0, 3, 2, 2, 0, 0, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3,\n",
       "        2, 3, 2, 2, 2, 2, 3, 2, 1, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 0, 2, 2,\n",
       "        2, 1, 2, 2, 3, 2, 3, 0, 3, 2, 1, 2, 0, 3, 2, 1, 2, 3, 0, 0, 2, 3, 2, 3,\n",
       "        2, 0, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 0, 2, 3, 2, 2, 2, 2, 2, 2, 2, 1, 2,\n",
       "        3, 3, 2, 2, 0, 3, 2, 2, 3, 3, 2, 2, 3, 2, 0, 1, 1, 0, 3, 0, 1, 3, 2, 3,\n",
       "        3, 1, 0, 3, 3, 2, 1, 3, 2, 0, 0, 3, 3, 3, 3, 2, 3, 2, 0, 0, 2, 2, 2, 1,\n",
       "        3, 1, 3, 2, 3, 3, 2, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 1, 1, 3,\n",
       "        3, 0, 0, 1, 2, 3, 1, 0, 3, 2, 0, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3,\n",
       "        3, 3, 2, 3, 2, 2, 2, 1, 3, 3, 3, 3, 0, 2, 2, 3, 3, 2, 3, 3, 0, 3, 3, 3,\n",
       "        0, 2, 2, 3, 3, 3, 3, 3, 0, 3, 3, 2, 2, 2, 3, 3, 2, 1, 3, 3, 2, 3, 0, 1,\n",
       "        3, 3, 3, 2, 1, 3, 3, 2, 2, 2, 3, 0, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3,\n",
       "        2, 3, 0, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 0], device='mps:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model_ft(X_test).detach(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4926dcc8-d390-4767-a985-c04798206781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.], device='mps:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Training Loop ---\n",
    "def train_model(model, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f'Test Accuracy: {acc:.4f}')\n",
    "\n",
    "# --- Run Training and Evaluation ---\n",
    "#train_model(model, criterion, optimizer, num_epochs)\n",
    "#evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77191f2d-0186-4942-89f1-679fe15bdf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
