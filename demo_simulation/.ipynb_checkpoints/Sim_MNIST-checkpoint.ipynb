{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78638a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import glob\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce2513e-3f5f-4b62-9dcb-d786cf7a0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag_mat_weights(dimp, type = 'first'):\n",
    "    if type == 'first':\n",
    "        dg = np.zeros((dimp-1, dimp))\n",
    "        for i in range(dimp-1):\n",
    "            dg[i,i] = -1\n",
    "            dg[i,i+1]= 1\n",
    "    elif type == 'second':\n",
    "        dg = np.zeros((dimp-2, dimp))\n",
    "        for i in range(dimp-2):\n",
    "            dg[i,i] = -1\n",
    "            dg[i,i+1]= 2\n",
    "            dg[i,i+2]= -1\n",
    "    else:\n",
    "        pass\n",
    "    return torch.Tensor(dg)\n",
    "    \n",
    "\n",
    "class BSL(nn.Module):\n",
    "    def __init__(self, degree, num_knots, num_neurons, bias = True):\n",
    "        super(BSL, self).__init__()\n",
    "        self.degree = degree\n",
    "        self.num_knots = num_knots\n",
    "        self.num_neurons = num_neurons\n",
    "        self.control_p = nn.Parameter(torch.randn(self.num_knots, self.num_neurons))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(self.num_neurons))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.inter = {}\n",
    "    \n",
    "    def basis_function(self, x, i, k, t):\n",
    "    \n",
    "        # Base case: degree 0 spline\n",
    "        if k == 0:\n",
    "            return ((t[i] <= x) & (x < t[i + 1])).float()\n",
    "    \n",
    "        # Recursive case\n",
    "        denom1 = t[i + k] - t[i]\n",
    "        denom2 = t[i + k + 1] - t[i + 1]\n",
    "    \n",
    "        term1 = 0\n",
    "        if denom1 != 0:\n",
    "            term1 = (x - t[i]) / denom1 * self.basis_function(x, i, k - 1, t)\n",
    "    \n",
    "        term2 = 0\n",
    "        if denom2 != 0:\n",
    "            term2 = (t[i + k + 1] - x) / denom2 * self.basis_function(x, i + 1, k - 1, t)\n",
    "    \n",
    "        return term1 + term2\n",
    "\n",
    "    def knots_distribution(self, dg, nk):\n",
    "\n",
    "        knots = torch.cat([torch.linspace(-0.002, -0.001, steps=dg),            # Add repeated values at the start for clamping\n",
    "            torch.linspace(0, 1, nk-2*dg-2),  # Uniform knot spacing in the middle\n",
    "            torch.linspace(1.001, 1.002, steps=dg)           # Add repeated values at the end for clamping\n",
    "            ]).view(-1,1)\n",
    "        \n",
    "        knots = torch.cat([torch.linspace(0, 1, nk-2)          # Add repeated values at the end for clamping\n",
    "            ]).view(-1,1)\n",
    "\n",
    "        return knots\n",
    "    \n",
    "    def basis_function2(self, x, spl):\n",
    "        basis_output = spl.fit_transform(x.cpu().numpy())\n",
    "        return basis_output\n",
    "            \n",
    "    def forward(self, x):\n",
    "        batch_size, num_features = x.size()\n",
    "        device = x.device\n",
    "        \n",
    "        # Create knot vector and apply B-spline basis functions for each feature\n",
    "    \n",
    "        basises = []\n",
    "        knots = self.knots_distribution(self.degree, self.num_knots)\n",
    "        spl = SplineTransformer(n_knots=self.num_knots, degree=self.degree, knots = knots)\n",
    "\n",
    "        \n",
    "        for feature in range(num_features):\n",
    "            # Calculate B-spline basis functions for this feature\n",
    "            \n",
    "            basis = self.basis_function2(x[:, feature].reshape(-1,1), spl)\n",
    "            basis = torch.Tensor(basis).to(device)\n",
    "            basises.append(basis)\n",
    "        \n",
    "        if num_features == 1:\n",
    "            tout = basises[0] @ self.control_p\n",
    "            self.inter['basic'] = basises[0].T\n",
    "        else:\n",
    "            self.inter['basic'] = torch.reshape(torch.stack(basises, dim = 1), (batch_size, self.num_knots * self.num_neurons)).T\n",
    "            basises = torch.stack(basises)\n",
    "            tout = basises.permute(1,2,0) * self.control_p\n",
    "            tout = tout.sum(dim =1)\n",
    "                \n",
    "        if self.bias is not None:\n",
    "            tout += self.bias        \n",
    "            \n",
    "        return tout\n",
    "\n",
    "\n",
    "class NormLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NormLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        min_val = torch.min(x, axis = 1).values.reshape(-1,1)\n",
    "        max_val = torch.max(x, axis = 1).values.reshape(-1,1)\n",
    "\n",
    "        x = (x - min_val)/(max_val - min_val)  # Rescale to [0, 1]\n",
    "        return x.detach()\n",
    "    \n",
    "class BSpline_block(nn.Module):\n",
    "    def __init__(self, degree, num_knots, num_neurons, dropout = 0.0, bias = True):\n",
    "        super(BSpline_block, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(OrderedDict([\n",
    "            ('norm', NormLayer()),\n",
    "            ('BSL', BSL(degree = degree, num_knots = num_knots, num_neurons = num_neurons, bias = bias)),\n",
    "            ('drop', nn.Dropout(dropout)),\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "        \n",
    "class StackBS_block(nn.Module):\n",
    "    def __init__(self, block, degree, num_knots, num_neurons, num_blocks, dropout = 0.0, bias = True):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleDict({\n",
    "            f'block_{i}': block(degree = degree, num_knots = num_knots, num_neurons = num_neurons)\n",
    "            for i in range(num_blocks)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        for name, block in self.model.items():\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3231c48-f7b3-4619-8cf8-4a7bdd4cf4be",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c7eef1-9f10-48e6-a6ec-fc1df9ac3a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        scheduler.step()\n",
    "        \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            output = torch.log_softmax(output, dim=1)\n",
    "            _, pred = torch.max(output, dim = 1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac7b108b-06e9-4891-b340-66fe802bfa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps:0\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Download the MNIST dataset\n",
    "train_dataset_full = datasets.MNIST(root='./data', train=True, download=True)\n",
    "test_dataset_full = datasets.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "# Function to extract N samples for a given digit\n",
    "def extract_n_samples(data, targets, digit, n):\n",
    "    indices = (targets == digit).nonzero().squeeze()\n",
    "    selected_indices = indices[:n]\n",
    "    return data[selected_indices], targets[selected_indices]\n",
    "\n",
    "data = train_dataset_full.data\n",
    "targets = train_dataset_full.targets\n",
    "\n",
    "# Extract 200 samples for each digit\n",
    "data_0, labels_0 = extract_n_samples(data, targets, digit=0, n=200)\n",
    "data_1, labels_1 = extract_n_samples(data, targets, digit=1, n=200)\n",
    "\n",
    "# Combine and apply transform manually\n",
    "X_train = torch.cat([data_0, data_1]).to(device)\n",
    "y_train = torch.cat([labels_0, labels_1]).to(device)\n",
    "\n",
    "X_train = X_train.unsqueeze(1).float() / 255.0\n",
    "X_train = (X_train - 0.1307) / 0.3081\n",
    "\n",
    "perm = torch.randperm(X_train.size(0))\n",
    "X_train = X_train[perm]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "data = test_dataset_full.data\n",
    "targets = test_dataset_full.targets\n",
    "\n",
    "# Extract 200 samples for each digit\n",
    "data_0, labels_0 = extract_n_samples(data, targets, digit=0, n=200)\n",
    "data_1, labels_1 = extract_n_samples(data, targets, digit=1, n=200)\n",
    "\n",
    "# Combine and apply transform manually\n",
    "X_test = torch.cat([data_0, data_1]).to(device)\n",
    "y_test = torch.cat([labels_0, labels_1]).to(device)\n",
    "\n",
    "X_test = X_test.unsqueeze(1).float() / 255.0\n",
    "X_test = (X_test - 0.1307) / 0.3081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b45c9015-c08b-4a47-8fea-ef000648222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self, dg, nk, nm, nbl, dropout, Fout, bias):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.gap = nn.Flatten()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1568, nm),\n",
    "            StackBS_block(BSpline_block, degree = dg, num_knots = nk, num_neurons = nm, num_blocks = nbl, dropout = dropout),\n",
    "            nn.Linear(nm, Fout))\n",
    "        self.sm = nn.Softmax(dim = 1)\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.classifier(x)\n",
    "        x = self.sm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_para_ecm(self, x):\n",
    "\n",
    "        '''\n",
    "        ecm_para: A dictionary that collects the parameter we need to the following ECM algorithm.\n",
    "        ecm_para.basic: Store the output of each B-Spline block; Dimension = [n_sample, n_neurons]\n",
    "        ecm_para.ebasic Store the weight matrix of each B-Spline expansion; Dimension = [n_knots * n_neurons, n_sample]\n",
    "\n",
    "        '''\n",
    "        ecm_para = {}\n",
    "        bs_block_out = {}\n",
    "        bs_spline_weight = {}\n",
    "        bs_spline_value = {}\n",
    "        bs_spline_bias = {}\n",
    "\n",
    "        _ = self(x)\n",
    "        \n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                bs_block_out[name] = output.detach()\n",
    "            return hook\n",
    "\n",
    "        handles = []\n",
    "        for name, layer in self.named_modules():\n",
    "            if 'block.drop' in name:\n",
    "                handles.append(layer.register_forward_hook(get_activation(name)))\n",
    "            elif 'block.BSL' in name:\n",
    "                bs_spline_value[name] = layer.inter['basic'].detach()\n",
    "                bs_spline_weight[name] = layer.control_p.detach()\n",
    "                bs_spline_bias[name] = layer.bias.detach()\n",
    "        # Run forward pass (triggers hooks)\n",
    "        _ = self(x)\n",
    "\n",
    "        # Clean up hooks\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "            \n",
    "        ecm_para['basic'] = torch.stack(list(bs_block_out.values()), dim=0)\n",
    "        ecm_para['ebasic'] = torch.stack(list(bs_spline_value.values()), dim=0)\n",
    "        ecm_para['wbasic'] = torch.stack(list(bs_spline_weight.values()), dim=0)\n",
    "        ecm_para['bbasic'] = torch.stack(list(bs_spline_bias.values()), dim=0)\n",
    "        del bs_block_out, bs_spline_weight, bs_spline_value, bs_spline_bias\n",
    "        \n",
    "        return ecm_para\n",
    "\n",
    "nm = 50; nk = 15; dg = 3; nl = 1\n",
    "mnist_DeepBS = MNISTClassifier(dg = dg, nk = nk, nm = nm, nbl = nl, dropout = 0.0, Fout = 2, bias = True).to(device)\n",
    "learning_r = 1e-2\n",
    "optimizer = torch.optim.Adam(mnist_DeepBS.parameters(), lr=learning_r)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd18a6-5a45-4860-98a7-2bf0e99b63ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "615a9942-375c-4965-a78e-d4bfcb5fd617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch:  1 / 100  | Loss:  0.7827  | Acc:  0.5\n",
      "tensor(0.4900, device='mps:0')\n",
      "| Epoch:  11 / 100  | Loss:  0.5086  | Acc:  0.875\n",
      "| Epoch:  21 / 100  | Loss:  0.3977  | Acc:  0.9725000262260437\n",
      "| Epoch:  31 / 100  | Loss:  0.3593  | Acc:  0.9850000143051147\n",
      "| Epoch:  41 / 100  | Loss:  0.3419  | Acc:  0.9950000047683716\n",
      "| Epoch:  51 / 100  | Loss:  0.3333  | Acc:  0.9950000047683716\n",
      "| Epoch:  61 / 100  | Loss:  0.3285  | Acc:  0.9950000047683716\n",
      "| Epoch:  71 / 100  | Loss:  0.3255  | Acc:  0.9975000023841858\n",
      "| Epoch:  81 / 100  | Loss:  0.3234  | Acc:  0.9975000023841858\n",
      "| Epoch:  91 / 100  | Loss:  0.3219  | Acc:  0.9975000023841858\n"
     ]
    }
   ],
   "source": [
    "for t in range(Iteration):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "    pyb_af = mnist_DeepBS(X_train)\n",
    "    loss = criterion(pyb_af, y_train)\n",
    "    \n",
    "    prediction = torch.argmax(pyb_af, axis = 1)\n",
    "    acc = (torch.argmax(pyb_af, axis = 1) == y_train).sum()/len(y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(t % 10 == 0):\n",
    "        print('| Epoch: ',t+1,'/',str(Iteration),' | Loss: ', np.round(loss.item(), 4),' | Acc: ', acc.item())\n",
    "        if(t % 100 == 0):\n",
    "            with torch.no_grad():\n",
    "                print((torch.argmax(mnist_DeepBS(X_test).detach(), axis = 1) == y_test).sum()/len(y_test))\n",
    "\n",
    "torch.save(mnist_DeepBS.state_dict(), './MNIST'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e9ab7-3e4b-4843-b9e5-db86392f56c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2a1697c-2a51-44e0-b467-cd4aecd081d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  tensor(399, device='mps:0')\n",
      "Accuracy:  tensor(397, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "eval_model = MNISTClassifier(dg = dg, nk = nk, nm = nm, nbl = nl, dropout = 0.0, Fout = 2, bias = True).to(device)\n",
    "eval_model.load_state_dict(torch.load('./MNIST'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk), weights_only = True))\n",
    "eval_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_postecm = eval_model(X_train)\n",
    "    print('Accuracy: ', (torch.argmax(pred_postecm, axis = 1) == y_train).sum())\n",
    "    pred_postecm = eval_model(X_test)\n",
    "    print('Accuracy: ', (torch.argmax(pred_postecm, axis = 1) == y_test).sum())\n",
    "    \n",
    "    CLoss = criterion(pred_postecm.detach(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82dfaedd-becf-4ab8-9419-809e72c05d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ECM(par, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4):\n",
    "    lambdab = initial_lambda\n",
    "    sigma = initial_sigma\n",
    "    xi = initial_xi\n",
    "    \n",
    "    n_block, num_knots, num_neurons = par['wbasic'].size()\n",
    "    ls_lambda = torch.empty(n_block)\n",
    "    \n",
    "    for l in range(n_block):\n",
    "        B = par['ebasic'][l]\n",
    "        By = par['basic'][l]\n",
    "        WB = par['wbasic'][l]\n",
    "        \n",
    "        DB = diag_mat_weights(WB.size()[0]).to(device)\n",
    "        size = B.size()[1]\n",
    "        S = DB.T @ DB\n",
    "        Cov_a = (xi**2)* torch.linalg.pinv(S)\n",
    "        Cov_a.to(device)\n",
    "        Cov_e = (torch.eye(size*num_neurons)* sigma).to(device)\n",
    "        \n",
    "        block_y = torch.reshape(By, (-1,1))\n",
    "        flatB = B.view(num_neurons, num_knots, size)\n",
    "            \n",
    "        sqr_xi= 0\n",
    "        sqr_sig = 0\n",
    "\n",
    "        for i in range(num_neurons):\n",
    "            Ncov = (Cov_a -(Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)]) @ flatB[i].T @ Cov_a))\n",
    "            Nmu = (Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)])) @ By[:,i].reshape(-1,1)\n",
    "            \n",
    "            first_xi = S @ Ncov\n",
    "            second_xi = (Nmu.T @ S @ Nmu)\n",
    "            sqr_xi += torch.trace(first_xi) + second_xi\n",
    "                \n",
    "            first_sig = torch.norm(By[:,i])\n",
    "            second_sig = 2 * (By[:,i] @ flatB[i].T) @ Nmu \n",
    "            third_sig = torch.trace((flatB[i] @ flatB[i].T) @ Ncov)\n",
    "            four_sig = (Nmu.T @ flatB[i] @ flatB[i].T @ Nmu)\n",
    "            \n",
    "            sqr_sig += (first_sig + second_sig + third_sig + four_sig)\n",
    "            \n",
    "            del first_xi, second_xi, first_sig, second_sig, third_sig, four_sig\n",
    "\n",
    "        sqr_xi /= num_neurons\n",
    "        sqr_sig /= (num_neurons*size)\n",
    "\n",
    "        ls_lambda[l] = (sqr_sig/sqr_xi).item()\n",
    "        \n",
    "        del Cov_a, Cov_e, flatB, B, By, WB\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return ls_lambda\n",
    "    \n",
    "def ECM_layersise_update(model, par, Lambda, x, y):\n",
    "\n",
    "    model.eval()\n",
    "    device = x.device\n",
    "    \n",
    "    B_out, B_in, B_w, B_b = par['basic'], par['ebasic'], par['wbasic'], par['bbasic']\n",
    "    n_layer, nk, nm = B_w.size()\n",
    "    DB = diag_mat_weights(B_w[0].size()[0], 'second').to(device)\n",
    "\n",
    "    Project_matrix = (torch.linalg.pinv(B_in[-1].T @ B_in[-1]) @ B_in[-1].T @ B_in[-1])\n",
    "    Size = [b.size()[1] for b in B_in]\n",
    "\n",
    "    B_in = B_in.view(n_layer, nm, nk, Size[0])\n",
    "\n",
    "    for l in range(n_layer):    \n",
    "        NW = torch.empty((nk, nm)).to(device)\n",
    "        NB = torch.empty((nm)).to(device)\n",
    "        \n",
    "        for i in range(nm):\n",
    "            B1y = B_out[l][:,i] - B_b[l][i]\n",
    "            BB = B_in[l][i].T\n",
    "    \n",
    "            # Update the weights and bias\n",
    "            NW[:, i] = (torch.inverse(BB.T @ BB + (Lambda[l]/Size[l]) * (DB.T @ DB)) @ BB.T @ B1y)\n",
    "            NB[i] = torch.mean(B_out[l][:,i] - (NW[:,i] @ BB.T))\n",
    "                \n",
    "        # update the weight\n",
    "        block = getattr(model.classifier[1].model, f'block_{l}')\n",
    "        getattr(block.block.BSL, 'control_p').data = NW\n",
    "        getattr(block.block.BSL, 'bias').data = NB\n",
    "\n",
    "        del NW, NB, B1y, BB, block\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        DPSy = model(x)\n",
    "        CLoss = criterion(DPSy.detach(), y)\n",
    "        GCV = CLoss/(Size[-1]-torch.trace(Project_matrix))\n",
    "    \n",
    "    return model, GCV\n",
    "\n",
    "def ECM_update(model, max_iter, x, y):\n",
    "    BestGCV = prev = 9999\n",
    "    patient = 10\n",
    "    pcount = 0\n",
    "    for i in range(max_iter):\n",
    "        _ = model(X_train)\n",
    "        ECM_para = model.get_para_ecm(x)\n",
    "        ECM_Lambda = ECM(ECM_para, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4)\n",
    "\n",
    "        model, GCV = ECM_layersise_update(model, ECM_para, ECM_Lambda, x, y)\n",
    "        if np.abs(prev - GCV.cpu().detach().numpy()) < 5e-5:\n",
    "            print('GCV Converge at ',i+1,' iteration')\n",
    "            break\n",
    "            \n",
    "        if GCV < BestGCV:\n",
    "            BestLambda = ECM_Lambda\n",
    "            BestGCV = GCV\n",
    "            pcount = 0\n",
    "        else:\n",
    "            pcount += 1\n",
    "\n",
    "        if pcount == patient:\n",
    "            print('GCV Converge at ',i,' iteration')\n",
    "            break\n",
    "\n",
    "        prev = GCV.cpu().detach().numpy()\n",
    "\n",
    "        del ECM_para, ECM_Lambda\n",
    "        \n",
    "    del model\n",
    "    \n",
    "    return BestLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338dd381-a752-4830-b5bf-93fc66068870",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = MNISTClassifier(dg = dg, nk = nk, nm = nm, nbl = nl, dropout = 0.0, Fout = 2, bias = True).to(device)\n",
    "eval_model.load_state_dict(torch.load('./MNIST'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk), weights_only = True))\n",
    "eval_model.eval()\n",
    "with torch.no_grad():\n",
    "    BestLambda = ECM_update(eval_model, 10, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f20ef1a-c2f0-4e8a-984c-d0d016afc0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch:  1 / 200  | Loss:  0.34722980856895447  | Acc:  0.9975\n",
      "tensor(0.9950, device='mps:0')\n",
      "| Epoch:  11 / 200  | Loss:  0.342163622379303  | Acc:  0.9975\n",
      "| Epoch:  21 / 200  | Loss:  0.3391888737678528  | Acc:  0.9975\n",
      "| Epoch:  31 / 200  | Loss:  0.3366234600543976  | Acc:  0.9975\n",
      "| Epoch:  41 / 200  | Loss:  0.3333474099636078  | Acc:  1.0\n",
      "| Epoch:  51 / 200  | Loss:  0.3313204050064087  | Acc:  1.0\n",
      "| Epoch:  61 / 200  | Loss:  0.329637348651886  | Acc:  1.0\n",
      "| Epoch:  71 / 200  | Loss:  0.3281386196613312  | Acc:  1.0\n",
      "| Epoch:  81 / 200  | Loss:  0.3267662525177002  | Acc:  1.0\n",
      "| Epoch:  91 / 200  | Loss:  0.3255162835121155  | Acc:  1.0\n",
      "| Epoch:  101 / 200  | Loss:  0.32437586784362793  | Acc:  1.0\n",
      "tensor(0.9925, device='mps:0')\n",
      "| Epoch:  111 / 200  | Loss:  0.32334110140800476  | Acc:  1.0\n",
      "| Epoch:  121 / 200  | Loss:  0.32240742444992065  | Acc:  1.0\n",
      "| Epoch:  131 / 200  | Loss:  0.32157114148139954  | Acc:  1.0\n",
      "| Epoch:  141 / 200  | Loss:  0.3208281993865967  | Acc:  1.0\n",
      "| Epoch:  151 / 200  | Loss:  0.3201737403869629  | Acc:  1.0\n",
      "| Epoch:  161 / 200  | Loss:  0.3196021318435669  | Acc:  1.0\n",
      "| Epoch:  171 / 200  | Loss:  0.3191065788269043  | Acc:  1.0\n",
      "| Epoch:  181 / 200  | Loss:  0.31867915391921997  | Acc:  1.0\n",
      "| Epoch:  191 / 200  | Loss:  0.3183114230632782  | Acc:  1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "`fast_epoch`: number of epoch to run the fast tuning\n",
    "\n",
    "\"\"\"\n",
    "fast_epoch = 200\n",
    "DPS = MNISTClassifier(dg = dg, nk = nk, nm = nm, nbl = nl, dropout = 0.0, Fout = 2, bias = True).to(device)\n",
    "DPS.load_state_dict(torch.load('./MNIST'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk), weights_only = True))\n",
    "\n",
    "lr_ft = 1e-2\n",
    "optimizer = torch.optim.Adam(DPS.parameters(), lr=lr_ft)\n",
    "\n",
    "for t in range(fast_epoch):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "    pyb_af = DPS(X_train)\n",
    "    loss = criterion(pyb_af, y_train)\n",
    "    \n",
    "    for l in range(nl):\n",
    "        block = getattr(DPS.classifier[1].model, f'block_{l}')\n",
    "        W = getattr(block.block.BSL, 'control_p')\n",
    "        D = diag_mat_weights(W.size()[0]).to(device)\n",
    "        loss += BestLambda[l].to(device)/X_train.size()[0] * torch.norm(D@W)\n",
    "    \n",
    "    prediction = torch.argmax(pyb_af, axis = 1)\n",
    "    acc = (torch.argmax(pyb_af, axis = 1) == y_train).sum()/len(y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        print('| Epoch: ',t+1,'/',str(fast_epoch),' | Loss: ', loss.item(),' | Acc: ', np.round(acc.item(), 5))\n",
    "        if t % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                print((torch.argmax(DPS(X_test).detach(), axis = 1) == y_test).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd8e860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2164fea2-142e-4abd-a295-0833b5664603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
