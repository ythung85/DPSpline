{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78638a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import glob\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6fe4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_brain(imgdir, w, h):\n",
    "\n",
    "    WIDTH, HEIGHT = w, h\n",
    "    \n",
    "    x = []\n",
    "    for i in range(len(imgdir)):\n",
    "        # Read and resize image\n",
    "        full_size_image = cv2.imread(imgdir[i])\n",
    "        x.append(cv2.resize(full_size_image, (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC)/255.0) \n",
    "\n",
    "    return x\n",
    "\n",
    "def diag_mat_weights(dimp, type = 'first'):\n",
    "    if type == 'first':\n",
    "        dg = np.zeros((dimp-1, dimp))\n",
    "        for i in range(dimp-1):\n",
    "            dg[i,i] = -1\n",
    "            dg[i,i+1]= 1\n",
    "    elif type == 'second':\n",
    "        dg = np.zeros((dimp-2, dimp))\n",
    "        for i in range(dimp-2):\n",
    "            dg[i,i] = -1\n",
    "            dg[i,i+1]= 2\n",
    "            dg[i,i+2]= -1\n",
    "    else:\n",
    "        pass\n",
    "    return torch.Tensor(dg)\n",
    "    \n",
    "class PRODBSplineLayerMultiFeature(nn.Module):\n",
    "    def __init__(self, input_dim, degree, num_knots, output_dim, num_neurons, bias = True):\n",
    "        super(PRODBSplineLayerMultiFeature, self).__init__()\n",
    "        self.degree = degree\n",
    "        self.num_knots = num_knots\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_neurons = num_neurons\n",
    "        \n",
    "        if input_dim == 2:\n",
    "            self.control_p = nn.Parameter(torch.randn(self.num_knots**2, self.output_dim))\n",
    "        else:\n",
    "            self.control_p = nn.Parameter(torch.randn(self.num_knots, self.num_neurons))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(self.num_neurons))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.inter = {}\n",
    "    \n",
    "    def basis_function(self, x, i, k, t):\n",
    "    \n",
    "        # Base case: degree 0 spline\n",
    "        if k == 0:\n",
    "            return ((t[i] <= x) & (x < t[i + 1])).float()\n",
    "    \n",
    "        # Recursive case\n",
    "        denom1 = t[i + k] - t[i]\n",
    "        denom2 = t[i + k + 1] - t[i + 1]\n",
    "    \n",
    "        term1 = 0\n",
    "        if denom1 != 0:\n",
    "            term1 = (x - t[i]) / denom1 * self.basis_function(x, i, k - 1, t)\n",
    "    \n",
    "        term2 = 0\n",
    "        if denom2 != 0:\n",
    "            term2 = (t[i + k + 1] - x) / denom2 * self.basis_function(x, i + 1, k - 1, t)\n",
    "    \n",
    "        return term1 + term2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, num_features = x.size()\n",
    "        device = x.device\n",
    "        \n",
    "        # Create knot vector\n",
    "        # knots = torch.linspace(0, 1, self.num_knots + self.degree + 1).to(device)\n",
    "        knots = torch.cat([\n",
    "                        torch.zeros(self.degree),               # Add repeated values at the start for clamping\n",
    "                        torch.linspace(0, 1, self.num_knots - self.degree + 1),  # Uniform knot spacing in the middle\n",
    "                        torch.ones(self.degree)                 # Add repeated values at the end for clamping\n",
    "                    ]).to(device)\n",
    "        # Apply B-spline basis functions for each feature\n",
    "        basises = []\n",
    "    \n",
    "        \n",
    "        for feature in range(num_features):\n",
    "            # Calculate B-spline basis functions for this feature\n",
    "            basis = torch.stack([self.basis_function(x[:, feature], i, self.degree, knots) \n",
    "                                 for i in range(self.num_knots)], dim=-1)\n",
    "            basises.append(basis)\n",
    "            \n",
    "        \n",
    "        if num_features == 1:\n",
    "            tout = basises[0] @ self.control_p\n",
    "            self.inter['basic'] = basises[0].T\n",
    "        else:\n",
    "            self.inter['basic'] = torch.reshape(torch.stack(basises, dim = 1), (batch_size, self.num_knots * self.num_neurons)).T\n",
    "            basises = torch.stack(basises)\n",
    "            tout = basises.permute(1,2,0) * self.control_p\n",
    "            tout = tout.sum(dim =1)\n",
    "                \n",
    "        if self.bias is not None:\n",
    "            tout += self.bias        \n",
    "            \n",
    "        return tout\n",
    "        \n",
    "class NormLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NormLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        min_val = torch.min(x, axis = 1).values.reshape(-1,1)\n",
    "        max_val = torch.max(x, axis = 1).values.reshape(-1,1)\n",
    "\n",
    "        x = (x - min_val)/(max_val - min_val)  # Rescale to [0, 1]\n",
    "        return x.detach()\n",
    "        \n",
    "def ECM(model, num_neurons, num_knots, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4, L = None):\n",
    "    lambdab = initial_lambda\n",
    "    sigma = initial_sigma\n",
    "    xi = initial_xi\n",
    "\n",
    "    if L == 1:\n",
    "        B = model.inter['ebasic']\n",
    "        By = model.inter['basic']\n",
    "        WB = model.sp1.control_p\n",
    "    else:\n",
    "        B = model.inter['ebasic2']\n",
    "        By = model.inter['basic2']\n",
    "        WB = model.sp2.control_p\n",
    "        \n",
    "    DB = diag_mat_weights(WB.size()[0]).to(device)\n",
    "    size = B.size()[1]\n",
    "    S = DB.T @ DB\n",
    "    Cov_a = (xi**2)* torch.linalg.pinv(S)\n",
    "    Cov_e = torch.eye(size*num_neurons)* sigma\n",
    "    \n",
    "    block_y = torch.reshape(By, (-1,1))\n",
    "    flatB = B.view(num_neurons, num_knots, size)\n",
    "        \n",
    "    sqr_xi= 0\n",
    "    sqr_sig = 0\n",
    "    \n",
    "    for i in range(num_neurons):\n",
    "        Ncov = (Cov_a -(Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)]) @ flatB[i].T @ Cov_a))\n",
    "        Nmu = (Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)])) @ By[:,i].reshape(-1,1)\n",
    "        \n",
    "        first_xi = S @ Ncov\n",
    "        second_xi = (Nmu.T @ S @ Nmu)\n",
    "        sqr_xi += torch.trace(first_xi) + second_xi\n",
    "            \n",
    "        first_sig = torch.norm(By[:,i])\n",
    "        second_sig = 2 * (By[:,i] @ flatB[i].T) @ Nmu \n",
    "        third_sig = torch.trace((flatB[i] @ flatB[i].T) @ Ncov)\n",
    "        four_sig = (Nmu.T @ flatB[i] @ flatB[i].T @ Nmu)\n",
    "        \n",
    "        sqr_sig += (first_sig + second_sig + third_sig + four_sig)\n",
    "    \n",
    "    sqr_xi /= num_neurons\n",
    "    sqr_sig /= (num_neurons*size)\n",
    "    \n",
    "    Lambda = sqr_sig/sqr_xi\n",
    "    \n",
    "    return Lambda.item()\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9966f6c2-493c-4777-ab80-510d7e2e107c",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "In this section, we will load the brain tumor MRI Images and resize it to 224x224 pixels and ensure the images are in gray scale for numerical stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44890839-b950-4217-8d23-9e8febc60d7a",
   "metadata": {},
   "source": [
    "## Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c21c26e0-ea82-4404-a9c6-9abec70af359",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = glob.glob('/Users/a080528/Downloads/chest_xray/train/PNEUMONIA/*.jpeg')\n",
    "train2 = glob.glob('/Users/a080528/Downloads/chest_xray/train/NORMAL/*.jpeg')\n",
    "\n",
    "test1 = glob.glob('/Users/a080528/Downloads/chest_xray/test/PNEUMONIA/*.jpeg')\n",
    "test2 = glob.glob('/Users/a080528/Downloads/chest_xray/test/NORMAL/*.jpeg')\n",
    "\n",
    "train = [train1, train2]; test = [test1, test2]\n",
    "\n",
    "trainx = []\n",
    "for f in train:\n",
    "    x = proc_brain(f, 224, 224)\n",
    "    trainx.append(x)\n",
    "\n",
    "testx = []\n",
    "for f in test:\n",
    "    x = proc_brain(f, 224, 224)\n",
    "    testx.append(x)\n",
    "\n",
    "Xtraind = np.concatenate((np.array(trainx[0]), np.array(trainx[1])))\n",
    "y_train = np.array([1]*len(trainx[0]))\n",
    "y_train = np.concatenate((y_train, [0]*len(trainx[1])))\n",
    "Xtestd = np.concatenate((np.array(testx[0]), np.array(testx[1])))\n",
    "y_test = np.array([1]*len(testx[0]))\n",
    "y_test = np.concatenate((y_test, [0]*len(testx[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc075de-dc73-45e1-aa79-89da50ad9398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdb20553-10c7-4420-bda9-9458921021c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Training) Number of glioma image: 1316 |  Number of non-tumor image: 3900 \n",
      " (Testing) Number of glioma image: 248 |  Number of non-tumor image: 376 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this block, you can customize the number of training size and testing size. According to your setting, we will \n",
    "randomly select the assigned number from image dataset. In this demo, we randomly select 500 images and 300 images\n",
    "from training and testing dataset respectively. Besides, in order to fulfill the requirement of Pytorch, we need to\n",
    "change the data to suitable type.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "trainsize = len(Xtraind); testsize = len(Xtestd)\n",
    "\n",
    "np.random.seed(42)\n",
    "trainid = np.random.choice(len(Xtraind), trainsize)\n",
    "testid = np.random.choice(len(Xtestd), testsize)\n",
    "\n",
    "print(f\"(Training) Number of glioma image: {Counter(y_train[trainid])[0]} |  Number of non-tumor image: {Counter(y_train[trainid])[1]} \")\n",
    "print(f\" (Testing) Number of glioma image: {Counter(y_test[testid])[0]} |  Number of non-tumor image: {Counter(y_test[testid])[1]} \")\n",
    "\n",
    "X_train = torch.Tensor(Xtraind[trainid]).permute(0, 3, 1, 2); y_train = torch.Tensor(y_train[trainid]).type(torch.LongTensor)\n",
    "X_test = torch.Tensor(Xtestd[testid]).permute(0, 3, 1, 2); y_test = torch.Tensor(y_test[testid]).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c99d121-5218-4a35-b17b-1b677bc12681",
   "metadata": {},
   "source": [
    "## Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2821748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = glob.glob('/Users/a080528/Downloads/BrainTumor/Training/glioma/*.jpg')\n",
    "train2 = glob.glob('/Users/a080528/Downloads/BrainTumor/Training/notumor/*.jpg')\n",
    "test1 = glob.glob('/Users/a080528/Downloads/BrainTumor/Testing/glioma/*.jpg')\n",
    "test2 = glob.glob('/Users/a080528/Downloads/BrainTumor/Testing/notumor/*.jpg')\n",
    "\n",
    "train = [train1, train2]; test = [test1, test2]\n",
    "\n",
    "trainx = []\n",
    "for f in train:\n",
    "    x = proc_brain(f, 224, 224)\n",
    "    trainx.append(x)\n",
    "\n",
    "testx = []\n",
    "for f in test:\n",
    "    x = proc_brain(f, 224, 224)\n",
    "    testx.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c2c19a-cc6b-489d-9f52-3734e1d881df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtraind = np.concatenate((np.array(trainx[0]), np.array(trainx[1])))\n",
    "y_train = np.array([1]*len(trainx[0]))\n",
    "y_train = np.concatenate((y_train, [0]*len(trainx[1])))\n",
    "Xtestd = np.concatenate((np.array(testx[0]), np.array(testx[1])))\n",
    "y_test = np.array([1]*len(testx[0]))\n",
    "y_test = np.concatenate((y_test, [0]*len(testx[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab838770-3743-40ce-9b1b-8fd7575ab25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Training) Number of glioma image: 274 |  Number of non-tumor image: 226 \n",
      " (Testing) Number of glioma image: 172 |  Number of non-tumor image: 128 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this block, you can customize the number of training size and testing size. According to your setting, we will \n",
    "randomly select the assigned number from image dataset. In this demo, we randomly select 500 images and 300 images\n",
    "from training and testing dataset respectively. Besides, in order to fulfill the requirement of Pytorch, we need to\n",
    "change the data to suitable type.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "trainsize = 500; testsize = 300\n",
    "\n",
    "np.random.seed(123)\n",
    "trainid = np.random.choice(len(Xtraind), trainsize)\n",
    "testid = np.random.choice(len(Xtestd), testsize)\n",
    "\n",
    "print(f\"(Training) Number of glioma image: {Counter(y_train[trainid])[0]} |  Number of non-tumor image: {Counter(y_train[trainid])[1]} \")\n",
    "print(f\" (Testing) Number of glioma image: {Counter(y_test[testid])[0]} |  Number of non-tumor image: {Counter(y_test[testid])[1]} \")\n",
    "\n",
    "X_train = torch.Tensor(Xtraind[trainid]).permute(0, 3, 1, 2); y_train = torch.Tensor(y_train[trainid]).type(torch.LongTensor)\n",
    "X_test = torch.Tensor(Xtestd[testid]).permute(0, 3, 1, 2); y_test = torch.Tensor(y_test[testid]).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3ec4c1-0d38-469f-a71d-774afab3424d",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e2b19",
   "metadata": {},
   "source": [
    "## CNDNN-S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf8374-370b-460b-8884-6c46511663ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ca85f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNS1(nn.Module):\n",
    "    def __init__(self, input_dim, degree, num_knots, num_neurons, output_dim, bias):\n",
    "        super(DNNS1, self).__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_knots = num_knots\n",
    "        self.ln1 = nn.Linear(input_dim, num_neurons)\n",
    "        self.nm1 = NormLayer() \n",
    "        self.sp1 = PRODBSplineLayerMultiFeature(input_dim = 1, degree = degree, num_knots = num_knots, num_neurons = num_neurons, output_dim= output_dim, bias = True)\n",
    "        self.ln2 = nn.Linear(num_neurons, output_dim)\n",
    "        self.inter = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ln1out = self.ln1(x)\n",
    "        ln1out = self.nm1(ln1out)\n",
    "        \n",
    "        device = ln1out.device\n",
    "        batch_size, _ = x.size()\n",
    "        \n",
    "        # # # # # # # # # # # # # #\n",
    "        #         SPLINE 1        #\n",
    "        # # # # # # # # # # # # # #\n",
    "        \n",
    "        sp1out = self.sp1(ln1out)\n",
    "        bslist = self.sp1.inter['basic']\n",
    "        \n",
    "        self.inter['ebasic'] = bslist\n",
    "        self.inter['basic'] = sp1out\n",
    "\n",
    "        ln2out = self.ln2(sp1out)        \n",
    "        return ln2out\n",
    "\n",
    "class TumorClassifier(nn.Module):\n",
    "    def __init__(self, Fin, dg, nk, nm, Fout, bias):\n",
    "        super(TumorClassifier, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.gap = nn.Flatten()\n",
    "        self.classifier = DNNS1(input_dim = Fin, degree = dg, num_knots = nk, num_neurons = nm, output_dim = Fout, bias = True).to(device)\n",
    "        self.sm = nn.Softmax(dim = 1)\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.classifier(x)\n",
    "        x = self.sm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3644a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model setting:\n",
    "\n",
    "`device`: running the program with cpu or gpu\n",
    "`tmc`: the classifier that equip with DNN-S \n",
    "`nm` : number of neuron in DNN-S\n",
    "`nk` : number of knot in DNN-S\n",
    "`patientc` : (early-stop crierion) If the model didn't improve in n epoch then stop.\n",
    "`patientr` : If the model didn't improve in n epoch then decrease learning rate with specific factor.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# experiment setting\n",
    "Iteration = 10000; bloss_list = []; tor = 1e-5; lr_tor = 1e-6\n",
    "patientc = 10; patientr = 5; tpat = 0; bloss = 9999\n",
    "nm = 100; nk = 15; doutput = 2\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model parameter \n",
    "tmc = TumorClassifier(Fin = 100352, dg = 3, nk = nk, nm = nm, Fout = doutput, bias = True)\n",
    "learning_r = 1e-2\n",
    "optimizer = torch.optim.Adam(tmc.parameters(), lr=learning_r)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe77a332-a3d3-4c68-978a-3c23fce7a8d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a07ad2-d621-40af-8634-8a143e45361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(Iteration):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "    pyb_af = tmc(X_train)\n",
    "    loss = criterion(pyb_af, y_train); bloss_list.append(loss.item())\n",
    "    \n",
    "    if (t > 0) and ((bloss_list[t-1]-bloss_list[t])<tor):        \n",
    "        if (tpat % patientr) == 0:\n",
    "            learning_r *= 0.2 \n",
    "            tpat += 1\n",
    "            #print('Learning rate reduce to ', learning_r)\n",
    "            optimizer = torch.optim.Adam(tmc.parameters(), lr=learning_r)\n",
    "            if learning_r <= lr_tor:\n",
    "                print('Convergence!')\n",
    "                break\n",
    "        elif tpat < patientc:\n",
    "            tpat += 1\n",
    "            pass\n",
    "        else:\n",
    "            print('Convergence!')\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        if loss < bloss:\n",
    "            #torch.save(tmc.state_dict(), './brainimg'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk))\n",
    "            bloss = loss.item()\n",
    "            tpat = 0\n",
    "        tpat += 1\n",
    "\n",
    "    if tpat == patientc:\n",
    "        print('Convergence!')\n",
    "        break\n",
    "    \n",
    "    prediction = torch.argmax(pyb_af, axis = 1)\n",
    "    acc = (torch.argmax(pyb_af, axis = 1) == y_train).sum()/len(y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(t % 10 == 0):\n",
    "        print('| Epoch: ',t+1,'/',str(Iteration),' | Loss: ', np.round(loss.item(), 4),' | Acc: ', acc.item())\n",
    "        if(t % 50 == 0):\n",
    "            with torch.no_grad():\n",
    "                print((torch.argmax(tmc(X_test).detach(), axis = 1) == y_test).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db9048f-3f4a-4a65-a120-338d4bf50bdb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ECM Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a8f42ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the ECM tunning for penalty in each layer\n",
      "Lambda: 0.2357| Training Loss: 0.6829500198364258| Training GCV: 4.268463726475602e-06\n",
      "Lambda: 0.27503| Training Loss: 0.5793099999427795| Training GCV: 3.62062542080821e-06\n",
      "Lambda: 0.27773| Training Loss: 0.7097499966621399| Training GCV: 4.435667051438941e-06\n",
      "Lambda: 0.20232| Training Loss: 0.6283699870109558| Training GCV: 3.927318175556138e-06\n",
      "Lambda: 0.19315| Training Loss: 0.6672599911689758| Training GCV: 4.173381967120804e-06\n",
      "Lambda: 0.1774| Training Loss: 0.6161199808120728| Training GCV: 4.7093672037590295e-06\n",
      "Lambda: 0.17239| Training Loss: 0.6477500200271606| Training GCV: 3.9890310290502384e-06\n",
      "Lambda: 0.16794| Training Loss: 0.6128600239753723| Training GCV: 3.708579697558889e-06\n",
      "Lambda: 0.16491| Training Loss: 0.6821600198745728| Training GCV: 4.254592113284161e-06\n",
      "Lambda: 0.16238| Training Loss: 0.6178699731826782| Training GCV: 3.938550435123034e-06\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "`ECM_epoch`: number of epoch to run the ecm tuning\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Running the ECM tunning for penalty in each layer\")\n",
    "\n",
    "ECM_epoch = 10\n",
    "with torch.no_grad():\n",
    "    eval_model = TumorClassifier(Fin = 100352, dg = 3, nk = nk, nm = nm, Fout = doutput, bias = True).to(device)\n",
    "    eval_model.load_state_dict(torch.load('./brainimg'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk), weights_only = True))\n",
    "\n",
    "    WB = eval_model.classifier.sp1.control_p\n",
    "    DB = diag_mat_weights(WB.size()[0], 'second').to(device)\n",
    "    \n",
    "    BestGCV = np.inf\n",
    "    n = X_train.size()[0]\n",
    "    \n",
    "    for i in range(ECM_epoch):\n",
    "        eval_model.train()\n",
    "        MPSy = eval_model(X_train)\n",
    "\n",
    "        # update following layer except for last layer\n",
    "        LambdaB1 = ECM(model = eval_model.classifier, num_neurons = nm, num_knots = nk, L = 1)\n",
    "        \n",
    "        B1 = eval_model.classifier.inter['ebasic']\n",
    "        By1 = eval_model.classifier.inter['basic']\n",
    "        P2 = By1 @ torch.inverse(By1.T @ By1) @ By1.T\n",
    "\n",
    "        \n",
    "        size1 = B1.size()[1]\n",
    "        B1 = B1.view(nk, nm, size1)\n",
    "\n",
    "        NW1 = torch.empty((nk, nm))\n",
    "        NB1 = torch.empty((nm))\n",
    "\n",
    "        \n",
    "        for i in range(nm):\n",
    "            B1y = By1[:,i] - eval_model.classifier.sp1.bias.data[i]\n",
    "            BB1 = B1[:,i].T\n",
    "\n",
    "            # Update the weights and bias\n",
    "            NW1[:, i] = (torch.inverse(BB1.T @ BB1 + (LambdaB1/size1) * (DB.T @ DB)) @ BB1.T @ B1y)\n",
    "            NB1[i] = torch.mean(By1[:,i] - (NW1[:,i] @ BB1.T))\n",
    "            \n",
    "        # update the weight\n",
    "        getattr(eval_model.classifier.sp1, 'control_p').data = NW1; getattr(eval_model.classifier.sp1, 'bias').data = NB1\n",
    "\n",
    "        # update the last layer\n",
    "        WholeB = torch.cat((torch.ones((n,1)), By1), dim = 1)\n",
    "        NLn2W = (torch.inverse(WholeB.T @ WholeB) @ WholeB.T @ MPSy.type(torch.FloatTensor)).T\n",
    "        getattr(eval_model.classifier.ln2, 'bias').data = NLn2W[:,0]; getattr(eval_model.classifier.ln2, 'weight').data = NLn2W[:,1:]\n",
    "        \n",
    "        eval_model.eval()\n",
    "        pred_postecm = eval_model(X_train)\n",
    "        CLoss = criterion(pred_postecm.detach(), y_train)\n",
    "        trainGCV = CLoss/(n-torch.trace(P2))**2\n",
    "        \n",
    "        if trainGCV < BestGCV:\n",
    "            BestLambdaB = LambdaB1\n",
    "            BestGCV = trainGCV\n",
    "            \n",
    "        print(f\"Lambda: {np.round(LambdaB1, 5)}| Training Loss: {np.round(CLoss, 5)}| Training GCV: {trainGCV.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d2cc9-3170-4ad5-aade-cfaa73cabc68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DPS fast tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20ef1a-c2f0-4e8a-984c-d0d016afc0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "`fast_epoch`: number of epoch to run the fast tuning\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "fast_epoch = 201\n",
    "DPS = TumorClassifier(Fin = 100352, dg = 3, nk = nk, nm = nm, Fout = doutput, bias = True).to(device)\n",
    "DPS.load_state_dict(torch.load('./brainimg'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk), weights_only = True))\n",
    "lr_ft = 1e-2\n",
    "optimizer = torch.optim.Adam(DPS.parameters(), lr=lr_ft)\n",
    "\n",
    "for t in range(1, fast_epoch):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "    pyb_af = DPS(X_train)\n",
    "\n",
    "    WB1 = DPS.classifier.sp1.control_p.data; DB1 = diag_mat_weights(WB1.size()[0]).to(device)\n",
    "\n",
    "    loss = criterion(pyb_af, y_train) + (BestLambdaB/n) * torch.norm(DB1 @ WB1)\n",
    "    bloss_list.append(loss.item())\n",
    "    \n",
    "    prediction = torch.argmax(pyb_af, axis = 1)\n",
    "    acc = (torch.argmax(pyb_af, axis = 1) == y_train).sum()/len(y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        print('| Epoch: ',t+1,'/',str(Iteration),' | Loss: ', loss.item(),' | Acc: ', np.round(acc.item(), 5))\n",
    "        if t % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                print((torch.argmax(DPS(X_test).detach(), axis = 1) == y_test).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd8e860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "856f41ac",
   "metadata": {},
   "source": [
    "## CN2DPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c243a-99fc-49d3-963f-42ca6f942a3e",
   "metadata": {},
   "source": [
    "### Model setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c657077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRODBSplineLayerMultiFeature(nn.Module):\n",
    "    def __init__(self, input_dim, degree, num_knots, output_dim, num_neurons, bias = True):\n",
    "        super(PRODBSplineLayerMultiFeature, self).__init__()\n",
    "        self.degree = degree\n",
    "        self.num_knots = num_knots\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_neurons = num_neurons\n",
    "        \n",
    "        if input_dim == 2:\n",
    "            self.control_p = nn.Parameter(torch.randn(self.num_knots**2, self.output_dim))\n",
    "        else:\n",
    "            self.control_p = nn.Parameter(torch.randn(self.num_knots, self.num_neurons))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(self.num_neurons))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.inter = {}\n",
    "    \n",
    "    def basis_function(self, x, i, k, t):\n",
    "    \n",
    "        # Base case: degree 0 spline\n",
    "        if k == 0:\n",
    "            return ((t[i] <= x) & (x < t[i + 1])).float()\n",
    "    \n",
    "        # Recursive case\n",
    "        denom1 = t[i + k] - t[i]\n",
    "        denom2 = t[i + k + 1] - t[i + 1]\n",
    "    \n",
    "        term1 = 0\n",
    "        if denom1 != 0:\n",
    "            term1 = (x - t[i]) / denom1 * self.basis_function(x, i, k - 1, t)\n",
    "    \n",
    "        term2 = 0\n",
    "        if denom2 != 0:\n",
    "            term2 = (t[i + k + 1] - x) / denom2 * self.basis_function(x, i + 1, k - 1, t)\n",
    "    \n",
    "        return term1 + term2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, num_features = x.size()\n",
    "        device = x.device\n",
    "        \n",
    "        # Create knot vector\n",
    "        # knots = torch.linspace(0, 1, self.num_knots + self.degree + 1).to(device)\n",
    "        knots = torch.cat([\n",
    "                        torch.zeros(self.degree),               # Add repeated values at the start for clamping\n",
    "                        torch.linspace(0, 1, self.num_knots - self.degree + 1),  # Uniform knot spacing in the middle\n",
    "                        torch.ones(self.degree)                 # Add repeated values at the end for clamping\n",
    "                    ]).to(device)\n",
    "        # Apply B-spline basis functions for each feature\n",
    "        basises = []\n",
    "    \n",
    "        \n",
    "        for feature in range(num_features):\n",
    "            # Calculate B-spline basis functions for this feature\n",
    "            basis = torch.stack([self.basis_function(x[:, feature], i, self.degree, knots) \n",
    "                                 for i in range(self.num_knots)], dim=-1)\n",
    "            basises.append(basis)\n",
    "            \n",
    "        if num_features == 1:\n",
    "            tout = basises[0] @ self.control_p\n",
    "            self.inter['eachbasic'] = basises[0].T\n",
    "        else:\n",
    "            #self.inter['basic'] = torch.reshape(torch.stack(basises, dim = 1), (batch_size, self.num_knots * self.num_neurons)).T\n",
    "            self.inter['eachbasic'] = torch.reshape(torch.stack(basises, dim = 1), (batch_size, self.num_knots * self.num_neurons)).T\n",
    "            \n",
    "            basises = torch.stack(basises)\n",
    "            tout = basises.permute(1,2,0) * self.control_p\n",
    "            tout = tout.sum(dim =1)\n",
    "                \n",
    "        if self.bias is not None:\n",
    "            tout += self.bias        \n",
    "\n",
    "        self.inter['basicoutput'] = tout\n",
    "        \n",
    "        return tout\n",
    "        \n",
    "class DNNS2(nn.Module):\n",
    "    def __init__(self, input_dim, degree, num_knots, num_neurons, output_dim, bias):\n",
    "        super(DNNS2, self).__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_knots = num_knots\n",
    "        self.ln1 = nn.Linear(input_dim, num_neurons)\n",
    "        self.nm1 = NormLayer() \n",
    "        self.sp1 = PRODBSplineLayerMultiFeature(input_dim = 1, degree = degree, num_knots = num_knots, num_neurons = num_neurons, output_dim= output_dim, bias = True)\n",
    "        self.nm2 = NormLayer() \n",
    "        self.sp2 = PRODBSplineLayerMultiFeature(input_dim = 1, degree = degree, num_knots = num_knots, num_neurons = num_neurons, output_dim= output_dim, bias = True)\n",
    "        self.ln2 = nn.Linear(num_neurons, output_dim)\n",
    "        self.inter = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ln1out = self.ln1(x)\n",
    "        ln1out = self.nm1(ln1out)\n",
    "        \n",
    "        device = ln1out.device\n",
    "        batch_size, _ = x.size()\n",
    "        \n",
    "        # # # # # # # # # # # # # #\n",
    "        #         SPLINE          #\n",
    "        # # # # # # # # # # # # # #\n",
    "        \n",
    "        sp1out = self.sp1(ln1out)\n",
    "        sp1out = self.nm2(sp1out)\n",
    "\n",
    "        # # # # # # # # # # # # # #\n",
    "        #         SPLINE 2        #\n",
    "        # # # # # # # # # # # # # #\n",
    "        \n",
    "        sp2out = self.sp2(sp1out)\n",
    "        ln2out = self.ln2(sp2out)\n",
    "        \n",
    "        return ln2out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5cdfa154",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorClassifier(nn.Module):\n",
    "    def __init__(self, Fin, dg, nk, nm, Fout, bias):\n",
    "        super(TumorClassifier, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.gap = nn.Flatten()\n",
    "        self.classifier = DNNS2(input_dim = 32*56*56, degree = dg, num_knots = nk, num_neurons = nm, output_dim = Fout, bias = True).to(device)\n",
    "        self.sm = nn.Softmax(dim = 1)\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.classifier(x)\n",
    "        x = self.sm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95e298a8-e59c-4e6f-a128-97f89518f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\"\"\"\n",
    "Model setting:\n",
    "\n",
    "`device`: running the program with cpu or gpu\n",
    "`tmc`: the classifier that equip with DNN-S \n",
    "`nm` : number of neuron in DNN-S\n",
    "`nk` : number of knot in DNN-S\n",
    "`patientc` : (early-stop crierion) If the model didn't improve in n epoch then stop.\n",
    "`patientr` : If the model didn't improve in n epoch then decrease learning rate with specific factor.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# experiment setting\n",
    "Iteration = 10000; bloss_list = []; tor = 1e-5; lr_tor = 1e-6\n",
    "patientc = 10; patientr = 5; tpat = 0; bloss = 9999\n",
    "nm = 50; nk = 15; doutput = 2\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model parameter \n",
    "tmc = TumorClassifier(Fin = 100352, dg = 3, nk = nk, nm = nm, Fout = doutput, bias = True)\n",
    "learning_r = 1e-3\n",
    "optimizer = torch.optim.Adam(tmc.parameters(), lr=learning_r)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a94e5-1957-4500-8f25-ab7c80163a56",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d95b3-4114-4843-868c-f0fb71da6b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch:  1 / 10000  | Loss:  0.6675  | Acc:  0.6253834366798401\n",
      "tensor(0.5337)\n"
     ]
    }
   ],
   "source": [
    "for t in range(Iteration):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "    pyb_af = tmc(X_train)\n",
    "    loss = criterion(pyb_af, y_train); bloss_list.append(loss.item())\n",
    "    \n",
    "    if (t > 0) and ((bloss_list[t-1]-bloss_list[t])<tor):        \n",
    "        if (tpat % patientr) == 0:\n",
    "            learning_r *= 0.2 \n",
    "            tpat += 1\n",
    "            #print('Learning rate reduce to ', learning_r)\n",
    "            optimizer = torch.optim.Adam(tmc.parameters(), lr=learning_r)\n",
    "            if learning_r <= lr_tor:\n",
    "                print('Convergence!')\n",
    "                break\n",
    "        elif tpat < patientc:\n",
    "            tpat += 1\n",
    "            pass\n",
    "        else:\n",
    "            print('Convergence!')\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        if loss < bloss:\n",
    "            #torch.save(tmc.state_dict(), './brainimg'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk))\n",
    "            bloss = loss.item()\n",
    "            tpat = 0\n",
    "        tpat += 1\n",
    "\n",
    "    if tpat == patientc:\n",
    "        print('Convergence!')\n",
    "        break\n",
    "    \n",
    "    prediction = torch.argmax(pyb_af, axis = 1)\n",
    "    acc = (torch.argmax(pyb_af, axis = 1) == y_train).sum()/len(y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(t % 10 == 0):\n",
    "        print('| Epoch: ',t+1,'/',str(Iteration),' | Loss: ', np.round(loss.item(), 4),' | Acc: ', acc.item())\n",
    "        if(t % 100 == 0):\n",
    "            with torch.no_grad():\n",
    "                print((torch.argmax(tmc(X_test).detach(), axis = 1) == y_test).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b2fa3-fecd-4c0d-902c-8abd9ca83813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "225c6bee-4b98-451f-8491-a49f4bba31d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    eval_model = TumorClassifier(Fin = 100352, dg = 3, nk = nk, nm = nm, Fout = doutput, bias = True).to(device)\n",
    "    eval_model.load_state_dict(torch.load('./2brainimg'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk), weights_only = True))\n",
    "    pred_postecm = eval_model(X_train)\n",
    "    CLoss = criterion(pred_postecm.detach(), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e51c0-2f99-4ef6-ba48-03f4d59b06d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ECM Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ae012a20-9b3f-4152-846f-c7d04b52301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ECM(model, num_neurons, num_knots, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4):\n",
    "    lambdab = initial_lambda\n",
    "    sigma = initial_sigma\n",
    "    xi = initial_xi\n",
    "\n",
    "    B = model.inter['eachbasic']\n",
    "    By = model.inter['basicoutput']\n",
    "    WB = model.control_p\n",
    "        \n",
    "    DB = diag_mat_weights(WB.size()[0]).to(device)\n",
    "    size = B.size()[1]\n",
    "    S = DB.T @ DB\n",
    "    Cov_a = (xi**2)* torch.linalg.pinv(S)\n",
    "    Cov_e = torch.eye(size*num_neurons)* sigma\n",
    "    \n",
    "    block_y = torch.reshape(By, (-1,1))\n",
    "    flatB = B.view(num_neurons, num_knots, size)\n",
    "        \n",
    "    sqr_xi= 0\n",
    "    sqr_sig = 0\n",
    "    \n",
    "    for i in range(num_neurons):\n",
    "        Ncov = (Cov_a -(Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)]) @ flatB[i].T @ Cov_a))\n",
    "        Nmu = (Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)])) @ By[:,i].reshape(-1,1)\n",
    "        \n",
    "        first_xi = S @ Ncov\n",
    "        second_xi = (Nmu.T @ S @ Nmu)\n",
    "        sqr_xi += torch.trace(first_xi) + second_xi\n",
    "            \n",
    "        first_sig = torch.norm(By[:,i])\n",
    "        second_sig = 2 * (By[:,i] @ flatB[i].T) @ Nmu \n",
    "        third_sig = torch.trace((flatB[i] @ flatB[i].T) @ Ncov)\n",
    "        four_sig = (Nmu.T @ flatB[i] @ flatB[i].T @ Nmu)\n",
    "        \n",
    "        sqr_sig += (first_sig + second_sig + third_sig + four_sig)\n",
    "    \n",
    "    sqr_xi /= num_neurons\n",
    "    sqr_sig /= (num_neurons*size)\n",
    "    \n",
    "    Lambda = sqr_sig/sqr_xi\n",
    "    \n",
    "    return Lambda.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "eda1c8f3-fd95-40e3-9816-d4edac44456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the ECM tunning for penalty in each layer\n",
      "Lambda: [0.30183088779449463, 0.30312493443489075]| Training Loss: 0.7657999992370605| Training GCV: 4.786255885846913e-06\n",
      "Lambda: [0.973455011844635, 0.6522708535194397]| Training Loss: 0.8041099905967712| Training GCV: 4.120976882404648e-06\n",
      "Lambda: [1.3706945180892944, 0.3747212588787079]| Training Loss: 0.7974600195884705| Training GCV: 5.1150018407497555e-06\n",
      "Lambda: [1.8239096403121948, 0.3827195465564728]| Training Loss: 0.8612599968910217| Training GCV: 4.769452061736956e-06\n",
      "Lambda: [1.5904974937438965, 1.7096126079559326]| Training Loss: 0.7672600150108337| Training GCV: 3.7891347801632946e-06\n",
      "Lambda: [1.670275092124939, 1.7180237770080566]| Training Loss: 0.7532600164413452| Training GCV: 3.5555015074351104e-06\n",
      "Lambda: [1.777961254119873, 0.859359860420227]| Training Loss: 0.907260000705719| Training GCV: 4.15523027186282e-06\n",
      "Lambda: [1.9107773303985596, 1.430779218673706]| Training Loss: 0.7932599782943726| Training GCV: 3.4246222639922053e-06\n",
      "Lambda: [1.9577510356903076, 2.0410945415496826]| Training Loss: 0.8152599930763245| Training GCV: 3.541840897014481e-06\n",
      "Lambda: [1.9773733615875244, 1.1929782629013062]| Training Loss: 0.7972599864006042| Training GCV: 3.7175316265347647e-06\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "`ECM_epoch`: number of epoch to run the ecm tuning\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "LoP = 2\n",
    "PSname = ['sp'+str(i+1) for i in range(LoP)]\n",
    "ECM_epoch = 10\n",
    "\n",
    "print(\"Running the ECM tunning for penalty in each layer\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    eval_model = TumorClassifier(Fin = 100352, dg = 3, nk = nk, nm = nm, Fout = doutput, bias = True).to(device)\n",
    "    eval_model.load_state_dict(torch.load('./2brainimg'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk), weights_only = True))\n",
    "\n",
    "    BestGCV = np.inf\n",
    "    n = X_train.size()[0]\n",
    "    \n",
    "    for i in range(ECM_epoch):\n",
    "        eval_model.train()\n",
    "        MPSy = eval_model(X_train)\n",
    "    \n",
    "        # update following layer except for last layer\n",
    "        LambdaL = []\n",
    "        for layer in PSname:\n",
    "            splayer = getattr(eval_model.classifier, layer)\n",
    "            WB = getattr(splayer, 'control_p')\n",
    "            DB = diag_mat_weights(WB.size()[0], 'second').to(device)\n",
    "            LambdaB = ECM(model = getattr(eval_model.classifier, layer), num_neurons = nm, num_knots = nk)\n",
    "            LambdaL.append(LambdaB)\n",
    "            \n",
    "            B1 = getattr(eval_model.classifier, layer).inter['eachbasic']\n",
    "            By1 = getattr(eval_model.classifier, layer).inter['basicoutput']\n",
    "            P2 = By1 @ torch.inverse(By1.T @ By1) @ By1.T\n",
    "    \n",
    "            size1 = B1.size()[1]\n",
    "            B1 = B1.view(nk, nm, size1)\n",
    "    \n",
    "            NW1 = torch.empty((nk, nm))\n",
    "            NB1 = torch.empty((nm))\n",
    "    \n",
    "        \n",
    "            for i in range(nm):\n",
    "                B1y = By1[:,i] - getattr(eval_model.classifier, layer).bias.data[i]\n",
    "                BB1 = B1[:,i].T\n",
    "        \n",
    "                # Update the weights and bias\n",
    "                NW1[:, i] = (torch.inverse(BB1.T @ BB1 + (LambdaB/size1) * (DB.T @ DB)) @ BB1.T @ B1y)\n",
    "                NB1[i] = torch.mean(By1[:,i] - (NW1[:,i] @ BB1.T))\n",
    "                \n",
    "            # update the weight\n",
    "            getattr(splayer, 'control_p').data = NW1; getattr(splayer, 'bias').data = NB1\n",
    "    \n",
    "        # update the last layer\n",
    "        WholeB = torch.cat((torch.ones((n,1)), By1), dim = 1)\n",
    "        NLn2W = (torch.inverse(WholeB.T @ WholeB) @ WholeB.T @ MPSy.type(torch.FloatTensor)).T\n",
    "        \n",
    "        getattr(eval_model.classifier.ln2, 'bias').data = NLn2W[:,0]; getattr(eval_model.classifier.ln2, 'weight').data = NLn2W[:,1:]\n",
    "        \n",
    "        eval_model.eval()\n",
    "        pred_postecm = eval_model(X_train)\n",
    "        CLoss = criterion(pred_postecm.detach(), y_train)\n",
    "        trainGCV = CLoss/(n-torch.trace(P2))**2\n",
    "        \n",
    "        if trainGCV < BestGCV:\n",
    "            BestLambdaB = LambdaB1\n",
    "            BestGCV = trainGCV\n",
    "            \n",
    "        print(f\"Lambda: {LambdaL}| Training Loss: {np.round(CLoss, 5)}| Training GCV: {trainGCV.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d6556-afd1-41ac-8a91-9730eae7dd61",
   "metadata": {},
   "source": [
    "### 2DPS Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "a9449ea7-7a9c-4ad4-8659-94451e6fc730",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "`fast_epoch`: number of epoch to run the fast tuning\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "fast_epoch = 201\n",
    "D2PS = TumorClassifier(Fin = 100352, dg = 3, nk = nk, nm = nm, Fout = doutput, bias = True).to(device)\n",
    "D2PS.load_state_dict(torch.load('./2brainimg'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk), weights_only = True))\n",
    "lr_ft = 1e-2\n",
    "optimizer = torch.optim.Adam(D2PS.parameters(), lr=lr_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df72b6da-e9b1-40dd-a842-5716c2d02a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1, fast_epoch):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "    pyb_af = D2PS(X_train)\n",
    "    loss = criterion(pyb_af, y_train)\n",
    "    \n",
    "    for l in range(len(PSname)):\n",
    "        WB = getattr(D2PS.classifier, PSname[l]).control_p.data; DB = diag_mat_weights(WB.size()[0]).to(device)\n",
    "        loss += (LambdaL[l]/n) * torch.norm(DB @ WB)\n",
    "            \n",
    "    prediction = torch.argmax(pyb_af, axis = 1)\n",
    "    acc = (torch.argmax(pyb_af, axis = 1) == y_train).sum()/len(y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        print('| Epoch: ',t+1,'/',str(Iteration),' | Loss: ', loss.item(),' | Acc: ', np.round(acc.item(), 5))\n",
    "        if t % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                print((torch.argmax(D2PS(X_test).detach(), axis = 1) == y_test).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd468e2a-5be2-4a58-a89e-464add088fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b9d3f-6c96-455e-a47f-9d373b800622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ccfc2c-3425-404c-b178-6ad679016ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
