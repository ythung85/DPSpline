{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d1745a-f822-44f1-af6d-7cf0b4f382ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sim_data(n, dim, Type):\n",
    "    if Type == 'A':\n",
    "        X = torch.rand((n,dim))\n",
    "        y = torch.exp(2*torch.sin(X[:,0]*0.5*torch.pi)+ 0.5*torch.cos(X[:,1]*2.5*torch.pi))\n",
    "        y = y.reshape(-1,1)\n",
    "        y = y.float()\n",
    "\n",
    "    elif Type == 'B':\n",
    "        X = torch.rand((n, dim))\n",
    "        y = 1\n",
    "        for d in range(dim):\n",
    "            a = (d+1)/2\n",
    "            y *= ((torch.abs(4*X[:,d]-2)+a)/(1+a))\n",
    "        y = y.reshape(-1,1)\n",
    "        y = y.float()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def norm(x):\n",
    "    return (x-torch.min(x))/(torch.max(x)-torch.min(x))\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def diag_mat_weights(dimp, type = 'first'):\n",
    "    if type == 'first':\n",
    "        dg = np.zeros((dimp-1, dimp))\n",
    "        for i in range(dimp-1):\n",
    "            dg[i,i] = -1\n",
    "            dg[i,i+1]= 1\n",
    "    elif type == 'second':\n",
    "        dg = np.zeros((dimp-2, dimp))\n",
    "        for i in range(dimp-2):\n",
    "            dg[i,i] = -1\n",
    "            dg[i,i+1]= 2\n",
    "            dg[i,i+2]= -1\n",
    "    else:\n",
    "        pass\n",
    "    return torch.Tensor(dg)\n",
    "\n",
    "def num_para(model):\n",
    "    tp = 0\n",
    "    for param in model.parameters():\n",
    "        tp += param.numel()\n",
    "    return tp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba56b5bf-a574-4b6b-94e4-5f69d6d294f7",
   "metadata": {},
   "source": [
    "## ECM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cac109e3-c01b-4a96-a6dc-7dc2427c54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ECM(par, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4):\n",
    "    lambdab = initial_lambda\n",
    "    sigma = initial_sigma\n",
    "    xi = initial_xi\n",
    "    \n",
    "    n_block, num_knots, num_neurons = par['wbasic'].size()\n",
    "    ls_lambda = torch.empty(n_block)\n",
    "    \n",
    "    for l in range(n_block):\n",
    "        B = par['ebasic'][l]\n",
    "        By = par['basic'][l]\n",
    "        WB = par['wbasic'][l]\n",
    "        \n",
    "        DB = diag_mat_weights(WB.size()[0]).to(device)\n",
    "        size = B.size()[1]\n",
    "        S = DB.T @ DB\n",
    "        Cov_a = (xi**2)* torch.linalg.pinv(S)\n",
    "        Cov_a.to(device)\n",
    "        Cov_e = (torch.eye(size*num_neurons)* sigma).to(device)\n",
    "        \n",
    "        block_y = torch.reshape(By, (-1,1))\n",
    "        flatB = B.view(num_neurons, num_knots, size)\n",
    "            \n",
    "        sqr_xi= 0\n",
    "        sqr_sig = 0\n",
    "\n",
    "        for i in range(num_neurons):\n",
    "            Ncov = (Cov_a -(Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)]) @ flatB[i].T @ Cov_a))\n",
    "            Nmu = (Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)])) @ By[:,i].reshape(-1,1)\n",
    "            \n",
    "            first_xi = S @ Ncov\n",
    "            second_xi = (Nmu.T @ S @ Nmu)\n",
    "            sqr_xi += torch.trace(first_xi) + second_xi\n",
    "                \n",
    "            first_sig = torch.norm(By[:,i])\n",
    "            second_sig = 2 * (By[:,i] @ flatB[i].T) @ Nmu \n",
    "            third_sig = torch.trace((flatB[i] @ flatB[i].T) @ Ncov)\n",
    "            four_sig = (Nmu.T @ flatB[i] @ flatB[i].T @ Nmu)\n",
    "            \n",
    "            sqr_sig += (first_sig + second_sig + third_sig + four_sig)\n",
    "            \n",
    "            del first_xi, second_xi, first_sig, second_sig, third_sig, four_sig\n",
    "\n",
    "        sqr_xi /= num_neurons\n",
    "        sqr_sig /= (num_neurons*size)\n",
    "\n",
    "        ls_lambda[l] = (sqr_sig/sqr_xi).item()\n",
    "        \n",
    "        del Cov_a, Cov_e, flatB\n",
    "    \n",
    "    return ls_lambda\n",
    "    \n",
    "def ECM_layersise_update(model, par, Lambda, x, y):\n",
    "\n",
    "    model.eval()\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        DSy = model(x)\n",
    "        print('Training Error: ', np.round(criterion(y, DSy.detach()).item(), 5))\n",
    "    '''\n",
    "    \n",
    "    device = x.device\n",
    "    \n",
    "    B_out, B_in, B_w, B_b = par['basic'], par['ebasic'], par['wbasic'], par['bbasic']\n",
    "    n_layer, nk, nm = B_w.size()\n",
    "    DB = diag_mat_weights(B_w[0].size()[0], 'second').to(device)\n",
    "\n",
    "    Project_matrix = (torch.linalg.pinv(B_in[-1].T @ B_in[-1]) @ B_in[-1].T @ B_in[-1])\n",
    "    Size = [b.size()[1] for b in B_in]\n",
    "\n",
    "    B_in = B_in.view(n_layer, nm, nk, Size[0])\n",
    "    \n",
    "    for l in range(n_layer):    \n",
    "        NW = torch.empty((nk, nm)).to(device)\n",
    "        NB = torch.empty((nm)).to(device)\n",
    "        \n",
    "        for i in range(nm):\n",
    "            B1y = B_out[l][:,i] - B_b[l][i]\n",
    "            BB = B_in[l][i].T\n",
    "    \n",
    "            # Update the weights and bias\n",
    "            NW[:, i] = (torch.inverse(BB.T @ BB + (Lambda[l]/Size[l]) * (DB.T @ DB)) @ BB.T @ B1y)\n",
    "            NB[i] = torch.mean(B_out[l][:,i] - (NW[:,i] @ BB.T))\n",
    "                \n",
    "        # update the weight\n",
    "        block = getattr(model.Spline_block.model, f'block_{l}')\n",
    "        getattr(block.block.BSL, 'control_p').data = NW\n",
    "        getattr(block.block.BSL, 'bias').data = NB\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        DPSy = model(x)\n",
    "        Update_Train_Loss = np.round(criterion(y, DPSy.detach()).item(), 5)\n",
    "        GCV = np.round((torch.norm(y - DPSy)/(Size[-1]-torch.trace(Project_matrix))).item(), 5)\n",
    "    \n",
    "    return model, GCV\n",
    "\n",
    "def ECM_update(model, max_iter, x, y):\n",
    "    BestGCV = 9999\n",
    "    patient = 5\n",
    "    pcount = 0\n",
    "    for i in range(max_iter):\n",
    "        _ = model(X_train)\n",
    "        ECM_para = model.get_para_ecm(x)\n",
    "        ECM_Lambda = ECM(ECM_para, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4)\n",
    "\n",
    "        print('Lambda: ', ECM_Lambda)\n",
    "        model, GCV = ECM_layersise_update(model, ECM_para, ECM_Lambda, x, y)\n",
    "\n",
    "        if GCV < BestGCV:\n",
    "            BestLambda = ECM_Lambda\n",
    "            BestGCV = GCV\n",
    "            pcount = 0\n",
    "        else:\n",
    "            pcount += 1\n",
    "\n",
    "        if pcount == patient:\n",
    "            break\n",
    "\n",
    "        del ECM_para, ECM_Lambda\n",
    "    \n",
    "    return BestLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb6ac5-a0e0-460c-8819-abd99bf34073",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d5b9ef9-aced-4426-b106-08585f6cce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSL(nn.Module):\n",
    "    def __init__(self, degree, num_knots, num_neurons, bias = True):\n",
    "        super(BSL, self).__init__()\n",
    "        self.degree = degree\n",
    "        self.num_knots = num_knots\n",
    "        self.num_neurons = num_neurons\n",
    "        self.control_p = nn.Parameter(torch.randn(self.num_knots, self.num_neurons))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(self.num_neurons))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.inter = {}\n",
    "    \n",
    "    def basis_function(self, x, i, k, t):\n",
    "    \n",
    "        # Base case: degree 0 spline\n",
    "        if k == 0:\n",
    "            return ((t[i] <= x) & (x < t[i + 1])).float()\n",
    "    \n",
    "        # Recursive case\n",
    "        denom1 = t[i + k] - t[i]\n",
    "        denom2 = t[i + k + 1] - t[i + 1]\n",
    "    \n",
    "        term1 = 0\n",
    "        if denom1 != 0:\n",
    "            term1 = (x - t[i]) / denom1 * self.basis_function(x, i, k - 1, t)\n",
    "    \n",
    "        term2 = 0\n",
    "        if denom2 != 0:\n",
    "            term2 = (t[i + k + 1] - x) / denom2 * self.basis_function(x, i + 1, k - 1, t)\n",
    "    \n",
    "        return term1 + term2\n",
    "\n",
    "    def knots_distribution(self, dg, nk):\n",
    "\n",
    "        knots = torch.cat([torch.linspace(-0.002, -0.001, steps=dg),            # Add repeated values at the start for clamping\n",
    "            torch.linspace(0, 1, nk-2*dg-2),  # Uniform knot spacing in the middle\n",
    "            torch.linspace(1.001, 1.002, steps=dg)           # Add repeated values at the end for clamping\n",
    "            ]).view(-1,1)\n",
    "     \n",
    "        return knots\n",
    "    \n",
    "    def basis_function2(self, x, spl):\n",
    "        basis_output = spl.fit_transform(x.cpu().numpy())\n",
    "        return basis_output\n",
    "            \n",
    "    def forward(self, x):\n",
    "        batch_size, num_features = x.size()\n",
    "        device = x.device\n",
    "        \n",
    "        # Create knot vector and apply B-spline basis functions for each feature\n",
    "\n",
    "        '''\n",
    "        knots = torch.cat([\n",
    "                        torch.zeros(self.degree),               # Add repeated values at the start for clamping\n",
    "                        torch.linspace(0, 1, self.num_knots - self.degree + 1),  # Uniform knot spacing in the middle\n",
    "                        torch.ones(self.degree)                 # Add repeated values at the end for clamping\n",
    "                    ]).to(device)\n",
    "\n",
    "        # Apply B-spline basis functions for each feature\n",
    "\n",
    "        basises = []\n",
    "        \n",
    "        \n",
    "        for feature in range(num_features):\n",
    "            # Calculate B-spline basis functions for this feature\n",
    "            basis = torch.stack([self.basis_function(x[:, feature], i, self.degree, knots) \n",
    "                                 for i in range(self.num_knots)], dim=-1)\n",
    "            basises.append(basis)\n",
    "            \n",
    "        '''\n",
    "    \n",
    "        basises = []\n",
    "        knots = self.knots_distribution(self.degree, self.num_knots)\n",
    "        #knots = knots.to(device)\n",
    "        spl = SplineTransformer(n_knots=self.num_knots, degree=self.degree, knots = knots)\n",
    "\n",
    "        \n",
    "        for feature in range(num_features):\n",
    "            # Calculate B-spline basis functions for this feature\n",
    "            \n",
    "            basis = self.basis_function2(x[:, feature].reshape(-1,1), spl)\n",
    "            basis = torch.Tensor(basis).to(device)\n",
    "            basises.append(basis)\n",
    "        \n",
    "        \n",
    "        if num_features == 1:\n",
    "            tout = basises[0] @ self.control_p\n",
    "            self.inter['basic'] = basises[0].T\n",
    "        else:\n",
    "            self.inter['basic'] = torch.reshape(torch.stack(basises, dim = 1), (batch_size, self.num_knots * self.num_neurons)).T\n",
    "            basises = torch.stack(basises)\n",
    "            tout = basises.permute(1,2,0) * self.control_p\n",
    "            tout = tout.sum(dim =1)\n",
    "                \n",
    "        if self.bias is not None:\n",
    "            tout += self.bias        \n",
    "            \n",
    "        return tout\n",
    "\n",
    "\n",
    "class NormLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NormLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        min_val = torch.min(x, axis = 1).values.reshape(-1,1)\n",
    "        max_val = torch.max(x, axis = 1).values.reshape(-1,1)\n",
    "\n",
    "        x = (x - min_val)/(max_val - min_val)  # Rescale to [0, 1]\n",
    "        return x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba6d1c7-50bb-4c92-9a2f-5ab66703d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSpline_block(nn.Module):\n",
    "    def __init__(self, degree, num_knots, num_neurons, dropout = 0.0, bias = True):\n",
    "        super(BSpline_block, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(OrderedDict([\n",
    "            ('norm', NormLayer()),\n",
    "            ('BSL', BSL(degree = degree, num_knots = num_knots, num_neurons = num_neurons, bias = bias)),\n",
    "            ('drop', nn.Dropout(dropout)),\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "        \n",
    "class StackBS_block(nn.Module):\n",
    "    def __init__(self, block, degree, num_knots, num_neurons, num_blocks, dropout = 0.0, bias = True):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleDict({\n",
    "            f'block_{i}': block(degree = degree, num_knots = num_knots, num_neurons = num_neurons)\n",
    "            for i in range(num_blocks)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        for name, block in self.model.items():\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d56f6179-804b-461f-bf1e-8423ab517582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DPS(nn.Module):\n",
    "    def __init__(self, input_dim, degree, num_knots, num_neurons, num_bsl, dropout, output_dim, bias):\n",
    "        super(DPS, self).__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_knots = num_knots\n",
    "        self.ln1 = nn.Linear(input_dim, num_neurons)\n",
    "        #self.nm1 = NormLayer() \n",
    "        #self.sp1 = BSL(degree = degree, num_knots = num_knots, num_neurons = num_neurons, bias = True)\n",
    "        self.Spline_block = StackBS_block(BSpline_block, degree = degree, num_knots = num_knots, num_neurons = num_neurons, num_blocks = num_bsl, dropout = dropout)\n",
    "        self.ln2 = nn.Linear(num_neurons, output_dim)\n",
    "        #self.inter = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.ln1(x)\n",
    "        #x = self.nm1(x)\n",
    "        # # # # # # # # # # # # # #\n",
    "        #          SPLINE         #\n",
    "        # # # # # # # # # # # # # #\n",
    "        \n",
    "        spout = self.Spline_block(x)\n",
    "\n",
    "        '''  \n",
    "        ln1out = self.nm1(ln1out)\n",
    "        device = ln1out.device\n",
    "        batch_size, _ = x.size()\n",
    "        \n",
    "        # # # # # # # # # # # # # #\n",
    "        #          SPLINE         #\n",
    "        # # # # # # # # # # # # # #\n",
    "        \n",
    "        sp1out = self.sp1(ln1out)\n",
    "        bslist = self.sp1.inter['basic']\n",
    "        \n",
    "        self.inter['ebasic'] = bslist\n",
    "        self.inter['basic'] = sp1out\n",
    "        '''\n",
    "        \n",
    "        output = self.ln2(spout)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def get_para_ecm(self, x):\n",
    "\n",
    "        '''\n",
    "        ecm_para: A dictionary that collects the parameter we need to the following ECM algorithm.\n",
    "        ecm_para.basic: Store the output of each B-Spline block; Dimension = [n_sample, n_neurons]\n",
    "        ecm_para.ebasic Store the weight matrix of each B-Spline expansion; Dimension = [n_knots * n_neurons, n_sample]\n",
    "\n",
    "        '''\n",
    "        ecm_para = {}\n",
    "        bs_block_out = {}\n",
    "        bs_spline_weight = {}\n",
    "        bs_spline_value = {}\n",
    "        bs_spline_bias = {}\n",
    "\n",
    "        _ = self(x)\n",
    "        \n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                bs_block_out[name] = output.detach()\n",
    "            return hook\n",
    "\n",
    "        handles = []\n",
    "        for name, layer in self.named_modules():\n",
    "            if 'block.drop' in name:\n",
    "                handles.append(layer.register_forward_hook(get_activation(name)))\n",
    "            elif 'block.BSL' in name:\n",
    "                bs_spline_value[name] = layer.inter['basic'].detach()\n",
    "                bs_spline_weight[name] = layer.control_p.detach()\n",
    "                bs_spline_bias[name] = layer.bias.detach()\n",
    "        # Run forward pass (triggers hooks)\n",
    "        _ = self(x)\n",
    "\n",
    "        # Clean up hooks\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "            \n",
    "        ecm_para['basic'] = torch.stack(list(bs_block_out.values()), dim=0)\n",
    "        ecm_para['ebasic'] = torch.stack(list(bs_spline_value.values()), dim=0)\n",
    "        ecm_para['wbasic'] = torch.stack(list(bs_spline_weight.values()), dim=0)\n",
    "        ecm_para['bbasic'] = torch.stack(list(bs_spline_bias.values()), dim=0)\n",
    "        del bs_block_out, bs_spline_weight, bs_spline_value, bs_spline_bias\n",
    "        \n",
    "        return ecm_para\n",
    "\n",
    "    def fit(self, x):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c23cb-4c93-4fbb-876f-91530c18b96f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "64e443ff-3729-4a1c-85ad-c500872d5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 200; ntest = 200; ndim = 2; ndf = 1\n",
    "Dtype = 'A'\n",
    "data = {}\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for d in range(ndf):\n",
    "    X_train, y_train = sim_data(ntrain, ndim, Dtype)\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device) \n",
    "    X_test, y_test = sim_data(ntest, ndim, Dtype)\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device) \n",
    "    \n",
    "    epstrain = torch.normal(0, torch.var(y_train)*0.05, size=y_train.size()).to(device) \n",
    "    epstest = torch.normal(0, torch.var(y_test)*0.05, size=y_test.size()).to(device) \n",
    "    \n",
    "    y_train, y_test = y_train + epstrain, y_test + epstest\n",
    "    data[str(d+1)] = {'TrainX': X_train, 'Trainy': y_train, 'TestX': X_test, 'Testy': y_test}\n",
    "    \n",
    "    \n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328c265-731b-4044-b65b-b6b39792fdae",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7098de70-4ba8-419a-9dbd-3165171a16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Setting \n",
    "dg = 3; nk = 10; nm = 30; Fout = 1; nl = 2\n",
    "DeepBS = DPS(input_dim = ndim, degree = dg, num_knots = nk, num_neurons = nm, num_bsl = nl, dropout = 0.1, output_dim = Fout, bias = True).to(device)\n",
    "learning_r = 1e-1\n",
    "optimizer = torch.optim.Adam(DeepBS.parameters(), lr=learning_r)\n",
    "Iteration = 10000; bloss_list = []; tor = 1e-5; lr_tor = 1e-6\n",
    "patientc = 30; patientr = 10; tpat = 0; bloss = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339875c5-45d2-489e-8774-8b89b0ad0b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63a7ea3-6516-4209-8777-64b4d6eab545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss:  30.032482147216797  | , previous best loss:  9999  | saving best model ...\n",
      "Current loss:  11.085083961486816  | , previous best loss:  30.032482147216797  | saving best model ...\n",
      "Current loss:  9.798272132873535  | , previous best loss:  11.085083961486816  | saving best model ...\n",
      "Current loss:  8.535736083984375  | , previous best loss:  9.798272132873535  | saving best model ...\n",
      "Current loss:  8.248806953430176  | , previous best loss:  8.535736083984375  | saving best model ...\n",
      "Current loss:  7.129239559173584  | , previous best loss:  8.248806953430176  | saving best model ...\n",
      "Current loss:  6.380927085876465  | , previous best loss:  7.129239559173584  | saving best model ...\n",
      "Current loss:  5.527121543884277  | , previous best loss:  6.380927085876465  | saving best model ...\n",
      "Current loss:  4.729115009307861  | , previous best loss:  5.527121543884277  | saving best model ...\n",
      "Current loss:  4.536695957183838  | , previous best loss:  4.729115009307861  | saving best model ...\n",
      "Current loss:  3.9337456226348877  | , previous best loss:  4.536695957183838  | saving best model ...\n",
      "Current loss:  3.4378881454467773  | , previous best loss:  3.9337456226348877  | saving best model ...\n",
      "Current loss:  3.0563879013061523  | , previous best loss:  3.4378881454467773  | saving best model ...\n",
      "Current loss:  3.0309622287750244  | , previous best loss:  3.0563879013061523  | saving best model ...\n",
      "Current loss:  2.952009916305542  | , previous best loss:  3.0309622287750244  | saving best model ...\n",
      "Current loss:  2.8998095989227295  | , previous best loss:  2.952009916305542  | saving best model ...\n",
      "Current loss:  2.8026344776153564  | , previous best loss:  2.8998095989227295  | saving best model ...\n",
      "Current loss:  2.7567546367645264  | , previous best loss:  2.8026344776153564  | saving best model ...\n",
      "Current loss:  2.708763599395752  | , previous best loss:  2.7567546367645264  | saving best model ...\n",
      "Current loss:  2.695516347885132  | , previous best loss:  2.708763599395752  | saving best model ...\n",
      "Current loss:  2.6754515171051025  | , previous best loss:  2.695516347885132  | saving best model ...\n",
      "Current loss:  2.666163682937622  | , previous best loss:  2.6754515171051025  | saving best model ...\n",
      "Current loss:  2.6562633514404297  | , previous best loss:  2.666163682937622  | saving best model ...\n",
      "Current loss:  2.6301510334014893  | , previous best loss:  2.6562633514404297  | saving best model ...\n",
      "Current loss:  2.622889995574951  | , previous best loss:  2.6301510334014893  | saving best model ...\n",
      "Current loss:  2.6046907901763916  | , previous best loss:  2.622889995574951  | saving best model ...\n",
      "Current loss:  2.583815813064575  | , previous best loss:  2.6046907901763916  | saving best model ...\n",
      "Current loss:  2.57938814163208  | , previous best loss:  2.583815813064575  | saving best model ...\n",
      "Current loss:  2.572288990020752  | , previous best loss:  2.57938814163208  | saving best model ...\n",
      "Current loss:  2.552980899810791  | , previous best loss:  2.572288990020752  | saving best model ...\n",
      "Current loss:  2.5412564277648926  | , previous best loss:  2.552980899810791  | saving best model ...\n",
      "Current loss:  2.5382559299468994  | , previous best loss:  2.5412564277648926  | saving best model ...\n",
      "Current loss:  2.526646137237549  | , previous best loss:  2.5382559299468994  | saving best model ...\n",
      "Current loss:  2.512706756591797  | , previous best loss:  2.526646137237549  | saving best model ...\n",
      "Current loss:  2.5081629753112793  | , previous best loss:  2.512706756591797  | saving best model ...\n",
      "Current loss:  2.5019030570983887  | , previous best loss:  2.5081629753112793  | saving best model ...\n",
      "Current loss:  2.4892189502716064  | , previous best loss:  2.5019030570983887  | saving best model ...\n",
      "Current loss:  2.481640577316284  | , previous best loss:  2.4892189502716064  | saving best model ...\n",
      "Current loss:  2.4766533374786377  | , previous best loss:  2.481640577316284  | saving best model ...\n",
      "Current loss:  2.4657366275787354  | , previous best loss:  2.4766533374786377  | saving best model ...\n",
      "Current loss:  2.455850601196289  | , previous best loss:  2.4657366275787354  | saving best model ...\n",
      "Current loss:  2.449856758117676  | , previous best loss:  2.455850601196289  | saving best model ...\n",
      "Current loss:  2.4401822090148926  | , previous best loss:  2.449856758117676  | saving best model ...\n",
      "Current loss:  2.429342269897461  | , previous best loss:  2.4401822090148926  | saving best model ...\n",
      "Current loss:  2.422116756439209  | , previous best loss:  2.429342269897461  | saving best model ...\n",
      "Current loss:  2.4133453369140625  | , previous best loss:  2.422116756439209  | saving best model ...\n",
      "Current loss:  2.4027349948883057  | , previous best loss:  2.4133453369140625  | saving best model ...\n",
      "Current loss:  2.394909143447876  | , previous best loss:  2.4027349948883057  | saving best model ...\n",
      "Current loss:  2.386918544769287  | , previous best loss:  2.394909143447876  | saving best model ...\n",
      "Current loss:  2.377161979675293  | , previous best loss:  2.386918544769287  | saving best model ...\n",
      "Current loss:  2.3693161010742188  | , previous best loss:  2.377161979675293  | saving best model ...\n",
      "Current loss:  2.361931562423706  | , previous best loss:  2.3693161010742188  | saving best model ...\n",
      "Current loss:  2.3530547618865967  | , previous best loss:  2.361931562423706  | saving best model ...\n",
      "Current loss:  2.345386505126953  | , previous best loss:  2.3530547618865967  | saving best model ...\n",
      "Current loss:  2.338331937789917  | , previous best loss:  2.345386505126953  | saving best model ...\n",
      "Current loss:  2.330089569091797  | , previous best loss:  2.338331937789917  | saving best model ...\n",
      "Current loss:  2.3226146697998047  | , previous best loss:  2.330089569091797  | saving best model ...\n",
      "Current loss:  2.3157005310058594  | , previous best loss:  2.3226146697998047  | saving best model ...\n",
      "Current loss:  2.307873487472534  | , previous best loss:  2.3157005310058594  | saving best model ...\n",
      "Current loss:  2.3006086349487305  | , previous best loss:  2.307873487472534  | saving best model ...\n",
      "Current loss:  2.293800115585327  | , previous best loss:  2.3006086349487305  | saving best model ...\n",
      "Current loss:  2.2863388061523438  | , previous best loss:  2.293800115585327  | saving best model ...\n",
      "Current loss:  2.279388427734375  | , previous best loss:  2.2863388061523438  | saving best model ...\n",
      "Current loss:  2.272770404815674  | , previous best loss:  2.279388427734375  | saving best model ...\n",
      "Current loss:  2.2657079696655273  | , previous best loss:  2.272770404815674  | saving best model ...\n",
      "Current loss:  2.2591500282287598  | , previous best loss:  2.2657079696655273  | saving best model ...\n",
      "Current loss:  2.2527754306793213  | , previous best loss:  2.2591500282287598  | saving best model ...\n",
      "Current loss:  2.2461354732513428  | , previous best loss:  2.2527754306793213  | saving best model ...\n",
      "Current loss:  2.2399709224700928  | , previous best loss:  2.2461354732513428  | saving best model ...\n",
      "Current loss:  2.2338266372680664  | , previous best loss:  2.2399709224700928  | saving best model ...\n",
      "Current loss:  2.2275688648223877  | , previous best loss:  2.2338266372680664  | saving best model ...\n",
      "Current loss:  2.22170352935791  | , previous best loss:  2.2275688648223877  | saving best model ...\n",
      "Current loss:  2.2157366275787354  | , previous best loss:  2.22170352935791  | saving best model ...\n",
      "Current loss:  2.2098116874694824  | , previous best loss:  2.2157366275787354  | saving best model ...\n",
      "Current loss:  2.204148530960083  | , previous best loss:  2.2098116874694824  | saving best model ...\n",
      "Current loss:  2.1983461380004883  | , previous best loss:  2.204148530960083  | saving best model ...\n",
      "Current loss:  2.1927075386047363  | , previous best loss:  2.1983461380004883  | saving best model ...\n",
      "Current loss:  2.187183380126953  | , previous best loss:  2.1927075386047363  | saving best model ...\n",
      "Current loss:  2.1815896034240723  | , previous best loss:  2.187183380126953  | saving best model ...\n",
      "Current loss:  2.1762075424194336  | , previous best loss:  2.1815896034240723  | saving best model ...\n",
      "Current loss:  2.1708221435546875  | , previous best loss:  2.1762075424194336  | saving best model ...\n",
      "Current loss:  2.1654863357543945  | , previous best loss:  2.1708221435546875  | saving best model ...\n",
      "Current loss:  2.1603076457977295  | , previous best loss:  2.1654863357543945  | saving best model ...\n",
      "Current loss:  2.1550979614257812  | , previous best loss:  2.1603076457977295  | saving best model ...\n",
      "Current loss:  2.150026559829712  | , previous best loss:  2.1550979614257812  | saving best model ...\n",
      "Current loss:  2.145005941390991  | , previous best loss:  2.150026559829712  | saving best model ...\n",
      "Current loss:  2.1400129795074463  | , previous best loss:  2.145005941390991  | saving best model ...\n",
      "Current loss:  2.135146379470825  | , previous best loss:  2.1400129795074463  | saving best model ...\n",
      "Current loss:  2.1302788257598877  | , previous best loss:  2.135146379470825  | saving best model ...\n",
      "Current loss:  2.125511646270752  | , previous best loss:  2.1302788257598877  | saving best model ...\n",
      "Current loss:  2.1207950115203857  | , previous best loss:  2.125511646270752  | saving best model ...\n",
      "Current loss:  2.116117000579834  | , previous best loss:  2.1207950115203857  | saving best model ...\n",
      "Current loss:  2.1115353107452393  | , previous best loss:  2.116117000579834  | saving best model ...\n",
      "Current loss:  2.1069748401641846  | , previous best loss:  2.1115353107452393  | saving best model ...\n",
      "Current loss:  2.1025078296661377  | , previous best loss:  2.1069748401641846  | saving best model ...\n",
      "Current loss:  2.0980849266052246  | , previous best loss:  2.1025078296661377  | saving best model ...\n",
      "Current loss:  2.0937273502349854  | , previous best loss:  2.0980849266052246  | saving best model ...\n",
      "Current loss:  2.0894460678100586  | , previous best loss:  2.0937273502349854  | saving best model ...\n",
      "Current loss:  2.0852115154266357  | , previous best loss:  2.0894460678100586  | saving best model ...\n",
      "Current loss:  2.081066608428955  | , previous best loss:  2.0852115154266357  | saving best model ...\n",
      "Current loss:  2.0769684314727783  | , previous best loss:  2.081066608428955  | saving best model ...\n",
      "Current loss:  2.0729563236236572  | , previous best loss:  2.0769684314727783  | saving best model ...\n",
      "Current loss:  2.069004774093628  | , previous best loss:  2.0729563236236572  | saving best model ...\n",
      "Current loss:  2.0651297569274902  | , previous best loss:  2.069004774093628  | saving best model ...\n",
      "Current loss:  2.0613293647766113  | , previous best loss:  2.0651297569274902  | saving best model ...\n",
      "Current loss:  2.057598114013672  | , previous best loss:  2.0613293647766113  | saving best model ...\n",
      "Current loss:  2.0539519786834717  | , previous best loss:  2.057598114013672  | saving best model ...\n",
      "Current loss:  2.0503737926483154  | , previous best loss:  2.0539519786834717  | saving best model ...\n",
      "Current loss:  2.046884775161743  | , previous best loss:  2.0503737926483154  | saving best model ...\n",
      "Current loss:  2.0434675216674805  | , previous best loss:  2.046884775161743  | saving best model ...\n",
      "Current loss:  2.0401391983032227  | , previous best loss:  2.0434675216674805  | saving best model ...\n",
      "Current loss:  2.036885976791382  | , previous best loss:  2.0401391983032227  | saving best model ...\n",
      "Current loss:  2.0337209701538086  | , previous best loss:  2.036885976791382  | saving best model ...\n",
      "Current loss:  2.030632972717285  | , previous best loss:  2.0337209701538086  | saving best model ...\n",
      "Current loss:  2.0276296138763428  | , previous best loss:  2.030632972717285  | saving best model ...\n",
      "Current loss:  2.024705410003662  | , previous best loss:  2.0276296138763428  | saving best model ...\n",
      "Current loss:  2.0218617916107178  | , previous best loss:  2.024705410003662  | saving best model ...\n",
      "Current loss:  2.0190956592559814  | , previous best loss:  2.0218617916107178  | saving best model ...\n",
      "Current loss:  2.0164053440093994  | , previous best loss:  2.0190956592559814  | saving best model ...\n",
      "Current loss:  2.013789415359497  | , previous best loss:  2.0164053440093994  | saving best model ...\n",
      "Current loss:  2.011244535446167  | , previous best loss:  2.013789415359497  | saving best model ...\n",
      "Current loss:  2.0087685585021973  | , previous best loss:  2.011244535446167  | saving best model ...\n",
      "Current loss:  2.0063576698303223  | , previous best loss:  2.0087685585021973  | saving best model ...\n",
      "Current loss:  2.0040090084075928  | , previous best loss:  2.0063576698303223  | saving best model ...\n",
      "Current loss:  2.001718759536743  | , previous best loss:  2.0040090084075928  | saving best model ...\n",
      "Current loss:  1.9994829893112183  | , previous best loss:  2.001718759536743  | saving best model ...\n",
      "Current loss:  1.9972978830337524  | , previous best loss:  1.9994829893112183  | saving best model ...\n",
      "Current loss:  1.9951595067977905  | , previous best loss:  1.9972978830337524  | saving best model ...\n",
      "Current loss:  1.9930639266967773  | , previous best loss:  1.9951595067977905  | saving best model ...\n",
      "Current loss:  1.9910061359405518  | , previous best loss:  1.9930639266967773  | saving best model ...\n",
      "Current loss:  1.988983154296875  | , previous best loss:  1.9910061359405518  | saving best model ...\n",
      "Current loss:  1.986990213394165  | , previous best loss:  1.988983154296875  | saving best model ...\n",
      "Current loss:  1.9850245714187622  | , previous best loss:  1.986990213394165  | saving best model ...\n",
      "Current loss:  1.9830812215805054  | , previous best loss:  1.9850245714187622  | saving best model ...\n",
      "Current loss:  1.9811577796936035  | , previous best loss:  1.9830812215805054  | saving best model ...\n",
      "Current loss:  1.9792507886886597  | , previous best loss:  1.9811577796936035  | saving best model ...\n",
      "Current loss:  1.9773571491241455  | , previous best loss:  1.9792507886886597  | saving best model ...\n",
      "Current loss:  1.9754737615585327  | , previous best loss:  1.9773571491241455  | saving best model ...\n",
      "Current loss:  1.9735997915267944  | , previous best loss:  1.9754737615585327  | saving best model ...\n",
      "Current loss:  1.9717309474945068  | , previous best loss:  1.9735997915267944  | saving best model ...\n",
      "Current loss:  1.969866156578064  | , previous best loss:  1.9717309474945068  | saving best model ...\n",
      "Current loss:  1.9680041074752808  | , previous best loss:  1.969866156578064  | saving best model ...\n",
      "Current loss:  1.966142177581787  | , previous best loss:  1.9680041074752808  | saving best model ...\n",
      "Current loss:  1.9642800092697144  | , previous best loss:  1.966142177581787  | saving best model ...\n",
      "Current loss:  1.962416172027588  | , previous best loss:  1.9642800092697144  | saving best model ...\n",
      "Current loss:  1.96055006980896  | , previous best loss:  1.962416172027588  | saving best model ...\n",
      "Current loss:  1.9586800336837769  | , previous best loss:  1.96055006980896  | saving best model ...\n",
      "Current loss:  1.9568063020706177  | , previous best loss:  1.9586800336837769  | saving best model ...\n",
      "Current loss:  1.954927921295166  | , previous best loss:  1.9568063020706177  | saving best model ...\n",
      "Current loss:  1.9530439376831055  | , previous best loss:  1.954927921295166  | saving best model ...\n",
      "Current loss:  1.9511547088623047  | , previous best loss:  1.9530439376831055  | saving best model ...\n",
      "Current loss:  1.949259638786316  | , previous best loss:  1.9511547088623047  | saving best model ...\n",
      "Current loss:  1.9473580121994019  | , previous best loss:  1.949259638786316  | saving best model ...\n",
      "Current loss:  1.9454498291015625  | , previous best loss:  1.9473580121994019  | saving best model ...\n",
      "Current loss:  1.943534255027771  | , previous best loss:  1.9454498291015625  | saving best model ...\n",
      "Current loss:  1.9416112899780273  | , previous best loss:  1.943534255027771  | saving best model ...\n",
      "Current loss:  1.9396816492080688  | , previous best loss:  1.9416112899780273  | saving best model ...\n",
      "Current loss:  1.9377435445785522  | , previous best loss:  1.9396816492080688  | saving best model ...\n",
      "Current loss:  1.935797095298767  | , previous best loss:  1.9377435445785522  | saving best model ...\n",
      "Current loss:  1.9338418245315552  | , previous best loss:  1.935797095298767  | saving best model ...\n",
      "Current loss:  1.9318777322769165  | , previous best loss:  1.9338418245315552  | saving best model ...\n",
      "Current loss:  1.9299044609069824  | , previous best loss:  1.9318777322769165  | saving best model ...\n",
      "Current loss:  1.9279210567474365  | , previous best loss:  1.9299044609069824  | saving best model ...\n",
      "Current loss:  1.925927996635437  | , previous best loss:  1.9279210567474365  | saving best model ...\n",
      "Current loss:  1.923924207687378  | , previous best loss:  1.925927996635437  | saving best model ...\n",
      "Current loss:  1.9219094514846802  | , previous best loss:  1.923924207687378  | saving best model ...\n",
      "Current loss:  1.9198837280273438  | , previous best loss:  1.9219094514846802  | saving best model ...\n",
      "Current loss:  1.9178463220596313  | , previous best loss:  1.9198837280273438  | saving best model ...\n",
      "Current loss:  1.9157962799072266  | , previous best loss:  1.9178463220596313  | saving best model ...\n",
      "Current loss:  1.9137344360351562  | , previous best loss:  1.9157962799072266  | saving best model ...\n",
      "Current loss:  1.9116592407226562  | , previous best loss:  1.9137344360351562  | saving best model ...\n",
      "Current loss:  1.9095709323883057  | , previous best loss:  1.9116592407226562  | saving best model ...\n",
      "Current loss:  1.9074690341949463  | , previous best loss:  1.9095709323883057  | saving best model ...\n",
      "Current loss:  1.90535306930542  | , previous best loss:  1.9074690341949463  | saving best model ...\n",
      "Current loss:  1.903222918510437  | , previous best loss:  1.90535306930542  | saving best model ...\n",
      "Current loss:  1.9010781049728394  | , previous best loss:  1.903222918510437  | saving best model ...\n",
      "Current loss:  1.8989183902740479  | , previous best loss:  1.9010781049728394  | saving best model ...\n",
      "Current loss:  1.8967431783676147  | , previous best loss:  1.8989183902740479  | saving best model ...\n",
      "Current loss:  1.894552230834961  | , previous best loss:  1.8967431783676147  | saving best model ...\n",
      "Current loss:  1.8923451900482178  | , previous best loss:  1.894552230834961  | saving best model ...\n",
      "Current loss:  1.8901220560073853  | , previous best loss:  1.8923451900482178  | saving best model ...\n",
      "Current loss:  1.8878819942474365  | , previous best loss:  1.8901220560073853  | saving best model ...\n",
      "Current loss:  1.8856256008148193  | , previous best loss:  1.8878819942474365  | saving best model ...\n",
      "Current loss:  1.8833510875701904  | , previous best loss:  1.8856256008148193  | saving best model ...\n",
      "Current loss:  1.8810595273971558  | , previous best loss:  1.8833510875701904  | saving best model ...\n",
      "Current loss:  1.8787496089935303  | , previous best loss:  1.8810595273971558  | saving best model ...\n",
      "Current loss:  1.8764214515686035  | , previous best loss:  1.8787496089935303  | saving best model ...\n",
      "Current loss:  1.8740748167037964  | , previous best loss:  1.8764214515686035  | saving best model ...\n",
      "Current loss:  1.8717092275619507  | , previous best loss:  1.8740748167037964  | saving best model ...\n",
      "Current loss:  1.8693245649337769  | , previous best loss:  1.8717092275619507  | saving best model ...\n",
      "Current loss:  1.866919994354248  | , previous best loss:  1.8693245649337769  | saving best model ...\n",
      "Current loss:  1.8644956350326538  | , previous best loss:  1.866919994354248  | saving best model ...\n",
      "Current loss:  1.862051010131836  | , previous best loss:  1.8644956350326538  | saving best model ...\n",
      "Current loss:  1.8595855236053467  | , previous best loss:  1.862051010131836  | saving best model ...\n",
      "Current loss:  1.8570997714996338  | , previous best loss:  1.8595855236053467  | saving best model ...\n",
      "Current loss:  1.854592204093933  | , previous best loss:  1.8570997714996338  | saving best model ...\n",
      "Current loss:  1.8520628213882446  | , previous best loss:  1.854592204093933  | saving best model ...\n",
      "Current loss:  1.849511981010437  | , previous best loss:  1.8520628213882446  | saving best model ...\n",
      "Current loss:  1.8469384908676147  | , previous best loss:  1.849511981010437  | saving best model ...\n",
      "Current loss:  1.8443421125411987  | , previous best loss:  1.8469384908676147  | saving best model ...\n",
      "Current loss:  1.8417229652404785  | , previous best loss:  1.8443421125411987  | saving best model ...\n",
      "Current loss:  1.839080572128296  | , previous best loss:  1.8417229652404785  | saving best model ...\n",
      "Current loss:  1.8364145755767822  | , previous best loss:  1.839080572128296  | saving best model ...\n",
      "Current loss:  1.8337243795394897  | , previous best loss:  1.8364145755767822  | saving best model ...\n",
      "Current loss:  1.8310097455978394  | , previous best loss:  1.8337243795394897  | saving best model ...\n",
      "Current loss:  1.8282705545425415  | , previous best loss:  1.8310097455978394  | saving best model ...\n",
      "Current loss:  1.8255059719085693  | , previous best loss:  1.8282705545425415  | saving best model ...\n",
      "Current loss:  1.8227163553237915  | , previous best loss:  1.8255059719085693  | saving best model ...\n",
      "Current loss:  1.8199008703231812  | , previous best loss:  1.8227163553237915  | saving best model ...\n",
      "Current loss:  1.8170593976974487  | , previous best loss:  1.8199008703231812  | saving best model ...\n",
      "Current loss:  1.8141915798187256  | , previous best loss:  1.8170593976974487  | saving best model ...\n",
      "Current loss:  1.8112969398498535  | , previous best loss:  1.8141915798187256  | saving best model ...\n",
      "Current loss:  1.8083754777908325  | , previous best loss:  1.8112969398498535  | saving best model ...\n",
      "Current loss:  1.8054265975952148  | , previous best loss:  1.8083754777908325  | saving best model ...\n",
      "Current loss:  1.80245041847229  | , previous best loss:  1.8054265975952148  | saving best model ...\n",
      "Current loss:  1.7994457483291626  | , previous best loss:  1.80245041847229  | saving best model ...\n",
      "Current loss:  1.7964129447937012  | , previous best loss:  1.7994457483291626  | saving best model ...\n",
      "Current loss:  1.7933520078659058  | , previous best loss:  1.7964129447937012  | saving best model ...\n",
      "Current loss:  1.7902617454528809  | , previous best loss:  1.7933520078659058  | saving best model ...\n",
      "Current loss:  1.787142276763916  | , previous best loss:  1.7902617454528809  | saving best model ...\n",
      "Current loss:  1.7839936017990112  | , previous best loss:  1.787142276763916  | saving best model ...\n",
      "Current loss:  1.7808153629302979  | , previous best loss:  1.7839936017990112  | saving best model ...\n",
      "Current loss:  1.777606725692749  | , previous best loss:  1.7808153629302979  | saving best model ...\n",
      "Current loss:  1.7743676900863647  | , previous best loss:  1.777606725692749  | saving best model ...\n",
      "Current loss:  1.771098256111145  | , previous best loss:  1.7743676900863647  | saving best model ...\n",
      "Current loss:  1.7677979469299316  | , previous best loss:  1.771098256111145  | saving best model ...\n",
      "Current loss:  1.7644667625427246  | , previous best loss:  1.7677979469299316  | saving best model ...\n",
      "Current loss:  1.7611037492752075  | , previous best loss:  1.7644667625427246  | saving best model ...\n",
      "Current loss:  1.757709264755249  | , previous best loss:  1.7611037492752075  | saving best model ...\n",
      "Current loss:  1.7542829513549805  | , previous best loss:  1.757709264755249  | saving best model ...\n",
      "Current loss:  1.7508245706558228  | , previous best loss:  1.7542829513549805  | saving best model ...\n",
      "Current loss:  1.7473340034484863  | , previous best loss:  1.7508245706558228  | saving best model ...\n",
      "Current loss:  1.7438106536865234  | , previous best loss:  1.7473340034484863  | saving best model ...\n",
      "Current loss:  1.7402541637420654  | , previous best loss:  1.7438106536865234  | saving best model ...\n",
      "Current loss:  1.736664891242981  | , previous best loss:  1.7402541637420654  | saving best model ...\n",
      "Current loss:  1.73304283618927  | , previous best loss:  1.736664891242981  | saving best model ...\n",
      "Current loss:  1.7293871641159058  | , previous best loss:  1.73304283618927  | saving best model ...\n",
      "Current loss:  1.7257006168365479  | , previous best loss:  1.7293871641159058  | saving best model ...\n",
      "Current loss:  1.7219927310943604  | , previous best loss:  1.7257006168365479  | saving best model ...\n",
      "Current loss:  1.7183259725570679  | , previous best loss:  1.7219927310943604  | saving best model ...\n",
      "Current loss:  1.7150808572769165  | , previous best loss:  1.7183259725570679  | saving best model ...\n",
      "Current loss:  1.714699625968933  | , previous best loss:  1.7150808572769165  | saving best model ...\n",
      "Current loss:  1.6933144330978394  | , previous best loss:  1.714699625968933  | saving best model ...\n",
      "Current loss:  1.6864643096923828  | , previous best loss:  1.6933144330978394  | saving best model ...\n",
      "Current loss:  1.6711034774780273  | , previous best loss:  1.6864643096923828  | saving best model ...\n",
      "Current loss:  1.6558423042297363  | , previous best loss:  1.6711034774780273  | saving best model ...\n",
      "Current loss:  1.6454116106033325  | , previous best loss:  1.6558423042297363  | saving best model ...\n",
      "Current loss:  1.6437492370605469  | , previous best loss:  1.6454116106033325  | saving best model ...\n",
      "Current loss:  1.6426464319229126  | , previous best loss:  1.6437492370605469  | saving best model ...\n",
      "Current loss:  1.628174901008606  | , previous best loss:  1.6426464319229126  | saving best model ...\n",
      "Current loss:  1.621335744857788  | , previous best loss:  1.628174901008606  | saving best model ...\n",
      "Current loss:  1.6203391551971436  | , previous best loss:  1.621335744857788  | saving best model ...\n",
      "Current loss:  1.6175543069839478  | , previous best loss:  1.6203391551971436  | saving best model ...\n",
      "Current loss:  1.6065584421157837  | , previous best loss:  1.6175543069839478  | saving best model ...\n",
      "Current loss:  1.6019270420074463  | , previous best loss:  1.6065584421157837  | saving best model ...\n",
      "Current loss:  1.601900577545166  | , previous best loss:  1.6019270420074463  | saving best model ...\n",
      "Current loss:  1.6000131368637085  | , previous best loss:  1.601900577545166  | saving best model ...\n",
      "Current loss:  1.5914546251296997  | , previous best loss:  1.6000131368637085  | saving best model ...\n",
      "Current loss:  1.5863796472549438  | , previous best loss:  1.5914546251296997  | saving best model ...\n",
      "Current loss:  1.5841323137283325  | , previous best loss:  1.5863796472549438  | saving best model ...\n",
      "Current loss:  1.5831223726272583  | , previous best loss:  1.5841323137283325  | saving best model ...\n",
      "Current loss:  1.5756865739822388  | , previous best loss:  1.5831223726272583  | saving best model ...\n",
      "Current loss:  1.57154381275177  | , previous best loss:  1.5756865739822388  | saving best model ...\n",
      "Current loss:  1.5663954019546509  | , previous best loss:  1.57154381275177  | saving best model ...\n",
      "Current loss:  1.5659441947937012  | , previous best loss:  1.5663954019546509  | saving best model ...\n",
      "Current loss:  1.5608092546463013  | , previous best loss:  1.5659441947937012  | saving best model ...\n",
      "Current loss:  1.557083249092102  | , previous best loss:  1.5608092546463013  | saving best model ...\n",
      "Current loss:  1.5557154417037964  | , previous best loss:  1.557083249092102  | saving best model ...\n",
      "Current loss:  1.5507136583328247  | , previous best loss:  1.5557154417037964  | saving best model ...\n",
      "Current loss:  1.5474046468734741  | , previous best loss:  1.5507136583328247  | saving best model ...\n",
      "Current loss:  1.5454833507537842  | , previous best loss:  1.5474046468734741  | saving best model ...\n",
      "Current loss:  1.540907859802246  | , previous best loss:  1.5454833507537842  | saving best model ...\n",
      "Current loss:  1.5374571084976196  | , previous best loss:  1.540907859802246  | saving best model ...\n",
      "Current loss:  1.535282850265503  | , previous best loss:  1.5374571084976196  | saving best model ...\n",
      "Current loss:  1.531230092048645  | , previous best loss:  1.535282850265503  | saving best model ...\n",
      "Current loss:  1.5274250507354736  | , previous best loss:  1.531230092048645  | saving best model ...\n",
      "Current loss:  1.5249912738800049  | , previous best loss:  1.5274250507354736  | saving best model ...\n",
      "Current loss:  1.521572232246399  | , previous best loss:  1.5249912738800049  | saving best model ...\n",
      "Current loss:  1.5175353288650513  | , previous best loss:  1.521572232246399  | saving best model ...\n",
      "Current loss:  1.5145604610443115  | , previous best loss:  1.5175353288650513  | saving best model ...\n",
      "Current loss:  1.5116863250732422  | , previous best loss:  1.5145604610443115  | saving best model ...\n",
      "Current loss:  1.5079270601272583  | , previous best loss:  1.5116863250732422  | saving best model ...\n",
      "Current loss:  1.5042834281921387  | , previous best loss:  1.5079270601272583  | saving best model ...\n",
      "Current loss:  1.5013259649276733  | , previous best loss:  1.5042834281921387  | saving best model ...\n",
      "Current loss:  1.4982210397720337  | , previous best loss:  1.5013259649276733  | saving best model ...\n",
      "Current loss:  1.494570255279541  | , previous best loss:  1.4982210397720337  | saving best model ...\n",
      "Current loss:  1.4909905195236206  | , previous best loss:  1.494570255279541  | saving best model ...\n",
      "Current loss:  1.4878449440002441  | , previous best loss:  1.4909905195236206  | saving best model ...\n",
      "Current loss:  1.4847506284713745  | , previous best loss:  1.4878449440002441  | saving best model ...\n",
      "Current loss:  1.481341004371643  | , previous best loss:  1.4847506284713745  | saving best model ...\n",
      "Current loss:  1.4777562618255615  | , previous best loss:  1.481341004371643  | saving best model ...\n",
      "Current loss:  1.4743201732635498  | , previous best loss:  1.4777562618255615  | saving best model ...\n",
      "Current loss:  1.4711021184921265  | , previous best loss:  1.4743201732635498  | saving best model ...\n",
      "Current loss:  1.467926263809204  | , previous best loss:  1.4711021184921265  | saving best model ...\n",
      "Current loss:  1.4646302461624146  | , previous best loss:  1.467926263809204  | saving best model ...\n",
      "Current loss:  1.461198091506958  | , previous best loss:  1.4646302461624146  | saving best model ...\n",
      "Current loss:  1.4577221870422363  | , previous best loss:  1.461198091506958  | saving best model ...\n",
      "Current loss:  1.4542876482009888  | , previous best loss:  1.4577221870422363  | saving best model ...\n",
      "Current loss:  1.450927734375  | , previous best loss:  1.4542876482009888  | saving best model ...\n",
      "Current loss:  1.4476304054260254  | , previous best loss:  1.450927734375  | saving best model ...\n",
      "Current loss:  1.444372534751892  | , previous best loss:  1.4476304054260254  | saving best model ...\n",
      "Current loss:  1.4411431550979614  | , previous best loss:  1.444372534751892  | saving best model ...\n",
      "Current loss:  1.4379525184631348  | , previous best loss:  1.4411431550979614  | saving best model ...\n",
      "Current loss:  1.4348479509353638  | , previous best loss:  1.4379525184631348  | saving best model ...\n",
      "Current loss:  1.4319292306900024  | , previous best loss:  1.4348479509353638  | saving best model ...\n",
      "Current loss:  1.4294389486312866  | , previous best loss:  1.4319292306900024  | saving best model ...\n",
      "Current loss:  1.4279460906982422  | , previous best loss:  1.4294389486312866  | saving best model ...\n",
      "Current loss:  1.399117112159729  | , previous best loss:  1.4279460906982422  | saving best model ...\n",
      "Current loss:  1.3907791376113892  | , previous best loss:  1.399117112159729  | saving best model ...\n",
      "Current loss:  1.379294991493225  | , previous best loss:  1.3907791376113892  | saving best model ...\n",
      "Current loss:  1.3700896501541138  | , previous best loss:  1.379294991493225  | saving best model ...\n",
      "Current loss:  1.3617626428604126  | , previous best loss:  1.3700896501541138  | saving best model ...\n",
      "Current loss:  1.354577660560608  | , previous best loss:  1.3617626428604126  | saving best model ...\n",
      "Current loss:  1.3493521213531494  | , previous best loss:  1.354577660560608  | saving best model ...\n",
      "Current loss:  1.346505880355835  | , previous best loss:  1.3493521213531494  | saving best model ...\n",
      "Current loss:  1.342035174369812  | , previous best loss:  1.346505880355835  | saving best model ...\n",
      "Current loss:  1.3295540809631348  | , previous best loss:  1.342035174369812  | saving best model ...\n",
      "Current loss:  1.3245781660079956  | , previous best loss:  1.3295540809631348  | saving best model ...\n",
      "Current loss:  1.321397066116333  | , previous best loss:  1.3245781660079956  | saving best model ...\n",
      "Current loss:  1.3118394613265991  | , previous best loss:  1.321397066116333  | saving best model ...\n",
      "Current loss:  1.311306118965149  | , previous best loss:  1.3118394613265991  | saving best model ...\n",
      "Current loss:  1.3098112344741821  | , previous best loss:  1.311306118965149  | saving best model ...\n",
      "Current loss:  1.3012574911117554  | , previous best loss:  1.3098112344741821  | saving best model ...\n",
      "Current loss:  1.2991536855697632  | , previous best loss:  1.3012574911117554  | saving best model ...\n",
      "Current loss:  1.298553466796875  | , previous best loss:  1.2991536855697632  | saving best model ...\n",
      "Current loss:  1.2919650077819824  | , previous best loss:  1.298553466796875  | saving best model ...\n",
      "Current loss:  1.2871780395507812  | , previous best loss:  1.2919650077819824  | saving best model ...\n",
      "Current loss:  1.286417841911316  | , previous best loss:  1.2871780395507812  | saving best model ...\n",
      "Current loss:  1.2862375974655151  | , previous best loss:  1.286417841911316  | saving best model ...\n",
      "Current loss:  1.2833311557769775  | , previous best loss:  1.2862375974655151  | saving best model ...\n",
      "Current loss:  1.2782984972000122  | , previous best loss:  1.2833311557769775  | saving best model ...\n",
      "Current loss:  1.273891568183899  | , previous best loss:  1.2782984972000122  | saving best model ...\n",
      "Current loss:  1.2715431451797485  | , previous best loss:  1.273891568183899  | saving best model ...\n",
      "Current loss:  1.270447015762329  | , previous best loss:  1.2715431451797485  | saving best model ...\n",
      "Current loss:  1.2689990997314453  | , previous best loss:  1.270447015762329  | saving best model ...\n",
      "Current loss:  1.2663270235061646  | , previous best loss:  1.2689990997314453  | saving best model ...\n",
      "Current loss:  1.2626677751541138  | , previous best loss:  1.2663270235061646  | saving best model ...\n",
      "Current loss:  1.2587618827819824  | , previous best loss:  1.2626677751541138  | saving best model ...\n",
      "Current loss:  1.255237102508545  | , previous best loss:  1.2587618827819824  | saving best model ...\n",
      "Current loss:  1.2523174285888672  | , previous best loss:  1.255237102508545  | saving best model ...\n",
      "Current loss:  1.2499243021011353  | , previous best loss:  1.2523174285888672  | saving best model ...\n",
      "Current loss:  1.2478797435760498  | , previous best loss:  1.2499243021011353  | saving best model ...\n",
      "Current loss:  1.2460641860961914  | , previous best loss:  1.2478797435760498  | saving best model ...\n",
      "Current loss:  1.2445000410079956  | , previous best loss:  1.2460641860961914  | saving best model ...\n",
      "Current loss:  1.2433781623840332  | , previous best loss:  1.2445000410079956  | saving best model ...\n",
      "Current loss:  1.2431718111038208  | , previous best loss:  1.2433781623840332  | saving best model ...\n",
      "Current loss:  1.212440848350525  | , previous best loss:  1.2431718111038208  | saving best model ...\n",
      "Current loss:  1.206434965133667  | , previous best loss:  1.212440848350525  | saving best model ...\n",
      "Current loss:  1.201599359512329  | , previous best loss:  1.206434965133667  | saving best model ...\n",
      "Current loss:  1.1910054683685303  | , previous best loss:  1.201599359512329  | saving best model ...\n",
      "Current loss:  1.1784359216690063  | , previous best loss:  1.1910054683685303  | saving best model ...\n",
      "Current loss:  1.170059323310852  | , previous best loss:  1.1784359216690063  | saving best model ...\n",
      "Current loss:  1.1608948707580566  | , previous best loss:  1.170059323310852  | saving best model ...\n",
      "Current loss:  1.1524910926818848  | , previous best loss:  1.1608948707580566  | saving best model ...\n",
      "Current loss:  1.1482279300689697  | , previous best loss:  1.1524910926818848  | saving best model ...\n",
      "Current loss:  1.139467477798462  | , previous best loss:  1.1482279300689697  | saving best model ...\n",
      "Current loss:  1.136294960975647  | , previous best loss:  1.139467477798462  | saving best model ...\n",
      "Current loss:  1.1282659769058228  | , previous best loss:  1.136294960975647  | saving best model ...\n",
      "Current loss:  1.1246870756149292  | , previous best loss:  1.1282659769058228  | saving best model ...\n",
      "Current loss:  1.1220985651016235  | , previous best loss:  1.1246870756149292  | saving best model ...\n",
      "Current loss:  1.1167434453964233  | , previous best loss:  1.1220985651016235  | saving best model ...\n",
      "Current loss:  1.1113675832748413  | , previous best loss:  1.1167434453964233  | saving best model ...\n",
      "Current loss:  1.1070103645324707  | , previous best loss:  1.1113675832748413  | saving best model ...\n",
      "Current loss:  1.1039412021636963  | , previous best loss:  1.1070103645324707  | saving best model ...\n",
      "Current loss:  1.1019388437271118  | , previous best loss:  1.1039412021636963  | saving best model ...\n",
      "Current loss:  1.1006922721862793  | , previous best loss:  1.1019388437271118  | saving best model ...\n",
      "Current loss:  1.1000730991363525  | , previous best loss:  1.1006922721862793  | saving best model ...\n",
      "Current loss:  1.0758177042007446  | , previous best loss:  1.1000730991363525  | saving best model ...\n",
      "Current loss:  1.0658318996429443  | , previous best loss:  1.0758177042007446  | saving best model ...\n",
      "Current loss:  1.0500215291976929  | , previous best loss:  1.0658318996429443  | saving best model ...\n",
      "Current loss:  1.0408302545547485  | , previous best loss:  1.0500215291976929  | saving best model ...\n",
      "Current loss:  1.0297659635543823  | , previous best loss:  1.0408302545547485  | saving best model ...\n",
      "Current loss:  1.0214745998382568  | , previous best loss:  1.0297659635543823  | saving best model ...\n",
      "Current loss:  1.0147970914840698  | , previous best loss:  1.0214745998382568  | saving best model ...\n",
      "Current loss:  1.0126124620437622  | , previous best loss:  1.0147970914840698  | saving best model ...\n",
      "Current loss:  1.0114623308181763  | , previous best loss:  1.0126124620437622  | saving best model ...\n",
      "Current loss:  1.0020267963409424  | , previous best loss:  1.0114623308181763  | saving best model ...\n",
      "Current loss:  1.0000203847885132  | , previous best loss:  1.0020267963409424  | saving best model ...\n",
      "Current loss:  0.992461085319519  | , previous best loss:  1.0000203847885132  | saving best model ...\n",
      "Current loss:  0.9888230562210083  | , previous best loss:  0.992461085319519  | saving best model ...\n",
      "Current loss:  0.9867618083953857  | , previous best loss:  0.9888230562210083  | saving best model ...\n",
      "Current loss:  0.9818128943443298  | , previous best loss:  0.9867618083953857  | saving best model ...\n",
      "Current loss:  0.9771538972854614  | , previous best loss:  0.9818128943443298  | saving best model ...\n",
      "Current loss:  0.9738418459892273  | , previous best loss:  0.9771538972854614  | saving best model ...\n",
      "Current loss:  0.9719991683959961  | , previous best loss:  0.9738418459892273  | saving best model ...\n",
      "Current loss:  0.9711846709251404  | , previous best loss:  0.9719991683959961  | saving best model ...\n",
      "Current loss:  0.9708729386329651  | , previous best loss:  0.9711846709251404  | saving best model ...\n",
      "Current loss:  0.97076016664505  | , previous best loss:  0.9708729386329651  | saving best model ...\n",
      "Current loss:  0.9520596265792847  | , previous best loss:  0.97076016664505  | saving best model ...\n"
     ]
    }
   ],
   "source": [
    "for t in range(Iteration):\n",
    "    # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "    pyb_af = DeepBS(X_train)\n",
    "    loss = criterion(y_train, pyb_af); bloss_list.append(loss.item())\n",
    "    \n",
    "    if (t > 0) and ((bloss_list[t-1]-bloss_list[t])<tor):        \n",
    "        if (tpat != 0) and (tpat % patientr) == 0:\n",
    "            learning_r *= 0.2 \n",
    "            tpat += 1\n",
    "            #print('Learning rate reduce to ', learning_r)\n",
    "            optimizer = torch.optim.Adam(DeepBS.parameters(), lr=learning_r)\n",
    "            if learning_r <= lr_tor:\n",
    "                if t < patientc + 1:\n",
    "                    conv = False\n",
    "                else:\n",
    "                    conv = True\n",
    "                print('Convergence!')\n",
    "                break\n",
    "        elif tpat < patientc:\n",
    "            tpat += 1\n",
    "            pass\n",
    "        else:\n",
    "            if t < patientc + 1:\n",
    "                conv = False\n",
    "            else:\n",
    "                conv = True\n",
    "            print('Convergence!')\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        if loss < bloss:\n",
    "            print('Current loss: ', loss.item(), ' | , previous best loss: ', bloss, ' | saving best model ...')\n",
    "            torch.save(DeepBS.state_dict(), './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1))\n",
    "            bloss = loss.item()\n",
    "            tpat = 0\n",
    "        else:\n",
    "            tpat += 1\n",
    "    \n",
    "    if tpat == patientc:\n",
    "        if t < patientc + 1:\n",
    "            conv = False\n",
    "        else:\n",
    "            conv = True\n",
    "            print('Convergence!')\n",
    "        break\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fb6f326c-ebc9-4069-9d1b-45c07561b046",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [86], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDPS\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_knots\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neurons\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bsl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload( \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./EXA\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(X_train\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(nm)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(nk)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(d\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), weights_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'dropout'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = 2, output_dim = Fout, bias = True).to(device)\n",
    "model.load_state_dict(torch.load( './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1), weights_only = True))\n",
    "\n",
    "with torch.no_grad():\n",
    "    pyb_af = model(X_train)\n",
    "    print(criterion(y_train, pyb_af))\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(pyb_af.cpu().detach().numpy(), y_train.cpu().detach().numpy())\n",
    "    pyb_af = model(X_test)\n",
    "    plt.subplot(122)\n",
    "    print(criterion(y_test, pyb_af))\n",
    "    plt.scatter(pyb_af.cpu().detach().numpy(), y_test.cpu().detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c8763-5ba2-483c-bdf1-dafef5db12a0",
   "metadata": {},
   "source": [
    "## ECM-Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bd5ec9-cd18-45fb-aa4a-853660da01be",
   "metadata": {},
   "source": [
    "### Via ECM to implement layer-wise optimization for tuning the weight for B-Spline Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3951de64-2fdf-4ce1-9c88-8923f465da77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda:  tensor([0.2663, 0.1106])\n",
      "Lambda:  tensor([0.2664, 0.1104])\n",
      "Lambda:  tensor([0.2664, 0.1103])\n",
      "Lambda:  tensor([0.2665, 0.1101])\n",
      "Lambda:  tensor([0.2666, 0.1100])\n"
     ]
    }
   ],
   "source": [
    "model = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = 2, output_dim = Fout, bias = True).to(device)\n",
    "model.load_state_dict(torch.load( './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1), weights_only = True))\n",
    "\n",
    "BestLambda = ECM_update(model, 10, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8726ee6-329a-4e2e-bcfe-61e758d03550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7500cd15-e680-4062-9469-0f142ed368a1",
   "metadata": {},
   "source": [
    "### Fast-Tuning for optimimal DPS parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8bb3d17b-eb10-47ac-983d-2f655fad7d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepPS = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = 2, output_dim = Fout, bias = True).to(device)\n",
    "DeepPS.load_state_dict(torch.load( './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1), weights_only = True))\n",
    "learning_r = 1e-1\n",
    "optimizer = torch.optim.Adam(DeepPS.parameters(), lr=learning_r)\n",
    "Iteration = 10000; bloss_list = []; tor = 1e-5; lr_tor = 1e-6\n",
    "patientc = 30; patientr = 10; tpat = 0; bloss = 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf8f85-c7d8-499b-9452-dd8f7aafe322",
   "metadata": {},
   "source": [
    "## Evaluation on DS and DPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef2fa0-ed52-4f74-aa33-c8eee06dd6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fast_tun_epoch = 1001\n",
    "for d in range(ndf):\n",
    "    print('Dataset '+str(d+1))\n",
    "    DeepPS = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = 2, output_dim = Fout, bias = True).to(device)\n",
    "    DeepPS.load_state_dict(torch.load( './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1), weights_only = True))\n",
    "    optimizer = torch.optim.Adam(DeepPS.parameters(), lr= 1e-2)\n",
    "    n = X_train.size()[0]\n",
    "\n",
    "    ## Access to the Weight matrix for spline\n",
    "    DPS_Wstack = []\n",
    "    for l in range(nl):    \n",
    "        block = getattr(model.Spline_block.model, f'block_{l}')\n",
    "        DPS_Wstack.append(getattr(block.block.BSL, 'control_p').data)\n",
    "\n",
    "    for t in range(1, Fast_tun_epoch):\n",
    "\n",
    "        # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "        pyb_af = DeepPS(X_train)\n",
    "        loss = criterion(y_train, pyb_af)\n",
    "\n",
    "        for l in range(nl):\n",
    "            loss += (BestLambda[l]/n * torch.norm(diag_mat_weights(DPS_Wstack[l].size()[0]).to(device) @ DPS_Wstack[l]))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        PMSPE = criterion(y_test, DeepPS(X_test).detach()).item()\n",
    "        Pres[d] = PMSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a1b2e9-1a43-4cfc-b16d-cba54ef57d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351bc8d1-8b22-4038-abe4-3d53c121f7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74568953-4b8f-4ebf-b210-903e59df02ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
