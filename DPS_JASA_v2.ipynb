{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "16d1745a-f822-44f1-af6d-7cf0b4f382ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sim_data(n, dim, Type):\n",
    "\tif Type == 'A':\n",
    "\t\tX = torch.rand((n,dim))\n",
    "\t\ty = torch.exp(2*torch.sin(X[:,0]*0.5*torch.pi)+ 0.5*torch.cos(X[:,1]*2.5*torch.pi))\n",
    "\t\ty = y.reshape(-1,1)\n",
    "\t\ty = y.float()\n",
    "\t\t\n",
    "\telif Type == 'B':\n",
    "\t\tX = torch.rand((n, dim))\n",
    "\t\ty = 1\n",
    "\t\tfor d in range(dim):\n",
    "\t\t\ta = (d+1)/2\n",
    "\t\t\ty *= ((torch.abs(4*X[:,d]-2)+a)/(1+a))\n",
    "\t\ty = y.reshape(-1,1)\n",
    "\t\ty = y.float()\n",
    "\telse:\n",
    "\t\tpass\n",
    "\t\n",
    "\treturn X, y\n",
    "\t\n",
    "def norm(x):\n",
    "\treturn (x-torch.min(x))/(torch.max(x)-torch.min(x))\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def diag_mat_weights(dimp, type = 'first'):\n",
    "\tif type == 'first':\n",
    "\t\tdg = np.zeros((dimp-1, dimp))\n",
    "\t\tfor i in range(dimp-1):\n",
    "\t\t\tdg[i,i] = -1\n",
    "\t\t\tdg[i,i+1]= 1\n",
    "\telif type == 'second':\n",
    "\t\tdg = np.zeros((dimp-2, dimp))\n",
    "\t\tfor i in range(dimp-2):\n",
    "\t\t\tdg[i,i] = -1\n",
    "\t\t\tdg[i,i+1]= 2\n",
    "\t\t\tdg[i,i+2]= -1\n",
    "\telse:\n",
    "\t\tpass\n",
    "\treturn torch.Tensor(dg)\n",
    "\n",
    "def num_para(model):\n",
    "\ttp = 0\n",
    "\tfor param in model.parameters():\n",
    "\t\ttp += param.numel()\n",
    "\treturn tp\n",
    "\n",
    "\n",
    "def ECM(model, num_neurons, num_knots, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4):\n",
    "\tlambdab = initial_lambda\n",
    "\tsigma = initial_sigma\n",
    "\txi = initial_xi\n",
    "\t\n",
    "\tB = model.inter['ebasic']\n",
    "\tBy = model.inter['basic']\n",
    "\tWB = model.sp1.control_p\n",
    "\tDB = diag_mat_weights(WB.size()[0]).to(device)\n",
    "\tsize = B.size()[1]\n",
    "\tS = DB.T @ DB\n",
    "\tCov_a = (xi**2)* torch.linalg.pinv(S)\n",
    "\tCov_e = torch.eye(size*num_neurons)* sigma\n",
    "\t\n",
    "\tblock_y = torch.reshape(By, (-1,1))\n",
    "\tflatB = B.view(num_neurons, num_knots, size)\n",
    "\t\t\n",
    "\tsqr_xi= 0\n",
    "\tsqr_sig = 0\n",
    "\t\n",
    "\tfor i in range(num_neurons):\n",
    "\t\tNcov = (Cov_a -(Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)]) @ flatB[i].T @ Cov_a))\n",
    "\t\tNmu = (Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)])) @ By[:,i].reshape(-1,1)\n",
    "\t\t\n",
    "\t\tfirst_xi = S @ Ncov\n",
    "\t\tsecond_xi = (Nmu.T @ S @ Nmu)\n",
    "\t\tsqr_xi += torch.trace(first_xi) + second_xi\n",
    "\t\t\t\n",
    "\t\tfirst_sig = torch.norm(By[:,i])\n",
    "\t\tsecond_sig = 2 * (By[:,i] @ flatB[i].T) @ Nmu \n",
    "\t\tthird_sig = torch.trace((flatB[i] @ flatB[i].T) @ Ncov)\n",
    "\t\tfour_sig = (Nmu.T @ flatB[i] @ flatB[i].T @ Nmu)\n",
    "\t\t\n",
    "\t\tsqr_sig += (first_sig + second_sig + third_sig + four_sig)\n",
    "\t\n",
    "\tsqr_xi /= num_neurons\n",
    "\tsqr_sig /= (num_neurons*size)\n",
    "\t\n",
    "\tLambda = sqr_sig/sqr_xi\n",
    "\t\n",
    "\treturn Lambda.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1135df76-e1b4-45c4-b304-b5d1a8a56c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2d5b9ef9-aced-4426-b106-08585f6cce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSL(nn.Module):\n",
    "    def __init__(self, degree, num_knots, num_neurons, bias = True):\n",
    "        super(BSL, self).__init__()\n",
    "        self.degree = degree\n",
    "        self.num_knots = num_knots\n",
    "        self.num_neurons = num_neurons\n",
    "        self.control_p = nn.Parameter(torch.randn(self.num_knots, self.num_neurons))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(self.num_neurons))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.inter = {}\n",
    "    \n",
    "    def basis_function(self, x, i, k, t):\n",
    "    \n",
    "        # Base case: degree 0 spline\n",
    "        if k == 0:\n",
    "            return ((t[i] <= x) & (x < t[i + 1])).float()\n",
    "    \n",
    "        # Recursive case\n",
    "        denom1 = t[i + k] - t[i]\n",
    "        denom2 = t[i + k + 1] - t[i + 1]\n",
    "    \n",
    "        term1 = 0\n",
    "        if denom1 != 0:\n",
    "            term1 = (x - t[i]) / denom1 * self.basis_function(x, i, k - 1, t)\n",
    "    \n",
    "        term2 = 0\n",
    "        if denom2 != 0:\n",
    "            term2 = (t[i + k + 1] - x) / denom2 * self.basis_function(x, i + 1, k - 1, t)\n",
    "    \n",
    "        return term1 + term2\n",
    "\n",
    "    def knots_distribution(self, dg, nk):\n",
    "\n",
    "        knots = torch.cat([torch.linspace(-0.002, -0.001, steps=dg),            # Add repeated values at the start for clamping\n",
    "            torch.linspace(0, 1, nk-2*dg-2),  # Uniform knot spacing in the middle\n",
    "            torch.linspace(1.001, 1.002, steps=dg)           # Add repeated values at the end for clamping\n",
    "            ]).view(-1,1)\n",
    "     \n",
    "        return knots\n",
    "    \n",
    "    def basis_function(self, x, spl):\n",
    "        basis_output = spl.fit_transform(x)\n",
    "        return basis_output\n",
    "            \n",
    "    def forward(self, x):\n",
    "        batch_size, num_features = x.size()\n",
    "        device = x.device\n",
    "        \n",
    "        # Create knot vector and apply B-spline basis functions for each feature\n",
    "        basises = []\n",
    "        knots = self.knots_distribution(self.degree, self.num_knots)\n",
    "        knots = knots.to(device)\n",
    "        spl = SplineTransformer(n_knots=self.num_knots, degree=self.degree, knots = knots)\n",
    "\n",
    "        \n",
    "        for feature in range(num_features):\n",
    "            # Calculate B-spline basis functions for this feature\n",
    "            \n",
    "            basis = self.basis_function(x[:, feature].reshape(-1,1), spl)\n",
    "            basis = torch.Tensor(basis)\n",
    "            basises.append(basis)\n",
    "\n",
    "        if num_features == 1:\n",
    "            tout = basises[0] @ self.control_p\n",
    "            self.inter['basic'] = basises[0].T\n",
    "        else:\n",
    "            self.inter['basic'] = torch.reshape(torch.stack(basises, dim = 1), (batch_size, self.num_knots * self.num_neurons)).T\n",
    "            basises = torch.stack(basises)\n",
    "            tout = basises.permute(1,2,0) * self.control_p\n",
    "            tout = tout.sum(dim =1)\n",
    "                \n",
    "        if self.bias is not None:\n",
    "            tout += self.bias        \n",
    "            \n",
    "        return tout\n",
    "\n",
    "\n",
    "class NormLayer(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(NormLayer, self).__init__()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tmin_val = torch.min(x, axis = 1).values.reshape(-1,1)\n",
    "\t\tmax_val = torch.max(x, axis = 1).values.reshape(-1,1)\n",
    "\n",
    "\t\tx = (x - min_val)/(max_val - min_val)  # Rescale to [0, 1]\n",
    "\t\treturn x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5ba6d1c7-50bb-4c92-9a2f-5ab66703d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSpline_block(nn.Module):\n",
    "    def __init__(self, degree, num_knots, num_neurons, dropout = 0.0, bias = True):\n",
    "        super(BSpline_block, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(OrderedDict([\n",
    "            ('norm', NormLayer()),\n",
    "            ('BSL', BSL(degree = degree, num_knots = num_knots, num_neurons = num_neurons, bias = bias)),\n",
    "            ('drop', nn.Dropout(dropout)),\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "        \n",
    "class StackBS_block(nn.Module):\n",
    "    def __init__(self, block, degree, num_knots, num_neurons, num_blocks, dropout = 0.0, bias = True):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleDict({\n",
    "            f'block_{i}': block(degree = degree, num_knots = num_knots, num_neurons = num_neurons)\n",
    "            for i in range(num_blocks)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        for name, block in self.model.items():\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "d56f6179-804b-461f-bf1e-8423ab517582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DPS(nn.Module):\n",
    "    def __init__(self, input_dim, degree, num_knots, num_neurons, num_bsl, output_dim, bias):\n",
    "        super(DPS, self).__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_knots = num_knots\n",
    "        self.ln1 = nn.Linear(input_dim, num_neurons)\n",
    "        #self.nm1 = NormLayer() \n",
    "        #self.sp1 = BSL(degree = degree, num_knots = num_knots, num_neurons = num_neurons, bias = True)\n",
    "        self.Spline_block = StackBS_block(BSpline_block, degree = degree, num_knots = num_knots, num_neurons = num_neurons, num_blocks = num_bsl)\n",
    "        self.ln2 = nn.Linear(num_neurons, output_dim)\n",
    "        #self.inter = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.ln1(x)\n",
    "\n",
    "        # # # # # # # # # # # # # #\n",
    "        #          SPLINE         #\n",
    "        # # # # # # # # # # # # # #\n",
    "        \n",
    "        spout = self.Spline_block(x)\n",
    "\n",
    "        '''  \n",
    "        ln1out = self.nm1(ln1out)\n",
    "        device = ln1out.device\n",
    "        batch_size, _ = x.size()\n",
    "        \n",
    "        # # # # # # # # # # # # # #\n",
    "        #          SPLINE         #\n",
    "        # # # # # # # # # # # # # #\n",
    "        \n",
    "        sp1out = self.sp1(ln1out)\n",
    "        bslist = self.sp1.inter['basic']\n",
    "        \n",
    "        self.inter['ebasic'] = bslist\n",
    "        self.inter['basic'] = sp1out\n",
    "        '''\n",
    "        \n",
    "        output = self.ln2(spout)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def get_para_ecm(self, x):\n",
    "\n",
    "        '''\n",
    "        ecm_para: A dictionary that collects the parameter we need to the following ECM algorithm.\n",
    "        ecm_para.basic: Store the output of each B-Spline block; Dimension = [n_sample, n_neurons]\n",
    "        ecm_para.ebasic Store the weight matrix of each B-Spline expansion; Dimension = [n_knots * n_neurons, n_sample]\n",
    "\n",
    "        '''\n",
    "        ecm_para = {}\n",
    "        bs_block_out = {}\n",
    "        bs_weight_out = {}\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                bs_block_out[name] = output.detach()\n",
    "            return hook\n",
    "\n",
    "        handles = []\n",
    "        for name, layer in self.named_modules():\n",
    "            if 'block.drop' in name:\n",
    "                handles.append(layer.register_forward_hook(get_activation(name)))\n",
    "            elif 'block.BSL' in name:\n",
    "                bs_weight_out[name] = layer.inter['basic'].detach()\n",
    "        # Run forward pass (triggers hooks)\n",
    "        _ = self(x)\n",
    "\n",
    "        # Clean up hooks\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "            \n",
    "        ecm_para['basic'] = bs_block_out\n",
    "        ecm_para['ebasic'] = bs_weight_out\n",
    "        return ecm_para\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f999d-6e4c-4d2c-8157-6c4aa46c9908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "0ace1a4a-4b66-45cf-a02a-7fbb9b377a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepPS = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = 2, output_dim = Fout, bias = True).to(device)\n",
    "_ =  DeepPS(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c23cb-4c93-4fbb-876f-91530c18b96f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "64e443ff-3729-4a1c-85ad-c500872d5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 1000; ntest = 2500; ndim = 10; ndf = 1; nk = 15; nm = 50; Fout = 1\n",
    "Dtype = 'A'\n",
    "data = {}\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for d in range(ndf):\n",
    "    X_train, y_train = sim_data(ntrain, ndim, Dtype)\n",
    "    X_test, y_test = sim_data(ntest, ndim, Dtype)\n",
    "    epstrain = torch.normal(0, torch.var(y_train)*0.05, size=y_train.size())\n",
    "    epstest = torch.normal(0,  torch.var(y_train)*0.05, size=y_test.size())\n",
    "    \n",
    "    y_train, y_test = y_train + epstrain, y_test + epstest\n",
    "    data[str(d+1)] = {'TrainX': X_train, 'Trainy': y_train, 'TestX': X_test, 'Testy': y_test}\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328c265-731b-4044-b65b-b6b39792fdae",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "7098de70-4ba8-419a-9dbd-3165171a16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepPS = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = 2, output_dim = Fout, bias = True).to(device)\n",
    "learning_r = 1e-3\n",
    "optimizer = torch.optim.Adam(DeepPS.parameters(), lr=learning_r)\n",
    "Iteration = 10000; bloss_list = []; tor = 1e-5; lr_tor = 1e-6\n",
    "patientc = 30; patientr = 10; tpat = 0; bloss = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "8e9e058f-7548-40d4-827c-3e3a547d0715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss:  41.59721755981445  | , previous best loss:  9999  | saving best model ...\n",
      "Current loss:  40.951812744140625  | , previous best loss:  41.59721755981445  | saving best model ...\n",
      "Current loss:  40.313018798828125  | , previous best loss:  40.951812744140625  | saving best model ...\n",
      "Current loss:  39.68095779418945  | , previous best loss:  40.313018798828125  | saving best model ...\n",
      "Current loss:  39.055702209472656  | , previous best loss:  39.68095779418945  | saving best model ...\n",
      "Current loss:  38.43734359741211  | , previous best loss:  39.055702209472656  | saving best model ...\n",
      "Current loss:  37.82595443725586  | , previous best loss:  38.43734359741211  | saving best model ...\n",
      "Current loss:  37.22157287597656  | , previous best loss:  37.82595443725586  | saving best model ...\n",
      "Current loss:  36.6242790222168  | , previous best loss:  37.22157287597656  | saving best model ...\n",
      "Current loss:  36.034114837646484  | , previous best loss:  36.6242790222168  | saving best model ...\n",
      "Current loss:  35.45112991333008  | , previous best loss:  36.034114837646484  | saving best model ...\n",
      "Current loss:  34.8753776550293  | , previous best loss:  35.45112991333008  | saving best model ...\n",
      "Current loss:  34.30690383911133  | , previous best loss:  34.8753776550293  | saving best model ...\n",
      "Current loss:  33.745750427246094  | , previous best loss:  34.30690383911133  | saving best model ...\n",
      "Current loss:  33.19195556640625  | , previous best loss:  33.745750427246094  | saving best model ...\n",
      "Current loss:  32.64555740356445  | , previous best loss:  33.19195556640625  | saving best model ...\n",
      "Current loss:  32.1065788269043  | , previous best loss:  32.64555740356445  | saving best model ...\n",
      "Current loss:  31.57505226135254  | , previous best loss:  32.1065788269043  | saving best model ...\n",
      "Current loss:  31.050989151000977  | , previous best loss:  31.57505226135254  | saving best model ...\n",
      "Current loss:  30.534412384033203  | , previous best loss:  31.050989151000977  | saving best model ...\n",
      "Current loss:  30.02532196044922  | , previous best loss:  30.534412384033203  | saving best model ...\n",
      "Current loss:  29.52372932434082  | , previous best loss:  30.02532196044922  | saving best model ...\n",
      "Current loss:  29.02962875366211  | , previous best loss:  29.52372932434082  | saving best model ...\n",
      "Current loss:  28.543018341064453  | , previous best loss:  29.02962875366211  | saving best model ...\n",
      "Current loss:  28.063879013061523  | , previous best loss:  28.543018341064453  | saving best model ...\n",
      "Current loss:  27.592205047607422  | , previous best loss:  28.063879013061523  | saving best model ...\n",
      "Current loss:  27.12797737121582  | , previous best loss:  27.592205047607422  | saving best model ...\n",
      "Current loss:  26.67116928100586  | , previous best loss:  27.12797737121582  | saving best model ...\n",
      "Current loss:  26.221759796142578  | , previous best loss:  26.67116928100586  | saving best model ...\n",
      "Current loss:  25.779722213745117  | , previous best loss:  26.221759796142578  | saving best model ...\n",
      "Current loss:  25.345022201538086  | , previous best loss:  25.779722213745117  | saving best model ...\n",
      "Current loss:  24.917625427246094  | , previous best loss:  25.345022201538086  | saving best model ...\n",
      "Current loss:  24.49749755859375  | , previous best loss:  24.917625427246094  | saving best model ...\n",
      "Current loss:  24.084596633911133  | , previous best loss:  24.49749755859375  | saving best model ...\n",
      "Current loss:  23.678882598876953  | , previous best loss:  24.084596633911133  | saving best model ...\n",
      "Current loss:  23.280315399169922  | , previous best loss:  23.678882598876953  | saving best model ...\n",
      "Current loss:  22.888845443725586  | , previous best loss:  23.280315399169922  | saving best model ...\n",
      "Current loss:  22.504425048828125  | , previous best loss:  22.888845443725586  | saving best model ...\n",
      "Current loss:  22.12700653076172  | , previous best loss:  22.504425048828125  | saving best model ...\n",
      "Current loss:  21.756540298461914  | , previous best loss:  22.12700653076172  | saving best model ...\n",
      "Current loss:  21.392972946166992  | , previous best loss:  21.756540298461914  | saving best model ...\n",
      "Current loss:  21.0362491607666  | , previous best loss:  21.392972946166992  | saving best model ...\n",
      "Current loss:  20.686321258544922  | , previous best loss:  21.0362491607666  | saving best model ...\n",
      "Current loss:  20.343122482299805  | , previous best loss:  20.686321258544922  | saving best model ...\n",
      "Current loss:  20.006603240966797  | , previous best loss:  20.343122482299805  | saving best model ...\n",
      "Current loss:  19.676698684692383  | , previous best loss:  20.006603240966797  | saving best model ...\n",
      "Current loss:  19.353357315063477  | , previous best loss:  19.676698684692383  | saving best model ...\n",
      "Current loss:  19.036516189575195  | , previous best loss:  19.353357315063477  | saving best model ...\n",
      "Current loss:  18.726116180419922  | , previous best loss:  19.036516189575195  | saving best model ...\n",
      "Current loss:  18.422088623046875  | , previous best loss:  18.726116180419922  | saving best model ...\n",
      "Current loss:  18.124374389648438  | , previous best loss:  18.422088623046875  | saving best model ...\n",
      "Current loss:  17.832914352416992  | , previous best loss:  18.124374389648438  | saving best model ...\n",
      "Current loss:  17.547639846801758  | , previous best loss:  17.832914352416992  | saving best model ...\n",
      "Current loss:  17.268491744995117  | , previous best loss:  17.547639846801758  | saving best model ...\n",
      "Current loss:  16.995403289794922  | , previous best loss:  17.268491744995117  | saving best model ...\n",
      "Current loss:  16.728303909301758  | , previous best loss:  16.995403289794922  | saving best model ...\n",
      "Current loss:  16.467132568359375  | , previous best loss:  16.728303909301758  | saving best model ...\n",
      "Current loss:  16.211822509765625  | , previous best loss:  16.467132568359375  | saving best model ...\n",
      "Current loss:  15.962309837341309  | , previous best loss:  16.211822509765625  | saving best model ...\n",
      "Current loss:  15.71851921081543  | , previous best loss:  15.962309837341309  | saving best model ...\n",
      "Current loss:  15.480390548706055  | , previous best loss:  15.71851921081543  | saving best model ...\n",
      "Current loss:  15.247852325439453  | , previous best loss:  15.480390548706055  | saving best model ...\n",
      "Current loss:  15.020834922790527  | , previous best loss:  15.247852325439453  | saving best model ...\n",
      "Current loss:  14.799272537231445  | , previous best loss:  15.020834922790527  | saving best model ...\n",
      "Current loss:  14.58309268951416  | , previous best loss:  14.799272537231445  | saving best model ...\n",
      "Current loss:  14.372228622436523  | , previous best loss:  14.58309268951416  | saving best model ...\n",
      "Current loss:  14.166606903076172  | , previous best loss:  14.372228622436523  | saving best model ...\n",
      "Current loss:  13.966157913208008  | , previous best loss:  14.166606903076172  | saving best model ...\n",
      "Current loss:  13.770814895629883  | , previous best loss:  13.966157913208008  | saving best model ...\n",
      "Current loss:  13.580497741699219  | , previous best loss:  13.770814895629883  | saving best model ...\n",
      "Current loss:  13.395145416259766  | , previous best loss:  13.580497741699219  | saving best model ...\n",
      "Current loss:  13.214678764343262  | , previous best loss:  13.395145416259766  | saving best model ...\n",
      "Current loss:  13.039027214050293  | , previous best loss:  13.214678764343262  | saving best model ...\n",
      "Current loss:  12.868120193481445  | , previous best loss:  13.039027214050293  | saving best model ...\n",
      "Current loss:  12.70188045501709  | , previous best loss:  12.868120193481445  | saving best model ...\n",
      "Current loss:  12.540236473083496  | , previous best loss:  12.70188045501709  | saving best model ...\n",
      "Current loss:  12.383116722106934  | , previous best loss:  12.540236473083496  | saving best model ...\n",
      "Current loss:  12.230446815490723  | , previous best loss:  12.383116722106934  | saving best model ...\n",
      "Current loss:  12.082152366638184  | , previous best loss:  12.230446815490723  | saving best model ...\n",
      "Current loss:  11.938157081604004  | , previous best loss:  12.082152366638184  | saving best model ...\n",
      "Current loss:  11.798388481140137  | , previous best loss:  11.938157081604004  | saving best model ...\n",
      "Current loss:  11.662769317626953  | , previous best loss:  11.798388481140137  | saving best model ...\n",
      "Current loss:  11.531229019165039  | , previous best loss:  11.662769317626953  | saving best model ...\n",
      "Current loss:  11.403688430786133  | , previous best loss:  11.531229019165039  | saving best model ...\n",
      "Current loss:  11.280075073242188  | , previous best loss:  11.403688430786133  | saving best model ...\n",
      "Current loss:  11.16031265258789  | , previous best loss:  11.280075073242188  | saving best model ...\n",
      "Current loss:  11.04432201385498  | , previous best loss:  11.16031265258789  | saving best model ...\n",
      "Current loss:  10.932034492492676  | , previous best loss:  11.04432201385498  | saving best model ...\n",
      "Current loss:  10.823369026184082  | , previous best loss:  10.932034492492676  | saving best model ...\n",
      "Current loss:  10.718252182006836  | , previous best loss:  10.823369026184082  | saving best model ...\n",
      "Current loss:  10.616608619689941  | , previous best loss:  10.718252182006836  | saving best model ...\n",
      "Current loss:  10.518362045288086  | , previous best loss:  10.616608619689941  | saving best model ...\n",
      "Current loss:  10.423437118530273  | , previous best loss:  10.518362045288086  | saving best model ...\n",
      "Current loss:  10.331761360168457  | , previous best loss:  10.423437118530273  | saving best model ...\n",
      "Current loss:  10.243255615234375  | , previous best loss:  10.331761360168457  | saving best model ...\n",
      "Current loss:  10.157849311828613  | , previous best loss:  10.243255615234375  | saving best model ...\n",
      "Current loss:  10.075465202331543  | , previous best loss:  10.157849311828613  | saving best model ...\n",
      "Current loss:  9.996026992797852  | , previous best loss:  10.075465202331543  | saving best model ...\n",
      "Current loss:  9.91946792602539  | , previous best loss:  9.996026992797852  | saving best model ...\n",
      "Current loss:  9.845707893371582  | , previous best loss:  9.91946792602539  | saving best model ...\n",
      "Current loss:  9.774677276611328  | , previous best loss:  9.845707893371582  | saving best model ...\n",
      "Current loss:  9.706302642822266  | , previous best loss:  9.774677276611328  | saving best model ...\n",
      "Current loss:  9.64051342010498  | , previous best loss:  9.706302642822266  | saving best model ...\n",
      "Current loss:  9.577238082885742  | , previous best loss:  9.64051342010498  | saving best model ...\n",
      "Current loss:  9.516403198242188  | , previous best loss:  9.577238082885742  | saving best model ...\n",
      "Current loss:  9.457942008972168  | , previous best loss:  9.516403198242188  | saving best model ...\n",
      "Current loss:  9.401782035827637  | , previous best loss:  9.457942008972168  | saving best model ...\n",
      "Current loss:  9.347859382629395  | , previous best loss:  9.401782035827637  | saving best model ...\n",
      "Current loss:  9.296100616455078  | , previous best loss:  9.347859382629395  | saving best model ...\n",
      "Current loss:  9.246443748474121  | , previous best loss:  9.296100616455078  | saving best model ...\n",
      "Current loss:  9.198819160461426  | , previous best loss:  9.246443748474121  | saving best model ...\n",
      "Current loss:  9.15316390991211  | , previous best loss:  9.198819160461426  | saving best model ...\n",
      "Current loss:  9.10941219329834  | , previous best loss:  9.15316390991211  | saving best model ...\n",
      "Current loss:  9.067501068115234  | , previous best loss:  9.10941219329834  | saving best model ...\n",
      "Current loss:  9.027369499206543  | , previous best loss:  9.067501068115234  | saving best model ...\n",
      "Current loss:  8.9889554977417  | , previous best loss:  9.027369499206543  | saving best model ...\n",
      "Current loss:  8.952198028564453  | , previous best loss:  8.9889554977417  | saving best model ...\n",
      "Current loss:  8.91703987121582  | , previous best loss:  8.952198028564453  | saving best model ...\n",
      "Current loss:  8.883421897888184  | , previous best loss:  8.91703987121582  | saving best model ...\n",
      "Current loss:  8.851285934448242  | , previous best loss:  8.883421897888184  | saving best model ...\n",
      "Current loss:  8.820581436157227  | , previous best loss:  8.851285934448242  | saving best model ...\n",
      "Current loss:  8.791247367858887  | , previous best loss:  8.820581436157227  | saving best model ...\n",
      "Current loss:  8.76323413848877  | , previous best loss:  8.791247367858887  | saving best model ...\n",
      "Current loss:  8.736490249633789  | , previous best loss:  8.76323413848877  | saving best model ...\n",
      "Current loss:  8.710965156555176  | , previous best loss:  8.736490249633789  | saving best model ...\n",
      "Current loss:  8.686607360839844  | , previous best loss:  8.710965156555176  | saving best model ...\n",
      "Current loss:  8.663370132446289  | , previous best loss:  8.686607360839844  | saving best model ...\n",
      "Current loss:  8.641205787658691  | , previous best loss:  8.663370132446289  | saving best model ...\n",
      "Current loss:  8.620070457458496  | , previous best loss:  8.641205787658691  | saving best model ...\n",
      "Current loss:  8.5999174118042  | , previous best loss:  8.620070457458496  | saving best model ...\n",
      "Current loss:  8.580703735351562  | , previous best loss:  8.5999174118042  | saving best model ...\n",
      "Current loss:  8.562389373779297  | , previous best loss:  8.580703735351562  | saving best model ...\n",
      "Current loss:  8.54493236541748  | , previous best loss:  8.562389373779297  | saving best model ...\n",
      "Current loss:  8.528294563293457  | , previous best loss:  8.54493236541748  | saving best model ...\n",
      "Current loss:  8.51243782043457  | , previous best loss:  8.528294563293457  | saving best model ...\n",
      "Current loss:  8.497323989868164  | , previous best loss:  8.51243782043457  | saving best model ...\n",
      "Current loss:  8.482918739318848  | , previous best loss:  8.497323989868164  | saving best model ...\n",
      "Current loss:  8.469188690185547  | , previous best loss:  8.482918739318848  | saving best model ...\n",
      "Current loss:  8.456098556518555  | , previous best loss:  8.469188690185547  | saving best model ...\n",
      "Current loss:  8.44361686706543  | , previous best loss:  8.456098556518555  | saving best model ...\n",
      "Current loss:  8.43171501159668  | , previous best loss:  8.44361686706543  | saving best model ...\n",
      "Current loss:  8.420363426208496  | , previous best loss:  8.43171501159668  | saving best model ...\n",
      "Current loss:  8.409531593322754  | , previous best loss:  8.420363426208496  | saving best model ...\n",
      "Current loss:  8.399191856384277  | , previous best loss:  8.409531593322754  | saving best model ...\n",
      "Current loss:  8.389320373535156  | , previous best loss:  8.399191856384277  | saving best model ...\n",
      "Current loss:  8.379890441894531  | , previous best loss:  8.389320373535156  | saving best model ...\n",
      "Current loss:  8.370879173278809  | , previous best loss:  8.379890441894531  | saving best model ...\n",
      "Current loss:  8.362262725830078  | , previous best loss:  8.370879173278809  | saving best model ...\n",
      "Current loss:  8.354019165039062  | , previous best loss:  8.362262725830078  | saving best model ...\n",
      "Current loss:  8.3461275100708  | , previous best loss:  8.354019165039062  | saving best model ...\n",
      "Current loss:  8.338569641113281  | , previous best loss:  8.3461275100708  | saving best model ...\n",
      "Current loss:  8.331323623657227  | , previous best loss:  8.338569641113281  | saving best model ...\n",
      "Current loss:  8.324372291564941  | , previous best loss:  8.331323623657227  | saving best model ...\n",
      "Current loss:  8.31769847869873  | , previous best loss:  8.324372291564941  | saving best model ...\n",
      "Current loss:  8.311285972595215  | , previous best loss:  8.31769847869873  | saving best model ...\n",
      "Current loss:  8.305118560791016  | , previous best loss:  8.311285972595215  | saving best model ...\n",
      "Current loss:  8.29918098449707  | , previous best loss:  8.305118560791016  | saving best model ...\n",
      "Current loss:  8.293458938598633  | , previous best loss:  8.29918098449707  | saving best model ...\n",
      "Current loss:  8.28794002532959  | , previous best loss:  8.293458938598633  | saving best model ...\n",
      "Current loss:  8.282611846923828  | , previous best loss:  8.28794002532959  | saving best model ...\n",
      "Current loss:  8.277461051940918  | , previous best loss:  8.282611846923828  | saving best model ...\n",
      "Current loss:  8.272476196289062  | , previous best loss:  8.277461051940918  | saving best model ...\n",
      "Current loss:  8.267647743225098  | , previous best loss:  8.272476196289062  | saving best model ...\n",
      "Current loss:  8.262965202331543  | , previous best loss:  8.267647743225098  | saving best model ...\n",
      "Current loss:  8.258419036865234  | , previous best loss:  8.262965202331543  | saving best model ...\n",
      "Current loss:  8.253997802734375  | , previous best loss:  8.258419036865234  | saving best model ...\n",
      "Current loss:  8.2496976852417  | , previous best loss:  8.253997802734375  | saving best model ...\n",
      "Current loss:  8.245506286621094  | , previous best loss:  8.2496976852417  | saving best model ...\n",
      "Current loss:  8.24141788482666  | , previous best loss:  8.245506286621094  | saving best model ...\n",
      "Current loss:  8.237425804138184  | , previous best loss:  8.24141788482666  | saving best model ...\n",
      "Current loss:  8.233522415161133  | , previous best loss:  8.237425804138184  | saving best model ...\n",
      "Current loss:  8.229702949523926  | , previous best loss:  8.233522415161133  | saving best model ...\n",
      "Current loss:  8.225958824157715  | , previous best loss:  8.229702949523926  | saving best model ...\n",
      "Current loss:  8.222289085388184  | , previous best loss:  8.225958824157715  | saving best model ...\n",
      "Current loss:  8.218684196472168  | , previous best loss:  8.222289085388184  | saving best model ...\n",
      "Current loss:  8.215140342712402  | , previous best loss:  8.218684196472168  | saving best model ...\n",
      "Current loss:  8.21165657043457  | , previous best loss:  8.215140342712402  | saving best model ...\n",
      "Current loss:  8.208224296569824  | , previous best loss:  8.21165657043457  | saving best model ...\n",
      "Current loss:  8.204840660095215  | , previous best loss:  8.208224296569824  | saving best model ...\n",
      "Current loss:  8.20150375366211  | , previous best loss:  8.204840660095215  | saving best model ...\n",
      "Current loss:  8.198208808898926  | , previous best loss:  8.20150375366211  | saving best model ...\n",
      "Current loss:  8.194952964782715  | , previous best loss:  8.198208808898926  | saving best model ...\n",
      "Current loss:  8.191734313964844  | , previous best loss:  8.194952964782715  | saving best model ...\n",
      "Current loss:  8.188547134399414  | , previous best loss:  8.191734313964844  | saving best model ...\n",
      "Current loss:  8.185392379760742  | , previous best loss:  8.188547134399414  | saving best model ...\n",
      "Current loss:  8.182266235351562  | , previous best loss:  8.185392379760742  | saving best model ...\n",
      "Current loss:  8.17916488647461  | , previous best loss:  8.182266235351562  | saving best model ...\n",
      "Current loss:  8.176091194152832  | , previous best loss:  8.17916488647461  | saving best model ...\n",
      "Current loss:  8.1730375289917  | , previous best loss:  8.176091194152832  | saving best model ...\n",
      "Current loss:  8.170005798339844  | , previous best loss:  8.1730375289917  | saving best model ...\n",
      "Current loss:  8.166993141174316  | , previous best loss:  8.170005798339844  | saving best model ...\n",
      "Current loss:  8.163997650146484  | , previous best loss:  8.166993141174316  | saving best model ...\n",
      "Current loss:  8.16102123260498  | , previous best loss:  8.163997650146484  | saving best model ...\n",
      "Current loss:  8.15805721282959  | , previous best loss:  8.16102123260498  | saving best model ...\n",
      "Current loss:  8.155109405517578  | , previous best loss:  8.15805721282959  | saving best model ...\n",
      "Current loss:  8.152173042297363  | , previous best loss:  8.155109405517578  | saving best model ...\n",
      "Current loss:  8.149250030517578  | , previous best loss:  8.152173042297363  | saving best model ...\n",
      "Current loss:  8.14633846282959  | , previous best loss:  8.149250030517578  | saving best model ...\n",
      "Current loss:  8.143436431884766  | , previous best loss:  8.14633846282959  | saving best model ...\n",
      "Current loss:  8.140543937683105  | , previous best loss:  8.143436431884766  | saving best model ...\n",
      "Current loss:  8.13766098022461  | , previous best loss:  8.140543937683105  | saving best model ...\n",
      "Current loss:  8.134786605834961  | , previous best loss:  8.13766098022461  | saving best model ...\n",
      "Current loss:  8.131918907165527  | , previous best loss:  8.134786605834961  | saving best model ...\n",
      "Current loss:  8.129059791564941  | , previous best loss:  8.131918907165527  | saving best model ...\n",
      "Current loss:  8.126205444335938  | , previous best loss:  8.129059791564941  | saving best model ...\n",
      "Current loss:  8.123359680175781  | , previous best loss:  8.126205444335938  | saving best model ...\n",
      "Current loss:  8.120518684387207  | , previous best loss:  8.123359680175781  | saving best model ...\n",
      "Current loss:  8.117683410644531  | , previous best loss:  8.120518684387207  | saving best model ...\n",
      "Current loss:  8.114852905273438  | , previous best loss:  8.117683410644531  | saving best model ...\n",
      "Current loss:  8.112027168273926  | , previous best loss:  8.114852905273438  | saving best model ...\n",
      "Current loss:  8.109206199645996  | , previous best loss:  8.112027168273926  | saving best model ...\n",
      "Current loss:  8.106389999389648  | , previous best loss:  8.109206199645996  | saving best model ...\n",
      "Current loss:  8.10357666015625  | , previous best loss:  8.106389999389648  | saving best model ...\n",
      "Current loss:  8.100767135620117  | , previous best loss:  8.10357666015625  | saving best model ...\n",
      "Current loss:  8.097963333129883  | , previous best loss:  8.100767135620117  | saving best model ...\n",
      "Current loss:  8.095160484313965  | , previous best loss:  8.097963333129883  | saving best model ...\n",
      "Current loss:  8.092361450195312  | , previous best loss:  8.095160484313965  | saving best model ...\n",
      "Current loss:  8.089567184448242  | , previous best loss:  8.092361450195312  | saving best model ...\n",
      "Current loss:  8.086774826049805  | , previous best loss:  8.089567184448242  | saving best model ...\n",
      "Current loss:  8.083984375  | , previous best loss:  8.086774826049805  | saving best model ...\n",
      "Current loss:  8.081197738647461  | , previous best loss:  8.083984375  | saving best model ...\n",
      "Current loss:  8.078412055969238  | , previous best loss:  8.081197738647461  | saving best model ...\n",
      "Current loss:  8.075631141662598  | , previous best loss:  8.078412055969238  | saving best model ...\n",
      "Current loss:  8.072850227355957  | , previous best loss:  8.075631141662598  | saving best model ...\n",
      "Current loss:  8.070074081420898  | , previous best loss:  8.072850227355957  | saving best model ...\n",
      "Current loss:  8.06729793548584  | , previous best loss:  8.070074081420898  | saving best model ...\n",
      "Current loss:  8.06452465057373  | , previous best loss:  8.06729793548584  | saving best model ...\n",
      "Current loss:  8.061753273010254  | , previous best loss:  8.06452465057373  | saving best model ...\n",
      "Current loss:  8.05898380279541  | , previous best loss:  8.061753273010254  | saving best model ...\n",
      "Current loss:  8.056215286254883  | , previous best loss:  8.05898380279541  | saving best model ...\n",
      "Current loss:  8.053450584411621  | , previous best loss:  8.056215286254883  | saving best model ...\n",
      "Current loss:  8.05068588256836  | , previous best loss:  8.053450584411621  | saving best model ...\n",
      "Current loss:  8.047924041748047  | , previous best loss:  8.05068588256836  | saving best model ...\n",
      "Current loss:  8.04516315460205  | , previous best loss:  8.047924041748047  | saving best model ...\n",
      "Current loss:  8.042404174804688  | , previous best loss:  8.04516315460205  | saving best model ...\n",
      "Current loss:  8.039645195007324  | , previous best loss:  8.042404174804688  | saving best model ...\n",
      "Current loss:  8.036890029907227  | , previous best loss:  8.039645195007324  | saving best model ...\n",
      "Current loss:  8.034134864807129  | , previous best loss:  8.036890029907227  | saving best model ...\n",
      "Current loss:  8.031381607055664  | , previous best loss:  8.034134864807129  | saving best model ...\n",
      "Current loss:  8.028631210327148  | , previous best loss:  8.031381607055664  | saving best model ...\n",
      "Current loss:  8.025879859924316  | , previous best loss:  8.028631210327148  | saving best model ...\n",
      "Current loss:  8.023130416870117  | , previous best loss:  8.025879859924316  | saving best model ...\n",
      "Current loss:  8.020381927490234  | , previous best loss:  8.023130416870117  | saving best model ...\n",
      "Current loss:  8.017635345458984  | , previous best loss:  8.020381927490234  | saving best model ...\n",
      "Current loss:  8.014888763427734  | , previous best loss:  8.017635345458984  | saving best model ...\n",
      "Current loss:  8.012145042419434  | , previous best loss:  8.014888763427734  | saving best model ...\n",
      "Current loss:  8.00940227508545  | , previous best loss:  8.012145042419434  | saving best model ...\n",
      "Current loss:  8.006658554077148  | , previous best loss:  8.00940227508545  | saving best model ...\n",
      "Current loss:  8.003917694091797  | , previous best loss:  8.006658554077148  | saving best model ...\n",
      "Current loss:  8.001176834106445  | , previous best loss:  8.003917694091797  | saving best model ...\n",
      "Current loss:  7.99843692779541  | , previous best loss:  8.001176834106445  | saving best model ...\n",
      "Current loss:  7.99569845199585  | , previous best loss:  7.99843692779541  | saving best model ...\n",
      "Current loss:  7.992960453033447  | , previous best loss:  7.99569845199585  | saving best model ...\n",
      "Current loss:  7.990223407745361  | , previous best loss:  7.992960453033447  | saving best model ...\n",
      "Current loss:  7.987488269805908  | , previous best loss:  7.990223407745361  | saving best model ...\n",
      "Current loss:  7.984753131866455  | , previous best loss:  7.987488269805908  | saving best model ...\n",
      "Current loss:  7.982017993927002  | , previous best loss:  7.984753131866455  | saving best model ...\n",
      "Current loss:  7.979283809661865  | , previous best loss:  7.982017993927002  | saving best model ...\n",
      "Current loss:  7.976550579071045  | , previous best loss:  7.979283809661865  | saving best model ...\n",
      "Current loss:  7.973818302154541  | , previous best loss:  7.976550579071045  | saving best model ...\n",
      "Current loss:  7.971087455749512  | , previous best loss:  7.973818302154541  | saving best model ...\n",
      "Current loss:  7.968356132507324  | , previous best loss:  7.971087455749512  | saving best model ...\n",
      "Current loss:  7.965624809265137  | , previous best loss:  7.968356132507324  | saving best model ...\n",
      "Current loss:  7.96289587020874  | , previous best loss:  7.965624809265137  | saving best model ...\n",
      "Current loss:  7.9601664543151855  | , previous best loss:  7.96289587020874  | saving best model ...\n",
      "Current loss:  7.957437992095947  | , previous best loss:  7.9601664543151855  | saving best model ...\n",
      "Current loss:  7.954709053039551  | , previous best loss:  7.957437992095947  | saving best model ...\n",
      "Current loss:  7.951981544494629  | , previous best loss:  7.954709053039551  | saving best model ...\n",
      "Current loss:  7.949253559112549  | , previous best loss:  7.951981544494629  | saving best model ...\n",
      "Current loss:  7.946526527404785  | , previous best loss:  7.949253559112549  | saving best model ...\n",
      "Current loss:  7.943800449371338  | , previous best loss:  7.946526527404785  | saving best model ...\n",
      "Current loss:  7.941073417663574  | , previous best loss:  7.943800449371338  | saving best model ...\n",
      "Current loss:  7.938347816467285  | , previous best loss:  7.941073417663574  | saving best model ...\n",
      "Current loss:  7.935622215270996  | , previous best loss:  7.938347816467285  | saving best model ...\n",
      "Current loss:  7.932896614074707  | , previous best loss:  7.935622215270996  | saving best model ...\n",
      "Current loss:  7.930171489715576  | , previous best loss:  7.932896614074707  | saving best model ...\n",
      "Current loss:  7.927447319030762  | , previous best loss:  7.930171489715576  | saving best model ...\n",
      "Current loss:  7.924722194671631  | , previous best loss:  7.927447319030762  | saving best model ...\n",
      "Current loss:  7.921998023986816  | , previous best loss:  7.924722194671631  | saving best model ...\n",
      "Current loss:  7.919273376464844  | , previous best loss:  7.921998023986816  | saving best model ...\n",
      "Current loss:  7.9165496826171875  | , previous best loss:  7.919273376464844  | saving best model ...\n",
      "Current loss:  7.913825988769531  | , previous best loss:  7.9165496826171875  | saving best model ...\n",
      "Current loss:  7.911101818084717  | , previous best loss:  7.913825988769531  | saving best model ...\n",
      "Current loss:  7.908379077911377  | , previous best loss:  7.911101818084717  | saving best model ...\n",
      "Current loss:  7.905654430389404  | , previous best loss:  7.908379077911377  | saving best model ...\n",
      "Current loss:  7.902931213378906  | , previous best loss:  7.905654430389404  | saving best model ...\n",
      "Current loss:  7.900207042694092  | , previous best loss:  7.902931213378906  | saving best model ...\n",
      "Current loss:  7.897484302520752  | , previous best loss:  7.900207042694092  | saving best model ...\n",
      "Current loss:  7.8947601318359375  | , previous best loss:  7.897484302520752  | saving best model ...\n",
      "Current loss:  7.8920369148254395  | , previous best loss:  7.8947601318359375  | saving best model ...\n",
      "Current loss:  7.889313697814941  | , previous best loss:  7.8920369148254395  | saving best model ...\n",
      "Current loss:  7.886589527130127  | , previous best loss:  7.889313697814941  | saving best model ...\n",
      "Current loss:  7.883865833282471  | , previous best loss:  7.886589527130127  | saving best model ...\n",
      "Current loss:  7.881141662597656  | , previous best loss:  7.883865833282471  | saving best model ...\n",
      "Current loss:  7.87841796875  | , previous best loss:  7.881141662597656  | saving best model ...\n",
      "Current loss:  7.875693321228027  | , previous best loss:  7.87841796875  | saving best model ...\n",
      "Current loss:  7.872968673706055  | , previous best loss:  7.875693321228027  | saving best model ...\n",
      "Current loss:  7.870244026184082  | , previous best loss:  7.872968673706055  | saving best model ...\n",
      "Current loss:  7.867519855499268  | , previous best loss:  7.870244026184082  | saving best model ...\n",
      "Current loss:  7.8647942543029785  | , previous best loss:  7.867519855499268  | saving best model ...\n",
      "Current loss:  7.862069129943848  | , previous best loss:  7.8647942543029785  | saving best model ...\n",
      "Current loss:  7.859343528747559  | , previous best loss:  7.862069129943848  | saving best model ...\n",
      "Current loss:  7.8566179275512695  | , previous best loss:  7.859343528747559  | saving best model ...\n",
      "Current loss:  7.8538923263549805  | , previous best loss:  7.8566179275512695  | saving best model ...\n",
      "Current loss:  7.851165294647217  | , previous best loss:  7.8538923263549805  | saving best model ...\n",
      "Current loss:  7.848438262939453  | , previous best loss:  7.851165294647217  | saving best model ...\n",
      "Current loss:  7.845711708068848  | , previous best loss:  7.848438262939453  | saving best model ...\n",
      "Current loss:  7.842983722686768  | , previous best loss:  7.845711708068848  | saving best model ...\n",
      "Current loss:  7.840256214141846  | , previous best loss:  7.842983722686768  | saving best model ...\n",
      "Current loss:  7.837527751922607  | , previous best loss:  7.840256214141846  | saving best model ...\n",
      "Current loss:  7.834798336029053  | , previous best loss:  7.837527751922607  | saving best model ...\n",
      "Current loss:  7.832069396972656  | , previous best loss:  7.834798336029053  | saving best model ...\n",
      "Current loss:  7.82934045791626  | , previous best loss:  7.832069396972656  | saving best model ...\n",
      "Current loss:  7.826610088348389  | , previous best loss:  7.82934045791626  | saving best model ...\n",
      "Current loss:  7.823878288269043  | , previous best loss:  7.826610088348389  | saving best model ...\n",
      "Current loss:  7.821147918701172  | , previous best loss:  7.823878288269043  | saving best model ...\n",
      "Current loss:  7.818416118621826  | , previous best loss:  7.821147918701172  | saving best model ...\n",
      "Current loss:  7.8156843185424805  | , previous best loss:  7.818416118621826  | saving best model ...\n",
      "Current loss:  7.81295108795166  | , previous best loss:  7.8156843185424805  | saving best model ...\n",
      "Current loss:  7.81021785736084  | , previous best loss:  7.81295108795166  | saving best model ...\n",
      "Current loss:  7.807483196258545  | , previous best loss:  7.81021785736084  | saving best model ...\n",
      "Current loss:  7.804749011993408  | , previous best loss:  7.807483196258545  | saving best model ...\n",
      "Current loss:  7.802014350891113  | , previous best loss:  7.804749011993408  | saving best model ...\n",
      "Current loss:  7.799278259277344  | , previous best loss:  7.802014350891113  | saving best model ...\n",
      "Current loss:  7.796541690826416  | , previous best loss:  7.799278259277344  | saving best model ...\n",
      "Current loss:  7.79380464553833  | , previous best loss:  7.796541690826416  | saving best model ...\n",
      "Current loss:  7.7910661697387695  | , previous best loss:  7.79380464553833  | saving best model ...\n",
      "Current loss:  7.788328170776367  | , previous best loss:  7.7910661697387695  | saving best model ...\n",
      "Current loss:  7.785587787628174  | , previous best loss:  7.788328170776367  | saving best model ...\n",
      "Current loss:  7.782848834991455  | , previous best loss:  7.785587787628174  | saving best model ...\n",
      "Current loss:  7.780107498168945  | , previous best loss:  7.782848834991455  | saving best model ...\n",
      "Current loss:  7.7773661613464355  | , previous best loss:  7.780107498168945  | saving best model ...\n",
      "Current loss:  7.774623870849609  | , previous best loss:  7.7773661613464355  | saving best model ...\n",
      "Current loss:  7.771880149841309  | , previous best loss:  7.774623870849609  | saving best model ...\n",
      "Current loss:  7.769136428833008  | , previous best loss:  7.771880149841309  | saving best model ...\n",
      "Current loss:  7.766392230987549  | , previous best loss:  7.769136428833008  | saving best model ...\n",
      "Current loss:  7.763646602630615  | , previous best loss:  7.766392230987549  | saving best model ...\n",
      "Current loss:  7.760899543762207  | , previous best loss:  7.763646602630615  | saving best model ...\n",
      "Current loss:  7.758152008056641  | , previous best loss:  7.760899543762207  | saving best model ...\n",
      "Current loss:  7.755404949188232  | , previous best loss:  7.758152008056641  | saving best model ...\n",
      "Current loss:  7.752654552459717  | , previous best loss:  7.755404949188232  | saving best model ...\n",
      "Current loss:  7.749904632568359  | , previous best loss:  7.752654552459717  | saving best model ...\n",
      "Current loss:  7.747154235839844  | , previous best loss:  7.749904632568359  | saving best model ...\n",
      "Current loss:  7.7444024085998535  | , previous best loss:  7.747154235839844  | saving best model ...\n",
      "Current loss:  7.741649627685547  | , previous best loss:  7.7444024085998535  | saving best model ...\n",
      "Current loss:  7.738895416259766  | , previous best loss:  7.741649627685547  | saving best model ...\n",
      "Current loss:  7.736140727996826  | , previous best loss:  7.738895416259766  | saving best model ...\n",
      "Current loss:  7.733384609222412  | , previous best loss:  7.736140727996826  | saving best model ...\n",
      "Current loss:  7.730628490447998  | , previous best loss:  7.733384609222412  | saving best model ...\n",
      "Current loss:  7.727869987487793  | , previous best loss:  7.730628490447998  | saving best model ...\n",
      "Current loss:  7.725111961364746  | , previous best loss:  7.727869987487793  | saving best model ...\n",
      "Current loss:  7.722352504730225  | , previous best loss:  7.725111961364746  | saving best model ...\n",
      "Current loss:  7.719590187072754  | , previous best loss:  7.722352504730225  | saving best model ...\n",
      "Current loss:  7.716829299926758  | , previous best loss:  7.719590187072754  | saving best model ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[257], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Iteration):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Forward pass: Compute predicted y by passing x to the modelsp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     pyb_af \u001b[38;5;241m=\u001b[39m DeepPS(X_train)\n\u001b[1;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_train, pyb_af); bloss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m ((bloss_list[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39mbloss_list[t])\u001b[38;5;241m<\u001b[39mtor):        \n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[249], line 21\u001b[0m, in \u001b[0;36mDPS.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# # # # # # # # # # # # # #\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#          SPLINE         #\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# # # # # # # # # # # # # #\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m spout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSpline_block(x)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''  \u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03mln1out = self.nm1(ln1out)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mdevice = ln1out.device\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03mself.inter['basic'] = sp1out\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     39\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(spout)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[138], line 24\u001b[0m, in \u001b[0;36mStackBS_block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 24\u001b[0m         x \u001b[38;5;241m=\u001b[39m block(x)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[138], line 12\u001b[0m, in \u001b[0;36mBSpline_block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[132], line 63\u001b[0m, in \u001b[0;36mBSL.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m spl \u001b[38;5;241m=\u001b[39m SplineTransformer(n_knots\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_knots, degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree, knots \u001b[38;5;241m=\u001b[39m knots)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_features):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Calculate B-spline basis functions for this feature\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     basis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis_function(x[:, feature]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), spl)\n\u001b[1;32m     64\u001b[0m     basis \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(basis)\n\u001b[1;32m     65\u001b[0m     basises\u001b[38;5;241m.\u001b[39mappend(basis)\n",
      "Cell \u001b[0;32mIn[132], line 46\u001b[0m, in \u001b[0;36mBSL.basis_function\u001b[0;34m(self, x, spl)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbasis_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, spl):\n\u001b[0;32m---> 46\u001b[0m     basis_output \u001b[38;5;241m=\u001b[39m spl\u001b[38;5;241m.\u001b[39mfit_transform(x)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m basis_output\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/base.py:1382\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1377\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1378\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1379\u001b[0m )\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[1;32m   1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/base.py:438\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    429\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     validate_parameter_constraints(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_constraints,\n\u001b[0;32m--> 438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    439\u001b[0m         caller_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    440\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/base.py:247\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03mGet parameters for this estimator.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m    Parameter names mapped to their values.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    246\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_param_names():\n\u001b[1;32m    248\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/base.py:212\u001b[0m, in \u001b[0;36mBaseEstimator._get_param_names\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# introspect the constructor arguments to find the model parameters\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# to represent\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m init_signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(init)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Consider the constructor parameters excluding 'self'\u001b[39;00m\n\u001b[1;32m    214\u001b[0m parameters \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    215\u001b[0m     p\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m init_signature\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m!=\u001b[39m p\u001b[38;5;241m.\u001b[39mVAR_KEYWORD\n\u001b[1;32m    218\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/inspect.py:3345\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msignature\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3344\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Signature\u001b[38;5;241m.\u001b[39mfrom_callable(obj, follow_wrapped\u001b[38;5;241m=\u001b[39mfollow_wrapped,\n\u001b[1;32m   3346\u001b[0m                                    \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/inspect.py:3085\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   3082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_callable\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   3083\u001b[0m                   follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_callable(obj, sigcls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   3086\u001b[0m                                     follow_wrapper_chains\u001b[38;5;241m=\u001b[39mfollow_wrapped,\n\u001b[1;32m   3087\u001b[0m                                     \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/inspect.py:2549\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2547\u001b[0m     sig \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m__signature__\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m-> 2549\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2552\u001b[0m         \u001b[38;5;66;03m# since __text_signature__ is not writable on classes, __signature__\u001b[39;00m\n\u001b[1;32m   2553\u001b[0m         \u001b[38;5;66;03m# may contain text (or be a callable that returns text);\u001b[39;00m\n\u001b[1;32m   2554\u001b[0m         \u001b[38;5;66;03m# if so, convert it\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(Iteration):\n",
    "    # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "    pyb_af = DeepPS(X_train)\n",
    "    loss = criterion(y_train, pyb_af); bloss_list.append(loss.item())\n",
    "    \n",
    "    if (t > 0) and ((bloss_list[t-1]-bloss_list[t])<tor):        \n",
    "        if (tpat != 0) and (tpat % patientr) == 0:\n",
    "            learning_r *= 0.2 \n",
    "            tpat += 1\n",
    "            #print('Learning rate reduce to ', learning_r)\n",
    "            optimizer = torch.optim.Adam(DeepPS.parameters(), lr=learning_r)\n",
    "            if learning_r <= lr_tor:\n",
    "                if t < patientc + 1:\n",
    "                    conv = False\n",
    "                else:\n",
    "                    conv = True\n",
    "                print('Convergence!')\n",
    "                break\n",
    "        elif tpat < patientc:\n",
    "            tpat += 1\n",
    "            pass\n",
    "        else:\n",
    "            if t < patientc + 1:\n",
    "                conv = False\n",
    "            else:\n",
    "                conv = True\n",
    "            print('Convergence!')\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        if loss < bloss:\n",
    "            print('Current loss: ', loss.item(), ' | , previous best loss: ', bloss, ' | saving best model ...')\n",
    "            torch.save(DeepPS.state_dict(), './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1))\n",
    "            bloss = loss.item()\n",
    "            tpat = 0\n",
    "        else:\n",
    "            tpat += 1\n",
    "    \n",
    "    if tpat == patientc:\n",
    "        if t < patientc + 1:\n",
    "            conv = False\n",
    "        else:\n",
    "            conv = True\n",
    "            print('Convergence!')\n",
    "        break\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "dabec794-0371-428a-b207-045dd5623b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecm_para = DeepPS.get_para_ecm(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5e637620-3f84-4074-9b53-553a120e9e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    eval_model = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = 1, output_dim = Fout, bias = True).to(device)\n",
    "    eval_model.load_state_dict(torch.load( './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1), weights_only = True))\n",
    "    DPSy = eval_model(X_train)\n",
    "    LambdaB = ECM(model = eval_model, num_neurons = nm, num_knots = nk)\n",
    "    Lambdalist[str(d+1)] = LambdaB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d902f267-cdaa-49ff-a1ff-cf668f90adcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5362)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(DPSy, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbfeb0-6fd8-45a6-95f7-ecfd487e887b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e937f0f-b150-4d0b-b4cc-b37c1f8e2604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTORCH",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
