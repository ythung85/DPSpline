{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "16d1745a-f822-44f1-af6d-7cf0b4f382ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sim_data(n, dim, Type):\n",
    "\tif Type == 'A':\n",
    "\t\tX = torch.rand((n,dim))\n",
    "\t\ty = torch.exp(2*torch.sin(X[:,0]*0.5*torch.pi)+ 0.5*torch.cos(X[:,1]*2.5*torch.pi))\n",
    "\t\ty = y.reshape(-1,1)\n",
    "\t\ty = y.float()\n",
    "\t\t\n",
    "\telif Type == 'B':\n",
    "\t\tX = torch.rand((n, dim))\n",
    "\t\ty = 1\n",
    "\t\tfor d in range(dim):\n",
    "\t\t\ta = (d+1)/2\n",
    "\t\t\ty *= ((torch.abs(4*X[:,d]-2)+a)/(1+a))\n",
    "\t\ty = y.reshape(-1,1)\n",
    "\t\ty = y.float()\n",
    "\telse:\n",
    "\t\tpass\n",
    "\t\n",
    "\treturn X, y\n",
    "\t\n",
    "def norm(x):\n",
    "\treturn (x-torch.min(x))/(torch.max(x)-torch.min(x))\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def diag_mat_weights(dimp, type = 'first'):\n",
    "\tif type == 'first':\n",
    "\t\tdg = np.zeros((dimp-1, dimp))\n",
    "\t\tfor i in range(dimp-1):\n",
    "\t\t\tdg[i,i] = -1\n",
    "\t\t\tdg[i,i+1]= 1\n",
    "\telif type == 'second':\n",
    "\t\tdg = np.zeros((dimp-2, dimp))\n",
    "\t\tfor i in range(dimp-2):\n",
    "\t\t\tdg[i,i] = -1\n",
    "\t\t\tdg[i,i+1]= 2\n",
    "\t\t\tdg[i,i+2]= -1\n",
    "\telse:\n",
    "\t\tpass\n",
    "\treturn torch.Tensor(dg)\n",
    "\n",
    "def num_para(model):\n",
    "\ttp = 0\n",
    "\tfor param in model.parameters():\n",
    "\t\ttp += param.numel()\n",
    "\treturn tp\n",
    "\n",
    "\n",
    "def ECM(model, num_neurons, num_knots, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4):\n",
    "\tlambdab = initial_lambda\n",
    "\tsigma = initial_sigma\n",
    "\txi = initial_xi\n",
    "\t\n",
    "\tB = model.inter['ebasic']\n",
    "\tBy = model.inter['basic']\n",
    "\tWB = model.sp1.control_p\n",
    "\tDB = diag_mat_weights(WB.size()[0]).to(device)\n",
    "\tsize = B.size()[1]\n",
    "\tS = DB.T @ DB\n",
    "\tCov_a = (xi**2)* torch.linalg.pinv(S)\n",
    "\tCov_e = torch.eye(size*num_neurons)* sigma\n",
    "\t\n",
    "\tblock_y = torch.reshape(By, (-1,1))\n",
    "\tflatB = B.view(num_neurons, num_knots, size)\n",
    "\t\t\n",
    "\tsqr_xi= 0\n",
    "\tsqr_sig = 0\n",
    "\t\n",
    "\tfor i in range(num_neurons):\n",
    "\t\tNcov = (Cov_a -(Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)]) @ flatB[i].T @ Cov_a))\n",
    "\t\tNmu = (Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)])) @ By[:,i].reshape(-1,1)\n",
    "\t\t\n",
    "\t\tfirst_xi = S @ Ncov\n",
    "\t\tsecond_xi = (Nmu.T @ S @ Nmu)\n",
    "\t\tsqr_xi += torch.trace(first_xi) + second_xi\n",
    "\t\t\t\n",
    "\t\tfirst_sig = torch.norm(By[:,i])\n",
    "\t\tsecond_sig = 2 * (By[:,i] @ flatB[i].T) @ Nmu \n",
    "\t\tthird_sig = torch.trace((flatB[i] @ flatB[i].T) @ Ncov)\n",
    "\t\tfour_sig = (Nmu.T @ flatB[i] @ flatB[i].T @ Nmu)\n",
    "\t\t\n",
    "\t\tsqr_sig += (first_sig + second_sig + third_sig + four_sig)\n",
    "\t\n",
    "\tsqr_xi /= num_neurons\n",
    "\tsqr_sig /= (num_neurons*size)\n",
    "\t\n",
    "\tLambda = sqr_sig/sqr_xi\n",
    "\t\n",
    "\treturn Lambda.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2d5b9ef9-aced-4426-b106-08585f6cce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRODBSplineLayerMultiFeature(nn.Module):\n",
    "    def __init__(self, degree, num_knots, output_dim, num_neurons, bias = True):\n",
    "        super(PRODBSplineLayerMultiFeature, self).__init__()\n",
    "        self.degree = degree\n",
    "        self.num_knots = num_knots\n",
    "        self.output_dim = output_dim\n",
    "        self.num_neurons = num_neurons\n",
    "        self.control_p = nn.Parameter(torch.randn(self.num_knots, self.num_neurons))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(self.num_neurons))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.inter = {}\n",
    "    \n",
    "    def basis_function(self, x, i, k, t):\n",
    "    \n",
    "        # Base case: degree 0 spline\n",
    "        if k == 0:\n",
    "            return ((t[i] <= x) & (x < t[i + 1])).float()\n",
    "    \n",
    "        # Recursive case\n",
    "        denom1 = t[i + k] - t[i]\n",
    "        denom2 = t[i + k + 1] - t[i + 1]\n",
    "    \n",
    "        term1 = 0\n",
    "        if denom1 != 0:\n",
    "            term1 = (x - t[i]) / denom1 * self.basis_function(x, i, k - 1, t)\n",
    "    \n",
    "        term2 = 0\n",
    "        if denom2 != 0:\n",
    "            term2 = (t[i + k + 1] - x) / denom2 * self.basis_function(x, i + 1, k - 1, t)\n",
    "    \n",
    "        return term1 + term2\n",
    "\n",
    "    def knots_distribution(self, dg, nk):\n",
    "\n",
    "        knots = torch.cat([torch.linspace(-0.002, -0.001, steps=dg),            # Add repeated values at the start for clamping\n",
    "            torch.linspace(0, 1, nk-2*dg-2),  # Uniform knot spacing in the middle\n",
    "            torch.linspace(1.001, 1.002, steps=dg)           # Add repeated values at the end for clamping\n",
    "            ]).view(-1,1)\n",
    "     \n",
    "        return knots\n",
    "    \n",
    "    def basis_function(self, x, spl):\n",
    "        basis_output = spl.fit_transform(x)\n",
    "        return basis_output\n",
    "            \n",
    "    def forward(self, x):\n",
    "        batch_size, num_features = x.size()\n",
    "        device = x.device\n",
    "        \n",
    "        # Create knot vector and apply B-spline basis functions for each feature\n",
    "        basises = []\n",
    "        knots = knots_distribution(self.degree, self.num_knots)\n",
    "        knots = knots.to(device)\n",
    "        spl = SplineTransformer(n_knots=self.num_knots, degree=self.degree, knots = knots)\n",
    "\n",
    "        \n",
    "        for feature in range(num_features):\n",
    "            # Calculate B-spline basis functions for this feature\n",
    "            \n",
    "            basis = basis_function(x[:, feature].reshape(-1,1), spl)\n",
    "            basis = torch.Tensor(basis)\n",
    "            basises.append(basis)\n",
    "\n",
    "        if num_features == 1:\n",
    "            tout = basises[0] @ self.control_p\n",
    "            self.inter['basic'] = basises[0].T\n",
    "        else:\n",
    "            self.inter['basic'] = torch.reshape(torch.stack(basises, dim = 1), (batch_size, self.num_knots * self.num_neurons)).T\n",
    "            basises = torch.stack(basises)\n",
    "            tout = basises.permute(1,2,0) * self.control_p\n",
    "            tout = tout.sum(dim =1)\n",
    "                \n",
    "        if self.bias is not None:\n",
    "            tout += self.bias        \n",
    "            \n",
    "        return tout\n",
    "\n",
    "\n",
    "class NormLayer(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(NormLayer, self).__init__()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tmin_val = torch.min(x, axis = 1).values.reshape(-1,1)\n",
    "\t\tmax_val = torch.max(x, axis = 1).values.reshape(-1,1)\n",
    "\n",
    "\t\tx = (x - min_val)/(max_val - min_val)  # Rescale to [0, 1]\n",
    "\t\treturn x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d56f6179-804b-461f-bf1e-8423ab517582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPS(nn.Module):\n",
    "    def __init__(self, input_dim, degree, num_knots, num_neurons, output_dim, bias):\n",
    "        super(DPS, self).__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_knots = num_knots\n",
    "        self.ln1 = nn.Linear(input_dim, num_neurons)\n",
    "        self.nm1 = NormLayer() \n",
    "        self.sp1 = PRODBSplineLayerMultiFeature(degree = degree, num_knots = num_knots, num_neurons = num_neurons, output_dim= output_dim, bias = True)\n",
    "        self.ln2 = nn.Linear(num_neurons, output_dim)\n",
    "        self.inter = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ln1out = self.ln1(x)\n",
    "        ln1out = self.nm1(ln1out)\n",
    "        \n",
    "        device = ln1out.device\n",
    "        batch_size, _ = x.size()\n",
    "        \n",
    "        # # # # # # # # # # # # # #\n",
    "        #         SPLINE 1        #\n",
    "        # # # # # # # # # # # # # #\n",
    "        \n",
    "        sp1out = self.sp1(ln1out)\n",
    "        bslist = self.sp1.inter['basic']\n",
    "        \n",
    "        self.inter['ebasic'] = bslist\n",
    "        self.inter['basic'] = sp1out\n",
    "        \n",
    "        ln2out = self.ln2(sp1out)\n",
    "        \n",
    "        return ln2out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749893b0-c145-462a-9ea5-6e95d88744a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace1a4a-4b66-45cf-a02a-7fbb9b377a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c4c23cb-4c93-4fbb-876f-91530c18b96f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "64e443ff-3729-4a1c-85ad-c500872d5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 1000; ntest = 2500; ndim = 10; ndf = 1; nk = 15; nm = 50; Fout = 1\n",
    "Dtype = 'A'\n",
    "data = {}\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for d in range(ndf):\n",
    "    X_train, y_train = sim_data(ntrain, ndim, Dtype)\n",
    "    X_test, y_test = sim_data(ntest, ndim, Dtype)\n",
    "    epstrain = torch.normal(0, torch.var(y_train)*0.05, size=y_train.size())\n",
    "    epstest = torch.normal(0,  torch.var(y_train)*0.05, size=y_test.size())\n",
    "    \n",
    "    y_train, y_test = y_train + epstrain, y_test + epstest\n",
    "    data[str(d+1)] = {'TrainX': X_train, 'Trainy': y_train, 'TestX': X_test, 'Testy': y_test}\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328c265-731b-4044-b65b-b6b39792fdae",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "7098de70-4ba8-419a-9dbd-3165171a16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepPS = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, output_dim = Fout, bias = True).to(device)\n",
    "learning_r = 1e-3\n",
    "optimizer = torch.optim.Adam(DeepPS.parameters(), lr=learning_r)\n",
    "Iteration = 10000; bloss_list = []; tor = 1e-5; lr_tor = 1e-6\n",
    "patientc = 30; patientr = 10; tpat = 0; bloss = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8e9e058f-7548-40d4-827c-3e3a547d0715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss:  32.93703079223633  | , previous best loss:  9999  | saving best model ...\n",
      "Current loss:  32.39961242675781  | , previous best loss:  32.93703079223633  | saving best model ...\n",
      "Current loss:  31.86819839477539  | , previous best loss:  32.39961242675781  | saving best model ...\n",
      "Current loss:  31.34288787841797  | , previous best loss:  31.86819839477539  | saving best model ...\n",
      "Current loss:  30.823780059814453  | , previous best loss:  31.34288787841797  | saving best model ...\n",
      "Current loss:  30.310958862304688  | , previous best loss:  30.823780059814453  | saving best model ...\n",
      "Current loss:  29.804523468017578  | , previous best loss:  30.310958862304688  | saving best model ...\n",
      "Current loss:  29.30453872680664  | , previous best loss:  29.804523468017578  | saving best model ...\n",
      "Current loss:  28.811079025268555  | , previous best loss:  29.30453872680664  | saving best model ...\n",
      "Current loss:  28.324203491210938  | , previous best loss:  28.811079025268555  | saving best model ...\n",
      "Current loss:  27.843961715698242  | , previous best loss:  28.324203491210938  | saving best model ...\n",
      "Current loss:  27.37040901184082  | , previous best loss:  27.843961715698242  | saving best model ...\n",
      "Current loss:  26.903587341308594  | , previous best loss:  27.37040901184082  | saving best model ...\n",
      "Current loss:  26.443538665771484  | , previous best loss:  26.903587341308594  | saving best model ...\n",
      "Current loss:  25.99030113220215  | , previous best loss:  26.443538665771484  | saving best model ...\n",
      "Current loss:  25.54389762878418  | , previous best loss:  25.99030113220215  | saving best model ...\n",
      "Current loss:  25.104360580444336  | , previous best loss:  25.54389762878418  | saving best model ...\n",
      "Current loss:  24.671710968017578  | , previous best loss:  25.104360580444336  | saving best model ...\n",
      "Current loss:  24.2459659576416  | , previous best loss:  24.671710968017578  | saving best model ...\n",
      "Current loss:  23.827138900756836  | , previous best loss:  24.2459659576416  | saving best model ...\n",
      "Current loss:  23.415237426757812  | , previous best loss:  23.827138900756836  | saving best model ...\n",
      "Current loss:  23.010263442993164  | , previous best loss:  23.415237426757812  | saving best model ...\n",
      "Current loss:  22.612220764160156  | , previous best loss:  23.010263442993164  | saving best model ...\n",
      "Current loss:  22.221105575561523  | , previous best loss:  22.612220764160156  | saving best model ...\n",
      "Current loss:  21.836912155151367  | , previous best loss:  22.221105575561523  | saving best model ...\n",
      "Current loss:  21.459625244140625  | , previous best loss:  21.836912155151367  | saving best model ...\n",
      "Current loss:  21.089229583740234  | , previous best loss:  21.459625244140625  | saving best model ...\n",
      "Current loss:  20.72571563720703  | , previous best loss:  21.089229583740234  | saving best model ...\n",
      "Current loss:  20.369056701660156  | , previous best loss:  20.72571563720703  | saving best model ...\n",
      "Current loss:  20.019229888916016  | , previous best loss:  20.369056701660156  | saving best model ...\n",
      "Current loss:  19.67620849609375  | , previous best loss:  20.019229888916016  | saving best model ...\n",
      "Current loss:  19.33995819091797  | , previous best loss:  19.67620849609375  | saving best model ...\n",
      "Current loss:  19.010448455810547  | , previous best loss:  19.33995819091797  | saving best model ...\n",
      "Current loss:  18.687641143798828  | , previous best loss:  19.010448455810547  | saving best model ...\n",
      "Current loss:  18.371498107910156  | , previous best loss:  18.687641143798828  | saving best model ...\n",
      "Current loss:  18.061973571777344  | , previous best loss:  18.371498107910156  | saving best model ...\n",
      "Current loss:  17.759023666381836  | , previous best loss:  18.061973571777344  | saving best model ...\n",
      "Current loss:  17.462608337402344  | , previous best loss:  17.759023666381836  | saving best model ...\n",
      "Current loss:  17.17266845703125  | , previous best loss:  17.462608337402344  | saving best model ...\n",
      "Current loss:  16.889156341552734  | , previous best loss:  17.17266845703125  | saving best model ...\n",
      "Current loss:  16.612018585205078  | , previous best loss:  16.889156341552734  | saving best model ...\n",
      "Current loss:  16.34119987487793  | , previous best loss:  16.612018585205078  | saving best model ...\n",
      "Current loss:  16.076641082763672  | , previous best loss:  16.34119987487793  | saving best model ...\n",
      "Current loss:  15.818279266357422  | , previous best loss:  16.076641082763672  | saving best model ...\n",
      "Current loss:  15.566061019897461  | , previous best loss:  15.818279266357422  | saving best model ...\n",
      "Current loss:  15.319915771484375  | , previous best loss:  15.566061019897461  | saving best model ...\n",
      "Current loss:  15.079781532287598  | , previous best loss:  15.319915771484375  | saving best model ...\n",
      "Current loss:  14.845593452453613  | , previous best loss:  15.079781532287598  | saving best model ...\n",
      "Current loss:  14.617281913757324  | , previous best loss:  14.845593452453613  | saving best model ...\n",
      "Current loss:  14.394781112670898  | , previous best loss:  14.617281913757324  | saving best model ...\n",
      "Current loss:  14.178016662597656  | , previous best loss:  14.394781112670898  | saving best model ...\n",
      "Current loss:  13.966920852661133  | , previous best loss:  14.178016662597656  | saving best model ...\n",
      "Current loss:  13.761418342590332  | , previous best loss:  13.966920852661133  | saving best model ...\n",
      "Current loss:  13.56143569946289  | , previous best loss:  13.761418342590332  | saving best model ...\n",
      "Current loss:  13.366899490356445  | , previous best loss:  13.56143569946289  | saving best model ...\n",
      "Current loss:  13.177735328674316  | , previous best loss:  13.366899490356445  | saving best model ...\n",
      "Current loss:  12.993861198425293  | , previous best loss:  13.177735328674316  | saving best model ...\n",
      "Current loss:  12.815207481384277  | , previous best loss:  12.993861198425293  | saving best model ...\n",
      "Current loss:  12.641685485839844  | , previous best loss:  12.815207481384277  | saving best model ...\n",
      "Current loss:  12.473223686218262  | , previous best loss:  12.641685485839844  | saving best model ...\n",
      "Current loss:  12.309739112854004  | , previous best loss:  12.473223686218262  | saving best model ...\n",
      "Current loss:  12.15114974975586  | , previous best loss:  12.309739112854004  | saving best model ...\n",
      "Current loss:  11.997371673583984  | , previous best loss:  12.15114974975586  | saving best model ...\n",
      "Current loss:  11.848329544067383  | , previous best loss:  11.997371673583984  | saving best model ...\n",
      "Current loss:  11.70393180847168  | , previous best loss:  11.848329544067383  | saving best model ...\n",
      "Current loss:  11.56409740447998  | , previous best loss:  11.70393180847168  | saving best model ...\n",
      "Current loss:  11.428744316101074  | , previous best loss:  11.56409740447998  | saving best model ...\n",
      "Current loss:  11.297784805297852  | , previous best loss:  11.428744316101074  | saving best model ...\n",
      "Current loss:  11.171133041381836  | , previous best loss:  11.297784805297852  | saving best model ...\n",
      "Current loss:  11.04870319366455  | , previous best loss:  11.171133041381836  | saving best model ...\n",
      "Current loss:  10.930408477783203  | , previous best loss:  11.04870319366455  | saving best model ...\n",
      "Current loss:  10.816160202026367  | , previous best loss:  10.930408477783203  | saving best model ...\n",
      "Current loss:  10.705875396728516  | , previous best loss:  10.816160202026367  | saving best model ...\n",
      "Current loss:  10.599459648132324  | , previous best loss:  10.705875396728516  | saving best model ...\n",
      "Current loss:  10.496830940246582  | , previous best loss:  10.599459648132324  | saving best model ...\n",
      "Current loss:  10.397900581359863  | , previous best loss:  10.496830940246582  | saving best model ...\n",
      "Current loss:  10.302576065063477  | , previous best loss:  10.397900581359863  | saving best model ...\n",
      "Current loss:  10.210773468017578  | , previous best loss:  10.302576065063477  | saving best model ...\n",
      "Current loss:  10.122401237487793  | , previous best loss:  10.210773468017578  | saving best model ...\n",
      "Current loss:  10.037372589111328  | , previous best loss:  10.122401237487793  | saving best model ...\n",
      "Current loss:  9.955598831176758  | , previous best loss:  10.037372589111328  | saving best model ...\n",
      "Current loss:  9.876993179321289  | , previous best loss:  9.955598831176758  | saving best model ...\n",
      "Current loss:  9.801466941833496  | , previous best loss:  9.876993179321289  | saving best model ...\n",
      "Current loss:  9.728934288024902  | , previous best loss:  9.801466941833496  | saving best model ...\n",
      "Current loss:  9.659306526184082  | , previous best loss:  9.728934288024902  | saving best model ...\n",
      "Current loss:  9.592496871948242  | , previous best loss:  9.659306526184082  | saving best model ...\n",
      "Current loss:  9.528421401977539  | , previous best loss:  9.592496871948242  | saving best model ...\n",
      "Current loss:  9.466995239257812  | , previous best loss:  9.528421401977539  | saving best model ...\n",
      "Current loss:  9.40813159942627  | , previous best loss:  9.466995239257812  | saving best model ...\n",
      "Current loss:  9.351749420166016  | , previous best loss:  9.40813159942627  | saving best model ...\n",
      "Current loss:  9.297762870788574  | , previous best loss:  9.351749420166016  | saving best model ...\n",
      "Current loss:  9.246092796325684  | , previous best loss:  9.297762870788574  | saving best model ...\n",
      "Current loss:  9.196656227111816  | , previous best loss:  9.246092796325684  | saving best model ...\n",
      "Current loss:  9.149375915527344  | , previous best loss:  9.196656227111816  | saving best model ...\n",
      "Current loss:  9.104169845581055  | , previous best loss:  9.149375915527344  | saving best model ...\n",
      "Current loss:  9.060962677001953  | , previous best loss:  9.104169845581055  | saving best model ...\n",
      "Current loss:  9.019679069519043  | , previous best loss:  9.060962677001953  | saving best model ...\n",
      "Current loss:  8.980242729187012  | , previous best loss:  9.019679069519043  | saving best model ...\n",
      "Current loss:  8.942580223083496  | , previous best loss:  8.980242729187012  | saving best model ...\n",
      "Current loss:  8.90661907196045  | , previous best loss:  8.942580223083496  | saving best model ...\n",
      "Current loss:  8.872288703918457  | , previous best loss:  8.90661907196045  | saving best model ...\n",
      "Current loss:  8.839521408081055  | , previous best loss:  8.872288703918457  | saving best model ...\n",
      "Current loss:  8.808250427246094  | , previous best loss:  8.839521408081055  | saving best model ...\n",
      "Current loss:  8.77840518951416  | , previous best loss:  8.808250427246094  | saving best model ...\n",
      "Current loss:  8.74992561340332  | , previous best loss:  8.77840518951416  | saving best model ...\n",
      "Current loss:  8.722747802734375  | , previous best loss:  8.74992561340332  | saving best model ...\n",
      "Current loss:  8.696812629699707  | , previous best loss:  8.722747802734375  | saving best model ...\n",
      "Current loss:  8.672057151794434  | , previous best loss:  8.696812629699707  | saving best model ...\n",
      "Current loss:  8.648427963256836  | , previous best loss:  8.672057151794434  | saving best model ...\n",
      "Current loss:  8.625866889953613  | , previous best loss:  8.648427963256836  | saving best model ...\n",
      "Current loss:  8.604321479797363  | , previous best loss:  8.625866889953613  | saving best model ...\n",
      "Current loss:  8.583739280700684  | , previous best loss:  8.604321479797363  | saving best model ...\n",
      "Current loss:  8.564069747924805  | , previous best loss:  8.583739280700684  | saving best model ...\n",
      "Current loss:  8.545265197753906  | , previous best loss:  8.564069747924805  | saving best model ...\n",
      "Current loss:  8.527276992797852  | , previous best loss:  8.545265197753906  | saving best model ...\n",
      "Current loss:  8.510063171386719  | , previous best loss:  8.527276992797852  | saving best model ...\n",
      "Current loss:  8.49357795715332  | , previous best loss:  8.510063171386719  | saving best model ...\n",
      "Current loss:  8.477781295776367  | , previous best loss:  8.49357795715332  | saving best model ...\n",
      "Current loss:  8.46263313293457  | , previous best loss:  8.477781295776367  | saving best model ...\n",
      "Current loss:  8.44809341430664  | , previous best loss:  8.46263313293457  | saving best model ...\n",
      "Current loss:  8.434128761291504  | , previous best loss:  8.44809341430664  | saving best model ...\n",
      "Current loss:  8.420703887939453  | , previous best loss:  8.434128761291504  | saving best model ...\n",
      "Current loss:  8.407784461975098  | , previous best loss:  8.420703887939453  | saving best model ...\n",
      "Current loss:  8.395339965820312  | , previous best loss:  8.407784461975098  | saving best model ...\n",
      "Current loss:  8.38333797454834  | , previous best loss:  8.395339965820312  | saving best model ...\n",
      "Current loss:  8.37175178527832  | , previous best loss:  8.38333797454834  | saving best model ...\n",
      "Current loss:  8.360554695129395  | , previous best loss:  8.37175178527832  | saving best model ...\n",
      "Current loss:  8.349720001220703  | , previous best loss:  8.360554695129395  | saving best model ...\n",
      "Current loss:  8.33922290802002  | , previous best loss:  8.349720001220703  | saving best model ...\n",
      "Current loss:  8.329041481018066  | , previous best loss:  8.33922290802002  | saving best model ...\n",
      "Current loss:  8.319151878356934  | , previous best loss:  8.329041481018066  | saving best model ...\n",
      "Current loss:  8.309536933898926  | , previous best loss:  8.319151878356934  | saving best model ...\n",
      "Current loss:  8.30017375946045  | , previous best loss:  8.309536933898926  | saving best model ...\n",
      "Current loss:  8.291046142578125  | , previous best loss:  8.30017375946045  | saving best model ...\n",
      "Current loss:  8.282135963439941  | , previous best loss:  8.291046142578125  | saving best model ...\n",
      "Current loss:  8.27342700958252  | , previous best loss:  8.282135963439941  | saving best model ...\n",
      "Current loss:  8.264904022216797  | , previous best loss:  8.27342700958252  | saving best model ...\n",
      "Current loss:  8.256553649902344  | , previous best loss:  8.264904022216797  | saving best model ...\n",
      "Current loss:  8.24836254119873  | , previous best loss:  8.256553649902344  | saving best model ...\n",
      "Current loss:  8.240318298339844  | , previous best loss:  8.24836254119873  | saving best model ...\n",
      "Current loss:  8.232407569885254  | , previous best loss:  8.240318298339844  | saving best model ...\n",
      "Current loss:  8.22461986541748  | , previous best loss:  8.232407569885254  | saving best model ...\n",
      "Current loss:  8.216948509216309  | , previous best loss:  8.22461986541748  | saving best model ...\n",
      "Current loss:  8.209380149841309  | , previous best loss:  8.216948509216309  | saving best model ...\n",
      "Current loss:  8.201908111572266  | , previous best loss:  8.209380149841309  | saving best model ...\n",
      "Current loss:  8.194522857666016  | , previous best loss:  8.201908111572266  | saving best model ...\n",
      "Current loss:  8.18721866607666  | , previous best loss:  8.194522857666016  | saving best model ...\n",
      "Current loss:  8.179986953735352  | , previous best loss:  8.18721866607666  | saving best model ...\n",
      "Current loss:  8.172821998596191  | , previous best loss:  8.179986953735352  | saving best model ...\n",
      "Current loss:  8.165719032287598  | , previous best loss:  8.172821998596191  | saving best model ...\n",
      "Current loss:  8.158669471740723  | , previous best loss:  8.165719032287598  | saving best model ...\n",
      "Current loss:  8.151671409606934  | , previous best loss:  8.158669471740723  | saving best model ...\n",
      "Current loss:  8.144718170166016  | , previous best loss:  8.151671409606934  | saving best model ...\n",
      "Current loss:  8.137805938720703  | , previous best loss:  8.144718170166016  | saving best model ...\n",
      "Current loss:  8.130931854248047  | , previous best loss:  8.137805938720703  | saving best model ...\n",
      "Current loss:  8.124090194702148  | , previous best loss:  8.130931854248047  | saving best model ...\n",
      "Current loss:  8.117279052734375  | , previous best loss:  8.124090194702148  | saving best model ...\n",
      "Current loss:  8.110494613647461  | , previous best loss:  8.117279052734375  | saving best model ...\n",
      "Current loss:  8.10373592376709  | , previous best loss:  8.110494613647461  | saving best model ...\n",
      "Current loss:  8.09699821472168  | , previous best loss:  8.10373592376709  | saving best model ...\n",
      "Current loss:  8.090278625488281  | , previous best loss:  8.09699821472168  | saving best model ...\n",
      "Current loss:  8.083577156066895  | , previous best loss:  8.090278625488281  | saving best model ...\n",
      "Current loss:  8.07689094543457  | , previous best loss:  8.083577156066895  | saving best model ...\n",
      "Current loss:  8.070218086242676  | , previous best loss:  8.07689094543457  | saving best model ...\n",
      "Current loss:  8.063556671142578  | , previous best loss:  8.070218086242676  | saving best model ...\n",
      "Current loss:  8.056905746459961  | , previous best loss:  8.063556671142578  | saving best model ...\n",
      "Current loss:  8.050263404846191  | , previous best loss:  8.056905746459961  | saving best model ...\n",
      "Current loss:  8.043628692626953  | , previous best loss:  8.050263404846191  | saving best model ...\n",
      "Current loss:  8.03700065612793  | , previous best loss:  8.043628692626953  | saving best model ...\n",
      "Current loss:  8.030377388000488  | , previous best loss:  8.03700065612793  | saving best model ...\n",
      "Current loss:  8.023757934570312  | , previous best loss:  8.030377388000488  | saving best model ...\n",
      "Current loss:  8.017143249511719  | , previous best loss:  8.023757934570312  | saving best model ...\n",
      "Current loss:  8.010529518127441  | , previous best loss:  8.017143249511719  | saving best model ...\n",
      "Current loss:  8.003918647766113  | , previous best loss:  8.010529518127441  | saving best model ...\n",
      "Current loss:  7.997308254241943  | , previous best loss:  8.003918647766113  | saving best model ...\n",
      "Current loss:  7.99069881439209  | , previous best loss:  7.997308254241943  | saving best model ...\n",
      "Current loss:  7.98408842086792  | , previous best loss:  7.99069881439209  | saving best model ...\n",
      "Current loss:  7.977476596832275  | , previous best loss:  7.98408842086792  | saving best model ...\n",
      "Current loss:  7.970864295959473  | , previous best loss:  7.977476596832275  | saving best model ...\n",
      "Current loss:  7.964249610900879  | , previous best loss:  7.970864295959473  | saving best model ...\n",
      "Current loss:  7.9576334953308105  | , previous best loss:  7.964249610900879  | saving best model ...\n",
      "Current loss:  7.951013565063477  | , previous best loss:  7.9576334953308105  | saving best model ...\n",
      "Current loss:  7.94439172744751  | , previous best loss:  7.951013565063477  | saving best model ...\n",
      "Current loss:  7.937765121459961  | , previous best loss:  7.94439172744751  | saving best model ...\n",
      "Current loss:  7.931135177612305  | , previous best loss:  7.937765121459961  | saving best model ...\n",
      "Current loss:  7.924499988555908  | , previous best loss:  7.931135177612305  | saving best model ...\n",
      "Current loss:  7.917861461639404  | , previous best loss:  7.924499988555908  | saving best model ...\n",
      "Current loss:  7.911217212677002  | , previous best loss:  7.917861461639404  | saving best model ...\n",
      "Current loss:  7.904568195343018  | , previous best loss:  7.911217212677002  | saving best model ...\n",
      "Current loss:  7.897912502288818  | , previous best loss:  7.904568195343018  | saving best model ...\n",
      "Current loss:  7.891252040863037  | , previous best loss:  7.897912502288818  | saving best model ...\n",
      "Current loss:  7.884585380554199  | , previous best loss:  7.891252040863037  | saving best model ...\n",
      "Current loss:  7.877912521362305  | , previous best loss:  7.884585380554199  | saving best model ...\n",
      "Current loss:  7.871232032775879  | , previous best loss:  7.877912521362305  | saving best model ...\n",
      "Current loss:  7.864545822143555  | , previous best loss:  7.871232032775879  | saving best model ...\n",
      "Current loss:  7.857850551605225  | , previous best loss:  7.864545822143555  | saving best model ...\n",
      "Current loss:  7.851149559020996  | , previous best loss:  7.857850551605225  | saving best model ...\n",
      "Current loss:  7.844439506530762  | , previous best loss:  7.851149559020996  | saving best model ...\n",
      "Current loss:  7.837723731994629  | , previous best loss:  7.844439506530762  | saving best model ...\n",
      "Current loss:  7.830997943878174  | , previous best loss:  7.837723731994629  | saving best model ...\n",
      "Current loss:  7.8242645263671875  | , previous best loss:  7.830997943878174  | saving best model ...\n",
      "Current loss:  7.817522048950195  | , previous best loss:  7.8242645263671875  | saving best model ...\n",
      "Current loss:  7.810770511627197  | , previous best loss:  7.817522048950195  | saving best model ...\n",
      "Current loss:  7.80401086807251  | , previous best loss:  7.810770511627197  | saving best model ...\n",
      "Current loss:  7.797242164611816  | , previous best loss:  7.80401086807251  | saving best model ...\n",
      "Current loss:  7.790463924407959  | , previous best loss:  7.797242164611816  | saving best model ...\n",
      "Current loss:  7.7836761474609375  | , previous best loss:  7.790463924407959  | saving best model ...\n",
      "Current loss:  7.776878833770752  | , previous best loss:  7.7836761474609375  | saving best model ...\n",
      "Current loss:  7.770071506500244  | , previous best loss:  7.776878833770752  | saving best model ...\n",
      "Current loss:  7.763253688812256  | , previous best loss:  7.770071506500244  | saving best model ...\n",
      "Current loss:  7.756425857543945  | , previous best loss:  7.763253688812256  | saving best model ...\n",
      "Current loss:  7.7495880126953125  | , previous best loss:  7.756425857543945  | saving best model ...\n",
      "Current loss:  7.742739200592041  | , previous best loss:  7.7495880126953125  | saving best model ...\n",
      "Current loss:  7.735879898071289  | , previous best loss:  7.742739200592041  | saving best model ...\n",
      "Current loss:  7.729009628295898  | , previous best loss:  7.735879898071289  | saving best model ...\n",
      "Current loss:  7.722128868103027  | , previous best loss:  7.729009628295898  | saving best model ...\n",
      "Current loss:  7.715236186981201  | , previous best loss:  7.722128868103027  | saving best model ...\n",
      "Current loss:  7.708333492279053  | , previous best loss:  7.715236186981201  | saving best model ...\n",
      "Current loss:  7.701417922973633  | , previous best loss:  7.708333492279053  | saving best model ...\n",
      "Current loss:  7.694492816925049  | , previous best loss:  7.701417922973633  | saving best model ...\n",
      "Current loss:  7.68755578994751  | , previous best loss:  7.694492816925049  | saving best model ...\n",
      "Current loss:  7.680605888366699  | , previous best loss:  7.68755578994751  | saving best model ...\n",
      "Current loss:  7.67364501953125  | , previous best loss:  7.680605888366699  | saving best model ...\n",
      "Current loss:  7.666671276092529  | , previous best loss:  7.67364501953125  | saving best model ...\n",
      "Current loss:  7.6596856117248535  | , previous best loss:  7.666671276092529  | saving best model ...\n",
      "Current loss:  7.652689456939697  | , previous best loss:  7.6596856117248535  | saving best model ...\n",
      "Current loss:  7.645679950714111  | , previous best loss:  7.652689456939697  | saving best model ...\n",
      "Current loss:  7.638658046722412  | , previous best loss:  7.645679950714111  | saving best model ...\n",
      "Current loss:  7.631624698638916  | , previous best loss:  7.638658046722412  | saving best model ...\n",
      "Current loss:  7.62457799911499  | , previous best loss:  7.631624698638916  | saving best model ...\n",
      "Current loss:  7.617518901824951  | , previous best loss:  7.62457799911499  | saving best model ...\n",
      "Current loss:  7.610448837280273  | , previous best loss:  7.617518901824951  | saving best model ...\n",
      "Current loss:  7.60336446762085  | , previous best loss:  7.610448837280273  | saving best model ...\n",
      "Current loss:  7.596268177032471  | , previous best loss:  7.60336446762085  | saving best model ...\n",
      "Current loss:  7.589158058166504  | , previous best loss:  7.596268177032471  | saving best model ...\n",
      "Current loss:  7.58203649520874  | , previous best loss:  7.589158058166504  | saving best model ...\n",
      "Current loss:  7.574901103973389  | , previous best loss:  7.58203649520874  | saving best model ...\n",
      "Current loss:  7.567753314971924  | , previous best loss:  7.574901103973389  | saving best model ...\n",
      "Current loss:  7.5605926513671875  | , previous best loss:  7.567753314971924  | saving best model ...\n",
      "Current loss:  7.553419589996338  | , previous best loss:  7.5605926513671875  | saving best model ...\n",
      "Current loss:  7.546232223510742  | , previous best loss:  7.553419589996338  | saving best model ...\n",
      "Current loss:  7.53903341293335  | , previous best loss:  7.546232223510742  | saving best model ...\n",
      "Current loss:  7.531820297241211  | , previous best loss:  7.53903341293335  | saving best model ...\n",
      "Current loss:  7.524594783782959  | , previous best loss:  7.531820297241211  | saving best model ...\n",
      "Current loss:  7.5173563957214355  | , previous best loss:  7.524594783782959  | saving best model ...\n",
      "Current loss:  7.510104656219482  | , previous best loss:  7.5173563957214355  | saving best model ...\n",
      "Current loss:  7.502839088439941  | , previous best loss:  7.510104656219482  | saving best model ...\n",
      "Current loss:  7.495560646057129  | , previous best loss:  7.502839088439941  | saving best model ...\n",
      "Current loss:  7.488269329071045  | , previous best loss:  7.495560646057129  | saving best model ...\n",
      "Current loss:  7.480964183807373  | , previous best loss:  7.488269329071045  | saving best model ...\n",
      "Current loss:  7.473647594451904  | , previous best loss:  7.480964183807373  | saving best model ...\n",
      "Current loss:  7.466316223144531  | , previous best loss:  7.473647594451904  | saving best model ...\n",
      "Current loss:  7.458972930908203  | , previous best loss:  7.466316223144531  | saving best model ...\n",
      "Current loss:  7.451614856719971  | , previous best loss:  7.458972930908203  | saving best model ...\n",
      "Current loss:  7.444245338439941  | , previous best loss:  7.451614856719971  | saving best model ...\n",
      "Current loss:  7.436861515045166  | , previous best loss:  7.444245338439941  | saving best model ...\n",
      "Current loss:  7.429464817047119  | , previous best loss:  7.436861515045166  | saving best model ...\n",
      "Current loss:  7.422054767608643  | , previous best loss:  7.429464817047119  | saving best model ...\n",
      "Current loss:  7.414632797241211  | , previous best loss:  7.422054767608643  | saving best model ...\n",
      "Current loss:  7.407196521759033  | , previous best loss:  7.414632797241211  | saving best model ...\n",
      "Current loss:  7.399747371673584  | , previous best loss:  7.407196521759033  | saving best model ...\n",
      "Current loss:  7.392285346984863  | , previous best loss:  7.399747371673584  | saving best model ...\n",
      "Current loss:  7.384810447692871  | , previous best loss:  7.392285346984863  | saving best model ...\n",
      "Current loss:  7.377322196960449  | , previous best loss:  7.384810447692871  | saving best model ...\n",
      "Current loss:  7.3698225021362305  | , previous best loss:  7.377322196960449  | saving best model ...\n",
      "Current loss:  7.362308502197266  | , previous best loss:  7.3698225021362305  | saving best model ...\n",
      "Current loss:  7.3547821044921875  | , previous best loss:  7.362308502197266  | saving best model ...\n",
      "Current loss:  7.347242832183838  | , previous best loss:  7.3547821044921875  | saving best model ...\n",
      "Current loss:  7.339690208435059  | , previous best loss:  7.347242832183838  | saving best model ...\n",
      "Current loss:  7.332126140594482  | , previous best loss:  7.339690208435059  | saving best model ...\n",
      "Current loss:  7.324548244476318  | , previous best loss:  7.332126140594482  | saving best model ...\n",
      "Current loss:  7.316957950592041  | , previous best loss:  7.324548244476318  | saving best model ...\n",
      "Current loss:  7.309354782104492  | , previous best loss:  7.316957950592041  | saving best model ...\n",
      "Current loss:  7.301740646362305  | , previous best loss:  7.309354782104492  | saving best model ...\n",
      "Current loss:  7.294112682342529  | , previous best loss:  7.301740646362305  | saving best model ...\n",
      "Current loss:  7.286472797393799  | , previous best loss:  7.294112682342529  | saving best model ...\n",
      "Current loss:  7.278820514678955  | , previous best loss:  7.286472797393799  | saving best model ...\n",
      "Current loss:  7.271155834197998  | , previous best loss:  7.278820514678955  | saving best model ...\n",
      "Current loss:  7.263479232788086  | , previous best loss:  7.271155834197998  | saving best model ...\n",
      "Current loss:  7.2557902336120605  | , previous best loss:  7.263479232788086  | saving best model ...\n",
      "Current loss:  7.24808931350708  | , previous best loss:  7.2557902336120605  | saving best model ...\n",
      "Current loss:  7.2403764724731445  | , previous best loss:  7.24808931350708  | saving best model ...\n",
      "Current loss:  7.232651233673096  | , previous best loss:  7.2403764724731445  | saving best model ...\n",
      "Current loss:  7.224915504455566  | , previous best loss:  7.232651233673096  | saving best model ...\n",
      "Current loss:  7.217165946960449  | , previous best loss:  7.224915504455566  | saving best model ...\n",
      "Current loss:  7.209405899047852  | , previous best loss:  7.217165946960449  | saving best model ...\n",
      "Current loss:  7.201634407043457  | , previous best loss:  7.209405899047852  | saving best model ...\n",
      "Current loss:  7.193850517272949  | , previous best loss:  7.201634407043457  | saving best model ...\n",
      "Current loss:  7.186056613922119  | , previous best loss:  7.193850517272949  | saving best model ...\n",
      "Current loss:  7.178250312805176  | , previous best loss:  7.186056613922119  | saving best model ...\n",
      "Current loss:  7.170433044433594  | , previous best loss:  7.178250312805176  | saving best model ...\n",
      "Current loss:  7.162604331970215  | , previous best loss:  7.170433044433594  | saving best model ...\n",
      "Current loss:  7.1547651290893555  | , previous best loss:  7.162604331970215  | saving best model ...\n",
      "Current loss:  7.146914958953857  | , previous best loss:  7.1547651290893555  | saving best model ...\n",
      "Current loss:  7.139053821563721  | , previous best loss:  7.146914958953857  | saving best model ...\n",
      "Current loss:  7.131181716918945  | , previous best loss:  7.139053821563721  | saving best model ...\n",
      "Current loss:  7.123298168182373  | , previous best loss:  7.131181716918945  | saving best model ...\n",
      "Current loss:  7.115405559539795  | , previous best loss:  7.123298168182373  | saving best model ...\n",
      "Current loss:  7.107501983642578  | , previous best loss:  7.115405559539795  | saving best model ...\n",
      "Current loss:  7.099588871002197  | , previous best loss:  7.107501983642578  | saving best model ...\n",
      "Current loss:  7.0916643142700195  | , previous best loss:  7.099588871002197  | saving best model ...\n",
      "Current loss:  7.083730697631836  | , previous best loss:  7.0916643142700195  | saving best model ...\n",
      "Current loss:  7.0757856369018555  | , previous best loss:  7.083730697631836  | saving best model ...\n",
      "Current loss:  7.0678324699401855  | , previous best loss:  7.0757856369018555  | saving best model ...\n",
      "Current loss:  7.059868335723877  | , previous best loss:  7.0678324699401855  | saving best model ...\n",
      "Current loss:  7.0518951416015625  | , previous best loss:  7.059868335723877  | saving best model ...\n",
      "Current loss:  7.043911457061768  | , previous best loss:  7.0518951416015625  | saving best model ...\n",
      "Current loss:  7.035919666290283  | , previous best loss:  7.043911457061768  | saving best model ...\n",
      "Current loss:  7.027917385101318  | , previous best loss:  7.035919666290283  | saving best model ...\n",
      "Current loss:  7.019906520843506  | , previous best loss:  7.027917385101318  | saving best model ...\n",
      "Current loss:  7.0118865966796875  | , previous best loss:  7.019906520843506  | saving best model ...\n",
      "Current loss:  7.0038580894470215  | , previous best loss:  7.0118865966796875  | saving best model ...\n",
      "Current loss:  6.995820999145508  | , previous best loss:  7.0038580894470215  | saving best model ...\n",
      "Current loss:  6.987774848937988  | , previous best loss:  6.995820999145508  | saving best model ...\n",
      "Current loss:  6.979720115661621  | , previous best loss:  6.987774848937988  | saving best model ...\n",
      "Current loss:  6.9716572761535645  | , previous best loss:  6.979720115661621  | saving best model ...\n",
      "Current loss:  6.963586330413818  | , previous best loss:  6.9716572761535645  | saving best model ...\n",
      "Current loss:  6.955505847930908  | , previous best loss:  6.963586330413818  | saving best model ...\n",
      "Current loss:  6.947419166564941  | , previous best loss:  6.955505847930908  | saving best model ...\n",
      "Current loss:  6.939323902130127  | , previous best loss:  6.947419166564941  | saving best model ...\n",
      "Current loss:  6.931220531463623  | , previous best loss:  6.939323902130127  | saving best model ...\n",
      "Current loss:  6.923110485076904  | , previous best loss:  6.931220531463623  | saving best model ...\n",
      "Current loss:  6.9149932861328125  | , previous best loss:  6.923110485076904  | saving best model ...\n",
      "Current loss:  6.906867027282715  | , previous best loss:  6.9149932861328125  | saving best model ...\n",
      "Current loss:  6.898735046386719  | , previous best loss:  6.906867027282715  | saving best model ...\n",
      "Current loss:  6.89059591293335  | , previous best loss:  6.898735046386719  | saving best model ...\n",
      "Current loss:  6.882449150085449  | , previous best loss:  6.89059591293335  | saving best model ...\n",
      "Current loss:  6.874296188354492  | , previous best loss:  6.882449150085449  | saving best model ...\n",
      "Current loss:  6.86613655090332  | , previous best loss:  6.874296188354492  | saving best model ...\n",
      "Current loss:  6.85797119140625  | , previous best loss:  6.86613655090332  | saving best model ...\n",
      "Current loss:  6.849799633026123  | , previous best loss:  6.85797119140625  | saving best model ...\n",
      "Current loss:  6.841620445251465  | , previous best loss:  6.849799633026123  | saving best model ...\n",
      "Current loss:  6.833436489105225  | , previous best loss:  6.841620445251465  | saving best model ...\n",
      "Current loss:  6.825247287750244  | , previous best loss:  6.833436489105225  | saving best model ...\n",
      "Current loss:  6.817051887512207  | , previous best loss:  6.825247287750244  | saving best model ...\n",
      "Current loss:  6.808849811553955  | , previous best loss:  6.817051887512207  | saving best model ...\n",
      "Current loss:  6.800643444061279  | , previous best loss:  6.808849811553955  | saving best model ...\n",
      "Current loss:  6.79243278503418  | , previous best loss:  6.800643444061279  | saving best model ...\n",
      "Current loss:  6.784215450286865  | , previous best loss:  6.79243278503418  | saving best model ...\n",
      "Current loss:  6.775994300842285  | , previous best loss:  6.784215450286865  | saving best model ...\n",
      "Current loss:  6.767767429351807  | , previous best loss:  6.775994300842285  | saving best model ...\n",
      "Current loss:  6.7595367431640625  | , previous best loss:  6.767767429351807  | saving best model ...\n",
      "Current loss:  6.7513017654418945  | , previous best loss:  6.7595367431640625  | saving best model ...\n",
      "Current loss:  6.7430620193481445  | , previous best loss:  6.7513017654418945  | saving best model ...\n",
      "Current loss:  6.734818458557129  | , previous best loss:  6.7430620193481445  | saving best model ...\n",
      "Current loss:  6.726571083068848  | , previous best loss:  6.734818458557129  | saving best model ...\n",
      "Current loss:  6.718319416046143  | , previous best loss:  6.726571083068848  | saving best model ...\n",
      "Current loss:  6.710064888000488  | , previous best loss:  6.718319416046143  | saving best model ...\n",
      "Current loss:  6.70180606842041  | , previous best loss:  6.710064888000488  | saving best model ...\n",
      "Current loss:  6.693544387817383  | , previous best loss:  6.70180606842041  | saving best model ...\n",
      "Current loss:  6.685279369354248  | , previous best loss:  6.693544387817383  | saving best model ...\n",
      "Current loss:  6.6770124435424805  | , previous best loss:  6.685279369354248  | saving best model ...\n",
      "Current loss:  6.668741226196289  | , previous best loss:  6.6770124435424805  | saving best model ...\n",
      "Current loss:  6.660468101501465  | , previous best loss:  6.668741226196289  | saving best model ...\n",
      "Current loss:  6.65219259262085  | , previous best loss:  6.660468101501465  | saving best model ...\n",
      "Current loss:  6.643914222717285  | , previous best loss:  6.65219259262085  | saving best model ...\n",
      "Current loss:  6.635634422302246  | , previous best loss:  6.643914222717285  | saving best model ...\n",
      "Current loss:  6.627351760864258  | , previous best loss:  6.635634422302246  | saving best model ...\n",
      "Current loss:  6.619068145751953  | , previous best loss:  6.627351760864258  | saving best model ...\n",
      "Current loss:  6.610783100128174  | , previous best loss:  6.619068145751953  | saving best model ...\n",
      "Current loss:  6.602495193481445  | , previous best loss:  6.610783100128174  | saving best model ...\n",
      "Current loss:  6.594207286834717  | , previous best loss:  6.602495193481445  | saving best model ...\n",
      "Current loss:  6.585917949676514  | , previous best loss:  6.594207286834717  | saving best model ...\n",
      "Current loss:  6.577627658843994  | , previous best loss:  6.585917949676514  | saving best model ...\n",
      "Current loss:  6.569335460662842  | , previous best loss:  6.577627658843994  | saving best model ...\n",
      "Current loss:  6.561043739318848  | , previous best loss:  6.569335460662842  | saving best model ...\n",
      "Current loss:  6.552751064300537  | , previous best loss:  6.561043739318848  | saving best model ...\n",
      "Current loss:  6.544457912445068  | , previous best loss:  6.552751064300537  | saving best model ...\n",
      "Current loss:  6.536165237426758  | , previous best loss:  6.544457912445068  | saving best model ...\n",
      "Current loss:  6.527872085571289  | , previous best loss:  6.536165237426758  | saving best model ...\n",
      "Current loss:  6.51957893371582  | , previous best loss:  6.527872085571289  | saving best model ...\n",
      "Current loss:  6.511287212371826  | , previous best loss:  6.51957893371582  | saving best model ...\n",
      "Current loss:  6.502994060516357  | , previous best loss:  6.511287212371826  | saving best model ...\n",
      "Current loss:  6.49470329284668  | , previous best loss:  6.502994060516357  | saving best model ...\n",
      "Current loss:  6.486412525177002  | , previous best loss:  6.49470329284668  | saving best model ...\n",
      "Current loss:  6.478123188018799  | , previous best loss:  6.486412525177002  | saving best model ...\n",
      "Current loss:  6.469834804534912  | , previous best loss:  6.478123188018799  | saving best model ...\n",
      "Current loss:  6.4615478515625  | , previous best loss:  6.469834804534912  | saving best model ...\n",
      "Current loss:  6.4532623291015625  | , previous best loss:  6.4615478515625  | saving best model ...\n",
      "Current loss:  6.444978713989258  | , previous best loss:  6.4532623291015625  | saving best model ...\n",
      "Current loss:  6.4366960525512695  | , previous best loss:  6.444978713989258  | saving best model ...\n",
      "Current loss:  6.428416728973389  | , previous best loss:  6.4366960525512695  | saving best model ...\n",
      "Current loss:  6.420138835906982  | , previous best loss:  6.428416728973389  | saving best model ...\n",
      "Current loss:  6.411862850189209  | , previous best loss:  6.420138835906982  | saving best model ...\n",
      "Current loss:  6.403589725494385  | , previous best loss:  6.411862850189209  | saving best model ...\n",
      "Current loss:  6.395318508148193  | , previous best loss:  6.403589725494385  | saving best model ...\n",
      "Current loss:  6.387051105499268  | , previous best loss:  6.395318508148193  | saving best model ...\n",
      "Current loss:  6.378786563873291  | , previous best loss:  6.387051105499268  | saving best model ...\n",
      "Current loss:  6.3705244064331055  | , previous best loss:  6.378786563873291  | saving best model ...\n",
      "Current loss:  6.362266540527344  | , previous best loss:  6.3705244064331055  | saving best model ...\n",
      "Current loss:  6.354011535644531  | , previous best loss:  6.362266540527344  | saving best model ...\n",
      "Current loss:  6.345761299133301  | , previous best loss:  6.354011535644531  | saving best model ...\n",
      "Current loss:  6.337513446807861  | , previous best loss:  6.345761299133301  | saving best model ...\n",
      "Current loss:  6.329270362854004  | , previous best loss:  6.337513446807861  | saving best model ...\n",
      "Current loss:  6.321030616760254  | , previous best loss:  6.329270362854004  | saving best model ...\n",
      "Current loss:  6.312795639038086  | , previous best loss:  6.321030616760254  | saving best model ...\n",
      "Current loss:  6.3045654296875  | , previous best loss:  6.312795639038086  | saving best model ...\n",
      "Current loss:  6.29633903503418  | , previous best loss:  6.3045654296875  | saving best model ...\n",
      "Current loss:  6.2881178855896  | , previous best loss:  6.29633903503418  | saving best model ...\n",
      "Current loss:  6.27990198135376  | , previous best loss:  6.2881178855896  | saving best model ...\n",
      "Current loss:  6.271690368652344  | , previous best loss:  6.27990198135376  | saving best model ...\n",
      "Current loss:  6.263484477996826  | , previous best loss:  6.271690368652344  | saving best model ...\n",
      "Current loss:  6.255283355712891  | , previous best loss:  6.263484477996826  | saving best model ...\n",
      "Current loss:  6.247088432312012  | , previous best loss:  6.255283355712891  | saving best model ...\n",
      "Current loss:  6.238898754119873  | , previous best loss:  6.247088432312012  | saving best model ...\n",
      "Current loss:  6.230715274810791  | , previous best loss:  6.238898754119873  | saving best model ...\n",
      "Current loss:  6.222537994384766  | , previous best loss:  6.230715274810791  | saving best model ...\n",
      "Current loss:  6.214366912841797  | , previous best loss:  6.222537994384766  | saving best model ...\n",
      "Current loss:  6.206201076507568  | , previous best loss:  6.214366912841797  | saving best model ...\n",
      "Current loss:  6.198041915893555  | , previous best loss:  6.206201076507568  | saving best model ...\n",
      "Current loss:  6.189889907836914  | , previous best loss:  6.198041915893555  | saving best model ...\n",
      "Current loss:  6.181745529174805  | , previous best loss:  6.189889907836914  | saving best model ...\n",
      "Current loss:  6.173607349395752  | , previous best loss:  6.181745529174805  | saving best model ...\n",
      "Current loss:  6.165475368499756  | , previous best loss:  6.173607349395752  | saving best model ...\n",
      "Current loss:  6.157351493835449  | , previous best loss:  6.165475368499756  | saving best model ...\n",
      "Current loss:  6.149234294891357  | , previous best loss:  6.157351493835449  | saving best model ...\n",
      "Current loss:  6.141125679016113  | , previous best loss:  6.149234294891357  | saving best model ...\n",
      "Current loss:  6.133024215698242  | , previous best loss:  6.141125679016113  | saving best model ...\n",
      "Current loss:  6.124930381774902  | , previous best loss:  6.133024215698242  | saving best model ...\n",
      "Current loss:  6.116844654083252  | , previous best loss:  6.124930381774902  | saving best model ...\n",
      "Current loss:  6.108767509460449  | , previous best loss:  6.116844654083252  | saving best model ...\n",
      "Current loss:  6.1006975173950195  | , previous best loss:  6.108767509460449  | saving best model ...\n",
      "Current loss:  6.092636585235596  | , previous best loss:  6.1006975173950195  | saving best model ...\n",
      "Current loss:  6.0845842361450195  | , previous best loss:  6.092636585235596  | saving best model ...\n",
      "Current loss:  6.076540470123291  | , previous best loss:  6.0845842361450195  | saving best model ...\n",
      "Current loss:  6.06850528717041  | , previous best loss:  6.076540470123291  | saving best model ...\n",
      "Current loss:  6.060479640960693  | , previous best loss:  6.06850528717041  | saving best model ...\n",
      "Current loss:  6.052463054656982  | , previous best loss:  6.060479640960693  | saving best model ...\n",
      "Current loss:  6.044455051422119  | , previous best loss:  6.052463054656982  | saving best model ...\n",
      "Current loss:  6.036457061767578  | , previous best loss:  6.044455051422119  | saving best model ...\n",
      "Current loss:  6.028468608856201  | , previous best loss:  6.036457061767578  | saving best model ...\n",
      "Current loss:  6.02048921585083  | , previous best loss:  6.028468608856201  | saving best model ...\n",
      "Current loss:  6.0125203132629395  | , previous best loss:  6.02048921585083  | saving best model ...\n",
      "Current loss:  6.004560947418213  | , previous best loss:  6.0125203132629395  | saving best model ...\n",
      "Current loss:  5.996611595153809  | , previous best loss:  6.004560947418213  | saving best model ...\n",
      "Current loss:  5.988672733306885  | , previous best loss:  5.996611595153809  | saving best model ...\n",
      "Current loss:  5.980744361877441  | , previous best loss:  5.988672733306885  | saving best model ...\n",
      "Current loss:  5.972825527191162  | , previous best loss:  5.980744361877441  | saving best model ...\n",
      "Current loss:  5.96491813659668  | , previous best loss:  5.972825527191162  | saving best model ...\n",
      "Current loss:  5.957021713256836  | , previous best loss:  5.96491813659668  | saving best model ...\n",
      "Current loss:  5.949134826660156  | , previous best loss:  5.957021713256836  | saving best model ...\n",
      "Current loss:  5.941259860992432  | , previous best loss:  5.949134826660156  | saving best model ...\n",
      "Current loss:  5.933395862579346  | , previous best loss:  5.941259860992432  | saving best model ...\n",
      "Current loss:  5.925543308258057  | , previous best loss:  5.933395862579346  | saving best model ...\n",
      "Current loss:  5.9177021980285645  | , previous best loss:  5.925543308258057  | saving best model ...\n",
      "Current loss:  5.909872055053711  | , previous best loss:  5.9177021980285645  | saving best model ...\n",
      "Current loss:  5.902053356170654  | , previous best loss:  5.909872055053711  | saving best model ...\n",
      "Current loss:  5.894247531890869  | , previous best loss:  5.902053356170654  | saving best model ...\n",
      "Current loss:  5.8864521980285645  | , previous best loss:  5.894247531890869  | saving best model ...\n",
      "Current loss:  5.878669738769531  | , previous best loss:  5.8864521980285645  | saving best model ...\n",
      "Current loss:  5.870898723602295  | , previous best loss:  5.878669738769531  | saving best model ...\n",
      "Current loss:  5.863140106201172  | , previous best loss:  5.870898723602295  | saving best model ...\n",
      "Current loss:  5.855393886566162  | , previous best loss:  5.863140106201172  | saving best model ...\n",
      "Current loss:  5.847660064697266  | , previous best loss:  5.855393886566162  | saving best model ...\n",
      "Current loss:  5.839939594268799  | , previous best loss:  5.847660064697266  | saving best model ...\n",
      "Current loss:  5.832230567932129  | , previous best loss:  5.839939594268799  | saving best model ...\n",
      "Current loss:  5.824534893035889  | , previous best loss:  5.832230567932129  | saving best model ...\n",
      "Current loss:  5.816851615905762  | , previous best loss:  5.824534893035889  | saving best model ...\n",
      "Current loss:  5.809182643890381  | , previous best loss:  5.816851615905762  | saving best model ...\n",
      "Current loss:  5.801525592803955  | , previous best loss:  5.809182643890381  | saving best model ...\n",
      "Current loss:  5.793881416320801  | , previous best loss:  5.801525592803955  | saving best model ...\n",
      "Current loss:  5.786251544952393  | , previous best loss:  5.793881416320801  | saving best model ...\n",
      "Current loss:  5.778635025024414  | , previous best loss:  5.786251544952393  | saving best model ...\n",
      "Current loss:  5.771032333374023  | , previous best loss:  5.778635025024414  | saving best model ...\n",
      "Current loss:  5.763442039489746  | , previous best loss:  5.771032333374023  | saving best model ...\n",
      "Current loss:  5.755866050720215  | , previous best loss:  5.763442039489746  | saving best model ...\n",
      "Current loss:  5.74830436706543  | , previous best loss:  5.755866050720215  | saving best model ...\n",
      "Current loss:  5.740756034851074  | , previous best loss:  5.74830436706543  | saving best model ...\n",
      "Current loss:  5.733222007751465  | , previous best loss:  5.740756034851074  | saving best model ...\n",
      "Current loss:  5.725701808929443  | , previous best loss:  5.733222007751465  | saving best model ...\n",
      "Current loss:  5.718196392059326  | , previous best loss:  5.725701808929443  | saving best model ...\n",
      "Current loss:  5.710705757141113  | , previous best loss:  5.718196392059326  | saving best model ...\n",
      "Current loss:  5.703227996826172  | , previous best loss:  5.710705757141113  | saving best model ...\n",
      "Current loss:  5.695765495300293  | , previous best loss:  5.703227996826172  | saving best model ...\n",
      "Current loss:  5.688318252563477  | , previous best loss:  5.695765495300293  | saving best model ...\n",
      "Current loss:  5.680885314941406  | , previous best loss:  5.688318252563477  | saving best model ...\n",
      "Current loss:  5.67346715927124  | , previous best loss:  5.680885314941406  | saving best model ...\n",
      "Current loss:  5.6660637855529785  | , previous best loss:  5.67346715927124  | saving best model ...\n",
      "Current loss:  5.658675193786621  | , previous best loss:  5.6660637855529785  | saving best model ...\n",
      "Current loss:  5.651302337646484  | , previous best loss:  5.658675193786621  | saving best model ...\n",
      "Current loss:  5.643944263458252  | , previous best loss:  5.651302337646484  | saving best model ...\n",
      "Current loss:  5.636601448059082  | , previous best loss:  5.643944263458252  | saving best model ...\n",
      "Current loss:  5.629274368286133  | , previous best loss:  5.636601448059082  | saving best model ...\n",
      "Current loss:  5.62196159362793  | , previous best loss:  5.629274368286133  | saving best model ...\n",
      "Current loss:  5.6146650314331055  | , previous best loss:  5.62196159362793  | saving best model ...\n",
      "Current loss:  5.607383728027344  | , previous best loss:  5.6146650314331055  | saving best model ...\n",
      "Current loss:  5.600118637084961  | , previous best loss:  5.607383728027344  | saving best model ...\n",
      "Current loss:  5.592869281768799  | , previous best loss:  5.600118637084961  | saving best model ...\n",
      "Current loss:  5.585635662078857  | , previous best loss:  5.592869281768799  | saving best model ...\n",
      "Current loss:  5.578417778015137  | , previous best loss:  5.585635662078857  | saving best model ...\n",
      "Current loss:  5.5712151527404785  | , previous best loss:  5.578417778015137  | saving best model ...\n",
      "Current loss:  5.564029216766357  | , previous best loss:  5.5712151527404785  | saving best model ...\n",
      "Current loss:  5.556859493255615  | , previous best loss:  5.564029216766357  | saving best model ...\n",
      "Current loss:  5.549705505371094  | , previous best loss:  5.556859493255615  | saving best model ...\n",
      "Current loss:  5.542568206787109  | , previous best loss:  5.549705505371094  | saving best model ...\n",
      "Current loss:  5.5354461669921875  | , previous best loss:  5.542568206787109  | saving best model ...\n",
      "Current loss:  5.528342247009277  | , previous best loss:  5.5354461669921875  | saving best model ...\n",
      "Current loss:  5.5212531089782715  | , previous best loss:  5.528342247009277  | saving best model ...\n",
      "Current loss:  5.514181613922119  | , previous best loss:  5.5212531089782715  | saving best model ...\n",
      "Current loss:  5.5071258544921875  | , previous best loss:  5.514181613922119  | saving best model ...\n",
      "Current loss:  5.500087738037109  | , previous best loss:  5.5071258544921875  | saving best model ...\n",
      "Current loss:  5.493065357208252  | , previous best loss:  5.500087738037109  | saving best model ...\n",
      "Current loss:  5.486060619354248  | , previous best loss:  5.493065357208252  | saving best model ...\n",
      "Current loss:  5.479072093963623  | , previous best loss:  5.486060619354248  | saving best model ...\n",
      "Current loss:  5.472100257873535  | , previous best loss:  5.479072093963623  | saving best model ...\n",
      "Current loss:  5.465145111083984  | , previous best loss:  5.472100257873535  | saving best model ...\n",
      "Current loss:  5.458208084106445  | , previous best loss:  5.465145111083984  | saving best model ...\n",
      "Current loss:  5.451287746429443  | , previous best loss:  5.458208084106445  | saving best model ...\n",
      "Current loss:  5.444383144378662  | , previous best loss:  5.451287746429443  | saving best model ...\n",
      "Current loss:  5.437497138977051  | , previous best loss:  5.444383144378662  | saving best model ...\n",
      "Current loss:  5.430628299713135  | , previous best loss:  5.437497138977051  | saving best model ...\n",
      "Current loss:  5.423775672912598  | , previous best loss:  5.430628299713135  | saving best model ...\n",
      "Current loss:  5.4169416427612305  | , previous best loss:  5.423775672912598  | saving best model ...\n",
      "Current loss:  5.410123825073242  | , previous best loss:  5.4169416427612305  | saving best model ...\n",
      "Current loss:  5.403323173522949  | , previous best loss:  5.410123825073242  | saving best model ...\n",
      "Current loss:  5.396541118621826  | , previous best loss:  5.403323173522949  | saving best model ...\n",
      "Current loss:  5.38977575302124  | , previous best loss:  5.396541118621826  | saving best model ...\n",
      "Current loss:  5.383028030395508  | , previous best loss:  5.38977575302124  | saving best model ...\n",
      "Current loss:  5.376297950744629  | , previous best loss:  5.383028030395508  | saving best model ...\n",
      "Current loss:  5.369585037231445  | , previous best loss:  5.376297950744629  | saving best model ...\n",
      "Current loss:  5.362890243530273  | , previous best loss:  5.369585037231445  | saving best model ...\n",
      "Current loss:  5.356213092803955  | , previous best loss:  5.362890243530273  | saving best model ...\n",
      "Current loss:  5.34955358505249  | , previous best loss:  5.356213092803955  | saving best model ...\n",
      "Current loss:  5.342911243438721  | , previous best loss:  5.34955358505249  | saving best model ...\n",
      "Current loss:  5.336287498474121  | , previous best loss:  5.342911243438721  | saving best model ...\n",
      "Current loss:  5.329680442810059  | , previous best loss:  5.336287498474121  | saving best model ...\n",
      "Current loss:  5.323092937469482  | , previous best loss:  5.329680442810059  | saving best model ...\n",
      "Current loss:  5.316522121429443  | , previous best loss:  5.323092937469482  | saving best model ...\n",
      "Current loss:  5.309968948364258  | , previous best loss:  5.316522121429443  | saving best model ...\n",
      "Current loss:  5.303433895111084  | , previous best loss:  5.309968948364258  | saving best model ...\n",
      "Current loss:  5.296916961669922  | , previous best loss:  5.303433895111084  | saving best model ...\n",
      "Current loss:  5.2904181480407715  | , previous best loss:  5.296916961669922  | saving best model ...\n",
      "Current loss:  5.283936977386475  | , previous best loss:  5.2904181480407715  | saving best model ...\n",
      "Current loss:  5.277473449707031  | , previous best loss:  5.283936977386475  | saving best model ...\n",
      "Current loss:  5.271028995513916  | , previous best loss:  5.277473449707031  | saving best model ...\n",
      "Current loss:  5.2646026611328125  | , previous best loss:  5.271028995513916  | saving best model ...\n",
      "Current loss:  5.258193016052246  | , previous best loss:  5.2646026611328125  | saving best model ...\n",
      "Current loss:  5.251802444458008  | , previous best loss:  5.258193016052246  | saving best model ...\n",
      "Current loss:  5.245429515838623  | , previous best loss:  5.251802444458008  | saving best model ...\n",
      "Current loss:  5.239075183868408  | , previous best loss:  5.245429515838623  | saving best model ...\n",
      "Current loss:  5.232738494873047  | , previous best loss:  5.239075183868408  | saving best model ...\n",
      "Current loss:  5.2264204025268555  | , previous best loss:  5.232738494873047  | saving best model ...\n",
      "Current loss:  5.220119476318359  | , previous best loss:  5.2264204025268555  | saving best model ...\n",
      "Current loss:  5.21383810043335  | , previous best loss:  5.220119476318359  | saving best model ...\n",
      "Current loss:  5.207574367523193  | , previous best loss:  5.21383810043335  | saving best model ...\n",
      "Current loss:  5.201328277587891  | , previous best loss:  5.207574367523193  | saving best model ...\n",
      "Current loss:  5.195101261138916  | , previous best loss:  5.201328277587891  | saving best model ...\n",
      "Current loss:  5.188891887664795  | , previous best loss:  5.195101261138916  | saving best model ...\n",
      "Current loss:  5.1827006340026855  | , previous best loss:  5.188891887664795  | saving best model ...\n",
      "Current loss:  5.176528453826904  | , previous best loss:  5.1827006340026855  | saving best model ...\n",
      "Current loss:  5.17037296295166  | , previous best loss:  5.176528453826904  | saving best model ...\n",
      "Current loss:  5.164237022399902  | , previous best loss:  5.17037296295166  | saving best model ...\n",
      "Current loss:  5.15811824798584  | , previous best loss:  5.164237022399902  | saving best model ...\n",
      "Current loss:  5.152019023895264  | , previous best loss:  5.15811824798584  | saving best model ...\n",
      "Current loss:  5.145936965942383  | , previous best loss:  5.152019023895264  | saving best model ...\n",
      "Current loss:  5.13987398147583  | , previous best loss:  5.145936965942383  | saving best model ...\n",
      "Current loss:  5.133828163146973  | , previous best loss:  5.13987398147583  | saving best model ...\n",
      "Current loss:  5.127801895141602  | , previous best loss:  5.133828163146973  | saving best model ...\n",
      "Current loss:  5.121792793273926  | , previous best loss:  5.127801895141602  | saving best model ...\n",
      "Current loss:  5.115801811218262  | , previous best loss:  5.121792793273926  | saving best model ...\n",
      "Current loss:  5.109829425811768  | , previous best loss:  5.115801811218262  | saving best model ...\n",
      "Current loss:  5.103875160217285  | , previous best loss:  5.109829425811768  | saving best model ...\n",
      "Current loss:  5.0979390144348145  | , previous best loss:  5.103875160217285  | saving best model ...\n",
      "Current loss:  5.092021465301514  | , previous best loss:  5.0979390144348145  | saving best model ...\n",
      "Current loss:  5.086121082305908  | , previous best loss:  5.092021465301514  | saving best model ...\n",
      "Current loss:  5.080240249633789  | , previous best loss:  5.086121082305908  | saving best model ...\n",
      "Current loss:  5.074377059936523  | , previous best loss:  5.080240249633789  | saving best model ...\n",
      "Current loss:  5.068530559539795  | , previous best loss:  5.074377059936523  | saving best model ...\n",
      "Current loss:  5.0627031326293945  | , previous best loss:  5.068530559539795  | saving best model ...\n",
      "Current loss:  5.056893825531006  | , previous best loss:  5.0627031326293945  | saving best model ...\n",
      "Current loss:  5.051103591918945  | , previous best loss:  5.056893825531006  | saving best model ...\n",
      "Current loss:  5.045330047607422  | , previous best loss:  5.051103591918945  | saving best model ...\n",
      "Current loss:  5.039575099945068  | , previous best loss:  5.045330047607422  | saving best model ...\n",
      "Current loss:  5.033838272094727  | , previous best loss:  5.039575099945068  | saving best model ...\n",
      "Current loss:  5.028119087219238  | , previous best loss:  5.033838272094727  | saving best model ...\n",
      "Current loss:  5.022418022155762  | , previous best loss:  5.028119087219238  | saving best model ...\n",
      "Current loss:  5.016735076904297  | , previous best loss:  5.022418022155762  | saving best model ...\n",
      "Current loss:  5.0110697746276855  | , previous best loss:  5.016735076904297  | saving best model ...\n",
      "Current loss:  5.005423069000244  | , previous best loss:  5.0110697746276855  | saving best model ...\n",
      "Current loss:  4.999793529510498  | , previous best loss:  5.005423069000244  | saving best model ...\n",
      "Current loss:  4.9941816329956055  | , previous best loss:  4.999793529510498  | saving best model ...\n",
      "Current loss:  4.988588333129883  | , previous best loss:  4.9941816329956055  | saving best model ...\n",
      "Current loss:  4.9830121994018555  | , previous best loss:  4.988588333129883  | saving best model ...\n",
      "Current loss:  4.97745418548584  | , previous best loss:  4.9830121994018555  | saving best model ...\n",
      "Current loss:  4.971914291381836  | , previous best loss:  4.97745418548584  | saving best model ...\n",
      "Current loss:  4.966391086578369  | , previous best loss:  4.971914291381836  | saving best model ...\n",
      "Current loss:  4.9608869552612305  | , previous best loss:  4.966391086578369  | saving best model ...\n",
      "Current loss:  4.955399513244629  | , previous best loss:  4.9608869552612305  | saving best model ...\n",
      "Current loss:  4.9499287605285645  | , previous best loss:  4.955399513244629  | saving best model ...\n",
      "Current loss:  4.94447660446167  | , previous best loss:  4.9499287605285645  | saving best model ...\n",
      "Current loss:  4.939042091369629  | , previous best loss:  4.94447660446167  | saving best model ...\n",
      "Current loss:  4.9336256980896  | , previous best loss:  4.939042091369629  | saving best model ...\n",
      "Current loss:  4.928225994110107  | , previous best loss:  4.9336256980896  | saving best model ...\n",
      "Current loss:  4.922843933105469  | , previous best loss:  4.928225994110107  | saving best model ...\n",
      "Current loss:  4.917479038238525  | , previous best loss:  4.922843933105469  | saving best model ...\n",
      "Current loss:  4.9121317863464355  | , previous best loss:  4.917479038238525  | saving best model ...\n",
      "Current loss:  4.906801700592041  | , previous best loss:  4.9121317863464355  | saving best model ...\n",
      "Current loss:  4.901488780975342  | , previous best loss:  4.906801700592041  | saving best model ...\n",
      "Current loss:  4.896193504333496  | , previous best loss:  4.901488780975342  | saving best model ...\n",
      "Current loss:  4.8909149169921875  | , previous best loss:  4.896193504333496  | saving best model ...\n",
      "Current loss:  4.885654449462891  | , previous best loss:  4.8909149169921875  | saving best model ...\n",
      "Current loss:  4.8804097175598145  | , previous best loss:  4.885654449462891  | saving best model ...\n",
      "Current loss:  4.875183582305908  | , previous best loss:  4.8804097175598145  | saving best model ...\n",
      "Current loss:  4.869973659515381  | , previous best loss:  4.875183582305908  | saving best model ...\n",
      "Current loss:  4.864780902862549  | , previous best loss:  4.869973659515381  | saving best model ...\n",
      "Current loss:  4.859604358673096  | , previous best loss:  4.864780902862549  | saving best model ...\n",
      "Current loss:  4.8544464111328125  | , previous best loss:  4.859604358673096  | saving best model ...\n",
      "Current loss:  4.849303722381592  | , previous best loss:  4.8544464111328125  | saving best model ...\n",
      "Current loss:  4.844178676605225  | , previous best loss:  4.849303722381592  | saving best model ...\n",
      "Current loss:  4.8390703201293945  | , previous best loss:  4.844178676605225  | saving best model ...\n",
      "Current loss:  4.833978652954102  | , previous best loss:  4.8390703201293945  | saving best model ...\n",
      "Current loss:  4.828903675079346  | , previous best loss:  4.833978652954102  | saving best model ...\n",
      "Current loss:  4.823845386505127  | , previous best loss:  4.828903675079346  | saving best model ...\n",
      "Current loss:  4.818803310394287  | , previous best loss:  4.823845386505127  | saving best model ...\n",
      "Current loss:  4.813777446746826  | , previous best loss:  4.818803310394287  | saving best model ...\n",
      "Current loss:  4.808769226074219  | , previous best loss:  4.813777446746826  | saving best model ...\n",
      "Current loss:  4.803776264190674  | , previous best loss:  4.808769226074219  | saving best model ...\n",
      "Current loss:  4.798800945281982  | , previous best loss:  4.803776264190674  | saving best model ...\n",
      "Current loss:  4.793840408325195  | , previous best loss:  4.798800945281982  | saving best model ...\n",
      "Current loss:  4.7888970375061035  | , previous best loss:  4.793840408325195  | saving best model ...\n",
      "Current loss:  4.783969879150391  | , previous best loss:  4.7888970375061035  | saving best model ...\n",
      "Current loss:  4.779058933258057  | , previous best loss:  4.783969879150391  | saving best model ...\n",
      "Current loss:  4.774164199829102  | , previous best loss:  4.779058933258057  | saving best model ...\n",
      "Current loss:  4.769285678863525  | , previous best loss:  4.774164199829102  | saving best model ...\n",
      "Current loss:  4.764422416687012  | , previous best loss:  4.769285678863525  | saving best model ...\n",
      "Current loss:  4.759575366973877  | , previous best loss:  4.764422416687012  | saving best model ...\n",
      "Current loss:  4.754744529724121  | , previous best loss:  4.759575366973877  | saving best model ...\n",
      "Current loss:  4.749929428100586  | , previous best loss:  4.754744529724121  | saving best model ...\n",
      "Current loss:  4.7451300621032715  | , previous best loss:  4.749929428100586  | saving best model ...\n",
      "Current loss:  4.740345478057861  | , previous best loss:  4.7451300621032715  | saving best model ...\n",
      "Current loss:  4.7355780601501465  | , previous best loss:  4.740345478057861  | saving best model ...\n",
      "Current loss:  4.730825901031494  | , previous best loss:  4.7355780601501465  | saving best model ...\n",
      "Current loss:  4.726089000701904  | , previous best loss:  4.730825901031494  | saving best model ...\n",
      "Current loss:  4.721367359161377  | , previous best loss:  4.726089000701904  | saving best model ...\n",
      "Current loss:  4.71666145324707  | , previous best loss:  4.721367359161377  | saving best model ...\n",
      "Current loss:  4.711970806121826  | , previous best loss:  4.71666145324707  | saving best model ...\n",
      "Current loss:  4.7072954177856445  | , previous best loss:  4.711970806121826  | saving best model ...\n",
      "Current loss:  4.702635765075684  | , previous best loss:  4.7072954177856445  | saving best model ...\n",
      "Current loss:  4.697991371154785  | , previous best loss:  4.702635765075684  | saving best model ...\n",
      "Current loss:  4.693360805511475  | , previous best loss:  4.697991371154785  | saving best model ...\n",
      "Current loss:  4.688745975494385  | , previous best loss:  4.693360805511475  | saving best model ...\n",
      "Current loss:  4.684146404266357  | , previous best loss:  4.688745975494385  | saving best model ...\n",
      "Current loss:  4.679562568664551  | , previous best loss:  4.684146404266357  | saving best model ...\n",
      "Current loss:  4.674992084503174  | , previous best loss:  4.679562568664551  | saving best model ...\n",
      "Current loss:  4.670437335968018  | , previous best loss:  4.674992084503174  | saving best model ...\n",
      "Current loss:  4.665896892547607  | , previous best loss:  4.670437335968018  | saving best model ...\n",
      "Current loss:  4.661371231079102  | , previous best loss:  4.665896892547607  | saving best model ...\n",
      "Current loss:  4.656861305236816  | , previous best loss:  4.661371231079102  | saving best model ...\n",
      "Current loss:  4.652365207672119  | , previous best loss:  4.656861305236816  | saving best model ...\n",
      "Current loss:  4.647883415222168  | , previous best loss:  4.652365207672119  | saving best model ...\n",
      "Current loss:  4.643415451049805  | , previous best loss:  4.647883415222168  | saving best model ...\n",
      "Current loss:  4.638962745666504  | , previous best loss:  4.643415451049805  | saving best model ...\n",
      "Current loss:  4.634524345397949  | , previous best loss:  4.638962745666504  | saving best model ...\n",
      "Current loss:  4.630099773406982  | , previous best loss:  4.634524345397949  | saving best model ...\n",
      "Current loss:  4.62568998336792  | , previous best loss:  4.630099773406982  | saving best model ...\n",
      "Current loss:  4.621294021606445  | , previous best loss:  4.62568998336792  | saving best model ...\n",
      "Current loss:  4.616911888122559  | , previous best loss:  4.621294021606445  | saving best model ...\n",
      "Current loss:  4.61254358291626  | , previous best loss:  4.616911888122559  | saving best model ...\n",
      "Current loss:  4.608190059661865  | , previous best loss:  4.61254358291626  | saving best model ...\n",
      "Current loss:  4.603849411010742  | , previous best loss:  4.608190059661865  | saving best model ...\n",
      "Current loss:  4.599523544311523  | , previous best loss:  4.603849411010742  | saving best model ...\n",
      "Current loss:  4.595211029052734  | , previous best loss:  4.599523544311523  | saving best model ...\n",
      "Current loss:  4.590911388397217  | , previous best loss:  4.595211029052734  | saving best model ...\n",
      "Current loss:  4.5866265296936035  | , previous best loss:  4.590911388397217  | saving best model ...\n",
      "Current loss:  4.582354545593262  | , previous best loss:  4.5866265296936035  | saving best model ...\n",
      "Current loss:  4.578096866607666  | , previous best loss:  4.582354545593262  | saving best model ...\n",
      "Current loss:  4.573851585388184  | , previous best loss:  4.578096866607666  | saving best model ...\n",
      "Current loss:  4.569620132446289  | , previous best loss:  4.573851585388184  | saving best model ...\n",
      "Current loss:  4.565402030944824  | , previous best loss:  4.569620132446289  | saving best model ...\n",
      "Current loss:  4.561196327209473  | , previous best loss:  4.565402030944824  | saving best model ...\n",
      "Current loss:  4.557004451751709  | , previous best loss:  4.561196327209473  | saving best model ...\n",
      "Current loss:  4.552825450897217  | , previous best loss:  4.557004451751709  | saving best model ...\n",
      "Current loss:  4.548659801483154  | , previous best loss:  4.552825450897217  | saving best model ...\n",
      "Current loss:  4.5445075035095215  | , previous best loss:  4.548659801483154  | saving best model ...\n",
      "Current loss:  4.540367126464844  | , previous best loss:  4.5445075035095215  | saving best model ...\n",
      "Current loss:  4.5362396240234375  | , previous best loss:  4.540367126464844  | saving best model ...\n",
      "Current loss:  4.532125473022461  | , previous best loss:  4.5362396240234375  | saving best model ...\n",
      "Current loss:  4.5280232429504395  | , previous best loss:  4.532125473022461  | saving best model ...\n",
      "Current loss:  4.523934364318848  | , previous best loss:  4.5280232429504395  | saving best model ...\n",
      "Current loss:  4.519857406616211  | , previous best loss:  4.523934364318848  | saving best model ...\n",
      "Current loss:  4.515793323516846  | , previous best loss:  4.519857406616211  | saving best model ...\n",
      "Current loss:  4.511741638183594  | , previous best loss:  4.515793323516846  | saving best model ...\n",
      "Current loss:  4.5077033042907715  | , previous best loss:  4.511741638183594  | saving best model ...\n",
      "Current loss:  4.50367546081543  | , previous best loss:  4.5077033042907715  | saving best model ...\n",
      "Current loss:  4.499660015106201  | , previous best loss:  4.50367546081543  | saving best model ...\n",
      "Current loss:  4.495657920837402  | , previous best loss:  4.499660015106201  | saving best model ...\n",
      "Current loss:  4.491666793823242  | , previous best loss:  4.495657920837402  | saving best model ...\n",
      "Current loss:  4.487688064575195  | , previous best loss:  4.491666793823242  | saving best model ...\n",
      "Current loss:  4.483721733093262  | , previous best loss:  4.487688064575195  | saving best model ...\n",
      "Current loss:  4.479767322540283  | , previous best loss:  4.483721733093262  | saving best model ...\n",
      "Current loss:  4.475824356079102  | , previous best loss:  4.479767322540283  | saving best model ...\n",
      "Current loss:  4.471893787384033  | , previous best loss:  4.475824356079102  | saving best model ...\n",
      "Current loss:  4.467973232269287  | , previous best loss:  4.471893787384033  | saving best model ...\n",
      "Current loss:  4.464065074920654  | , previous best loss:  4.467973232269287  | saving best model ...\n",
      "Current loss:  4.460168838500977  | , previous best loss:  4.464065074920654  | saving best model ...\n",
      "Current loss:  4.4562835693359375  | , previous best loss:  4.460168838500977  | saving best model ...\n",
      "Current loss:  4.4524102210998535  | , previous best loss:  4.4562835693359375  | saving best model ...\n",
      "Current loss:  4.448547840118408  | , previous best loss:  4.4524102210998535  | saving best model ...\n",
      "Current loss:  4.444697380065918  | , previous best loss:  4.448547840118408  | saving best model ...\n",
      "Current loss:  4.440857410430908  | , previous best loss:  4.444697380065918  | saving best model ...\n",
      "Current loss:  4.437028884887695  | , previous best loss:  4.440857410430908  | saving best model ...\n",
      "Current loss:  4.433211803436279  | , previous best loss:  4.437028884887695  | saving best model ...\n",
      "Current loss:  4.429405212402344  | , previous best loss:  4.433211803436279  | saving best model ...\n",
      "Current loss:  4.425609588623047  | , previous best loss:  4.429405212402344  | saving best model ...\n",
      "Current loss:  4.421825408935547  | , previous best loss:  4.425609588623047  | saving best model ...\n",
      "Current loss:  4.418051719665527  | , previous best loss:  4.421825408935547  | saving best model ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[268], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Iteration):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Forward pass: Compute predicted y by passing x to the modelsp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     pyb_af \u001b[38;5;241m=\u001b[39m DeepPS(X_train)\n\u001b[1;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_train, pyb_af); bloss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m ((bloss_list[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39mbloss_list[t])\u001b[38;5;241m<\u001b[39mtor):        \n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[262], line 23\u001b[0m, in \u001b[0;36mDPS.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m batch_size, _ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# # # # # # # # # # # # # #\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#         SPLINE 1        #\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# # # # # # # # # # # # # #\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m sp1out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp1(ln1out)\n\u001b[1;32m     24\u001b[0m bslist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp1\u001b[38;5;241m.\u001b[39minter[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasic\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minter[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mebasic\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m bslist\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[231], line 64\u001b[0m, in \u001b[0;36mPRODBSplineLayerMultiFeature.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m spl \u001b[38;5;241m=\u001b[39m SplineTransformer(n_knots\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_knots, degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree, knots \u001b[38;5;241m=\u001b[39m knots)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_features):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Calculate B-spline basis functions for this feature\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     basis \u001b[38;5;241m=\u001b[39m basis_function(x[:, feature]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), spl)\n\u001b[1;32m     65\u001b[0m     basis \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(basis)\n\u001b[1;32m     66\u001b[0m     basises\u001b[38;5;241m.\u001b[39mappend(basis)\n",
      "Cell \u001b[0;32mIn[190], line 12\u001b[0m, in \u001b[0;36mbasis_function\u001b[0;34m(x, spl)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbasis_function\u001b[39m(x, spl):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spl\u001b[38;5;241m.\u001b[39mfit_transform(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/preprocessing/_polynomial.py:1058\u001b[0m, in \u001b[0;36mSplineTransformer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1056\u001b[0m             XBS_sparse[mask_inv, :] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1058\u001b[0m         XBS[mask, (i \u001b[38;5;241m*\u001b[39m n_splines) : ((i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m n_splines)] \u001b[38;5;241m=\u001b[39m spl(X[mask, i])\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# Note for extrapolation:\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# 'continue' is already returned as is by scipy BSplines\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextrapolation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# BSpline with extrapolate=False does not raise an error, but\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;66;03m# outputs np.nan.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/scipy/interpolate/_bsplines.py:523\u001b[0m, in \u001b[0;36mBSpline.__call__\u001b[0;34m(self, x, nu, extrapolate)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    521\u001b[0m     cc \u001b[38;5;241m=\u001b[39m cc\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 523\u001b[0m _dierckx\u001b[38;5;241m.\u001b[39mevaluate_spline(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt, cc\u001b[38;5;241m.\u001b[39mreshape(cc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    524\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, x, nu, extrapolate, out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mfloat\u001b[39m))\n\u001b[1;32m    526\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mreshape(x_shape \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;66;03m# transpose to move the calculated values to the interpolation axis\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(Iteration):\n",
    "    # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "    pyb_af = DeepPS(X_train)\n",
    "    loss = criterion(y_train, pyb_af); bloss_list.append(loss.item())\n",
    "    \n",
    "    if (t > 0) and ((bloss_list[t-1]-bloss_list[t])<tor):        \n",
    "        if (tpat != 0) and (tpat % patientr) == 0:\n",
    "            learning_r *= 0.2 \n",
    "            tpat += 1\n",
    "            #print('Learning rate reduce to ', learning_r)\n",
    "            optimizer = torch.optim.Adam(DeepPS.parameters(), lr=learning_r)\n",
    "            if learning_r <= lr_tor:\n",
    "                if t < patientc + 1:\n",
    "                    conv = False\n",
    "                else:\n",
    "                    conv = True\n",
    "                print('Convergence!')\n",
    "                break\n",
    "        elif tpat < patientc:\n",
    "            tpat += 1\n",
    "            pass\n",
    "        else:\n",
    "            if t < patientc + 1:\n",
    "                conv = False\n",
    "            else:\n",
    "                conv = True\n",
    "            print('Convergence!')\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        if loss < bloss:\n",
    "            print('Current loss: ', loss.item(), ' | , previous best loss: ', bloss, ' | saving best model ...')\n",
    "            torch.save(DeepPS.state_dict(), './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1))\n",
    "            bloss = loss.item()\n",
    "            tpat = 0\n",
    "        else:\n",
    "            tpat += 1\n",
    "    \n",
    "    if tpat == patientc:\n",
    "        if t < patientc + 1:\n",
    "            conv = False\n",
    "        else:\n",
    "            conv = True\n",
    "            print('Convergence!')\n",
    "        break\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabec794-0371-428a-b207-045dd5623b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "5e637620-3f84-4074-9b53-553a120e9e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    eval_model = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, output_dim = Fout, bias = True).to(device)\n",
    "    eval_model.load_state_dict(torch.load( './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1), weights_only = True))\n",
    "    DPSy = eval_model(X_train)\n",
    "    LambdaB = ECM(model = eval_model, num_neurons = nm, num_knots = nk)\n",
    "    Lambdalist[str(d+1)] = LambdaB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "d902f267-cdaa-49ff-a1ff-cf668f90adcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.4181)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(DPSy, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbfeb0-6fd8-45a6-95f7-ecfd487e887b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e937f0f-b150-4d0b-b4cc-b37c1f8e2604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTORCH",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
