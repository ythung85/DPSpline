{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d1745a-f822-44f1-af6d-7cf0b4f382ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sim_data(n, dim, Type):\n",
    "\tif Type == 'A':\n",
    "\t\tX = torch.rand((n,dim))\n",
    "\t\ty = torch.exp(2*torch.sin(X[:,0]*0.5*torch.pi)+ 0.5*torch.cos(X[:,1]*2.5*torch.pi))\n",
    "\t\ty = y.reshape(-1,1)\n",
    "\t\ty = y.float()\n",
    "\t\t\n",
    "\telif Type == 'B':\n",
    "\t\tX = torch.rand((n, dim))\n",
    "\t\ty = 1\n",
    "\t\tfor d in range(dim):\n",
    "\t\t\ta = (d+1)/2\n",
    "\t\t\ty *= ((torch.abs(4*X[:,d]-2)+a)/(1+a))\n",
    "\t\ty = y.reshape(-1,1)\n",
    "\t\ty = y.float()\n",
    "\telse:\n",
    "\t\tpass\n",
    "\t\n",
    "\treturn X, y\n",
    "\t\n",
    "def norm(x):\n",
    "\treturn (x-torch.min(x))/(torch.max(x)-torch.min(x))\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def diag_mat_weights(dimp, type = 'first'):\n",
    "\tif type == 'first':\n",
    "\t\tdg = np.zeros((dimp-1, dimp))\n",
    "\t\tfor i in range(dimp-1):\n",
    "\t\t\tdg[i,i] = -1\n",
    "\t\t\tdg[i,i+1]= 1\n",
    "\telif type == 'second':\n",
    "\t\tdg = np.zeros((dimp-2, dimp))\n",
    "\t\tfor i in range(dimp-2):\n",
    "\t\t\tdg[i,i] = -1\n",
    "\t\t\tdg[i,i+1]= 2\n",
    "\t\t\tdg[i,i+2]= -1\n",
    "\telse:\n",
    "\t\tpass\n",
    "\treturn torch.Tensor(dg)\n",
    "\n",
    "\n",
    "class PRODBSplineLayerMultiFeature(nn.Module):\n",
    "\tdef __init__(self, input_dim, degree, num_knots, output_dim, num_neurons, bias = True):\n",
    "\t\tsuper(PRODBSplineLayerMultiFeature, self).__init__()\n",
    "\t\tself.degree = degree\n",
    "\t\tself.num_knots = num_knots\n",
    "\t\tself.input_dim = input_dim\n",
    "\t\tself.output_dim = output_dim\n",
    "\t\tself.num_neurons = num_neurons\n",
    "\t\t\n",
    "\t\tif input_dim == 2:\n",
    "\t\t\tself.control_p = nn.Parameter(torch.randn(self.num_knots**2, self.output_dim))\n",
    "\t\telse:\n",
    "\t\t\tself.control_p = nn.Parameter(torch.randn(self.num_knots, self.num_neurons))\n",
    "\t\tif bias:\n",
    "\t\t\tself.bias = nn.Parameter(torch.randn(self.num_neurons))\n",
    "\t\telse:\n",
    "\t\t\tself.register_parameter('bias', None)\n",
    "\t\t\t\n",
    "\t\tself.inter = {}\n",
    "\t\n",
    "\tdef basis_function(self, x, i, k, t):\n",
    "\t\n",
    "\t\t# Base case: degree 0 spline\n",
    "\t\tif k == 0:\n",
    "\t\t\treturn ((t[i] <= x) & (x < t[i + 1])).float()\n",
    "\t\n",
    "\t\t# Recursive case\n",
    "\t\tdenom1 = t[i + k] - t[i]\n",
    "\t\tdenom2 = t[i + k + 1] - t[i + 1]\n",
    "\t\n",
    "\t\tterm1 = 0\n",
    "\t\tif denom1 != 0:\n",
    "\t\t\tterm1 = (x - t[i]) / denom1 * self.basis_function(x, i, k - 1, t)\n",
    "\t\n",
    "\t\tterm2 = 0\n",
    "\t\tif denom2 != 0:\n",
    "\t\t\tterm2 = (t[i + k + 1] - x) / denom2 * self.basis_function(x, i + 1, k - 1, t)\n",
    "\t\n",
    "\t\treturn term1 + term2\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tbatch_size, num_features = x.size()\n",
    "\t\tdevice = x.device\n",
    "\t\t\n",
    "\t\t# Create knot vector\n",
    "\t\t# knots = torch.linspace(0, 1, self.num_knots + self.degree + 1).to(device)\n",
    "\t\tknots = torch.cat([\n",
    "\t\t\t\t\t\ttorch.zeros(self.degree),               # Add repeated values at the start for clamping\n",
    "\t\t\t\t\t\ttorch.linspace(0, 1, self.num_knots - self.degree + 1),  # Uniform knot spacing in the middle\n",
    "\t\t\t\t\t\ttorch.ones(self.degree)                 # Add repeated values at the end for clamping\n",
    "\t\t\t\t\t]).to(device)\n",
    "\t\t# Apply B-spline basis functions for each feature\n",
    "\t\tbasises = []\n",
    "\t\n",
    "\t\t\n",
    "\t\tfor feature in range(num_features):\n",
    "\t\t\t# Calculate B-spline basis functions for this feature\n",
    "\t\t\tbasis = torch.stack([self.basis_function(x[:, feature], i, self.degree, knots) \n",
    "\t\t\t\t\t\t\t\t for i in range(self.num_knots)], dim=-1)\n",
    "\t\t\tbasises.append(basis)\n",
    "\t\t\t\n",
    "\t\t\n",
    "\t\tif num_features == 1:\n",
    "\t\t\ttout = basises[0] @ self.control_p\n",
    "\t\t\tself.inter['basic'] = basises[0].T\n",
    "\t\telse:\n",
    "\t\t\tself.inter['basic'] = torch.reshape(torch.stack(basises, dim = 1), (batch_size, self.num_knots * self.num_neurons)).T\n",
    "\t\t\tbasises = torch.stack(basises)\n",
    "\t\t\ttout = basises.permute(1,2,0) * self.control_p\n",
    "\t\t\ttout = tout.sum(dim =1)\n",
    "\t\t\t\t\n",
    "\t\tif self.bias is not None:\n",
    "\t\t\ttout += self.bias        \n",
    "\t\t\t\n",
    "\t\treturn tout\n",
    "\n",
    "\n",
    "class NormLayer(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(NormLayer, self).__init__()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tmin_val = torch.min(x, axis = 1).values.reshape(-1,1)\n",
    "\t\tmax_val = torch.max(x, axis = 1).values.reshape(-1,1)\n",
    "\n",
    "\t\tx = (x - min_val)/(max_val - min_val)  # Rescale to [0, 1]\n",
    "\t\treturn x.detach()\n",
    "\n",
    "class MPSv3(nn.Module):\n",
    "\tdef __init__(self, input_dim, degree, num_knots, num_neurons, output_dim, bias):\n",
    "\t\tsuper(MPSv3, self).__init__()\n",
    "\t\tself.num_neurons = num_neurons\n",
    "\t\tself.num_knots = num_knots\n",
    "\t\tself.ln1 = nn.Linear(input_dim, num_neurons)\n",
    "\t\tself.nm1 = NormLayer() \n",
    "\t\tself.sp1 = PRODBSplineLayerMultiFeature(input_dim = 1, degree = degree, num_knots = num_knots, num_neurons = num_neurons, output_dim= output_dim, bias = True)\n",
    "\t\tself.ln2 = nn.Linear(num_neurons, output_dim)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.inter = {}\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tln1out = self.ln1(x)\n",
    "\t\tln1out = self.nm1(ln1out)\n",
    "\t\t\n",
    "\t\tdevice = ln1out.device\n",
    "\t\tbatch_size, _ = x.size()\n",
    "\t\t\n",
    "\t\t# # # # # # # # # # # # # #\n",
    "\t\t#         SPLINE 1        #\n",
    "\t\t# # # # # # # # # # # # # #\n",
    "\t\t\n",
    "\t\tsp1out = self.sp1(ln1out)\n",
    "\t\tbslist = self.sp1.inter['basic']\n",
    "\t\t\n",
    "\t\tself.inter['ebasic'] = bslist\n",
    "\t\tself.inter['basic'] = sp1out\n",
    "\n",
    "\t\tln2out = self.ln2(sp1out)\n",
    "\t\tln2out = self.relu(ln2out)\n",
    "\t\t\n",
    "\t\treturn ln2out\n",
    "\n",
    "def num_para(model):\n",
    "\ttp = 0\n",
    "\tfor param in model.parameters():\n",
    "\t\ttp += param.numel()\n",
    "\treturn tp\n",
    "\n",
    "\n",
    "def ECM(model, num_neurons, num_knots, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4):\n",
    "\tlambdab = initial_lambda\n",
    "\tsigma = initial_sigma\n",
    "\txi = initial_xi\n",
    "\t\n",
    "\tB = model.inter['ebasic']\n",
    "\tBy = model.inter['basic']\n",
    "\tWB = model.sp1.control_p\n",
    "\tDB = diag_mat_weights(WB.size()[0]).to(device)\n",
    "\tsize = B.size()[1]\n",
    "\tS = DB.T @ DB\n",
    "\tCov_a = (xi**2)* torch.linalg.pinv(S)\n",
    "\tCov_e = torch.eye(size*num_neurons)* sigma\n",
    "\t\n",
    "\tblock_y = torch.reshape(By, (-1,1))\n",
    "\tflatB = B.view(num_neurons, num_knots, size)\n",
    "\t\t\n",
    "\tsqr_xi= 0\n",
    "\tsqr_sig = 0\n",
    "\t\n",
    "\tfor i in range(num_neurons):\n",
    "\t\tNcov = (Cov_a -(Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)]) @ flatB[i].T @ Cov_a))\n",
    "\t\tNmu = (Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)])) @ By[:,i].reshape(-1,1)\n",
    "\t\t\n",
    "\t\tfirst_xi = S @ Ncov\n",
    "\t\tsecond_xi = (Nmu.T @ S @ Nmu)\n",
    "\t\tsqr_xi += torch.trace(first_xi) + second_xi\n",
    "\t\t\t\n",
    "\t\tfirst_sig = torch.norm(By[:,i])\n",
    "\t\tsecond_sig = 2 * (By[:,i] @ flatB[i].T) @ Nmu \n",
    "\t\tthird_sig = torch.trace((flatB[i] @ flatB[i].T) @ Ncov)\n",
    "\t\tfour_sig = (Nmu.T @ flatB[i] @ flatB[i].T @ Nmu)\n",
    "\t\t\n",
    "\t\tsqr_sig += (first_sig + second_sig + third_sig + four_sig)\n",
    "\t\n",
    "\tsqr_xi /= num_neurons\n",
    "\tsqr_sig /= (num_neurons*size)\n",
    "\t\n",
    "\tLambda = sqr_sig/sqr_xi\n",
    "\t\n",
    "\treturn Lambda.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c23cb-4c93-4fbb-876f-91530c18b96f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64e443ff-3729-4a1c-85ad-c500872d5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 1000; ntest = 2500; ndim = 10; ndf = 1; nk = 15; nm = 50; Fout = 1\n",
    "Dtype = 'A'\n",
    "data = {}\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for d in range(ndf):\n",
    "    X_train, y_train = sim_data(ntrain, ndim, Dtype)\n",
    "    X_test, y_test = sim_data(ntest, ndim, Dtype)\n",
    "    epstrain = torch.normal(0, torch.var(y_train)*0.05, size=y_train.size())\n",
    "    epstest = torch.normal(0,  torch.var(y_train)*0.05, size=y_test.size())\n",
    "    \n",
    "    y_train, y_test = y_train + epstrain, y_test + epstest\n",
    "    data[str(d+1)] = {'TrainX': X_train, 'Trainy': y_train, 'TestX': X_test, 'Testy': y_test}\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7098de70-4ba8-419a-9dbd-3165171a16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MBS = MPSv3(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, output_dim = Fout, bias = True).to(device)\n",
    "learning_r = 1e-3\n",
    "optimizer = torch.optim.Adam(MBS.parameters(), lr=learning_r)\n",
    "Iteration = 10000; bloss_list = []; tor = 1e-5; lr_tor = 1e-6\n",
    "patientc = 30; patientr = 10; tpat = 0; bloss = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd6d1160-a7d7-452b-b9a0-e8daa13b9ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss:  5.4331231117248535  | , previous best loss:  5.440192222595215  | saving best model ...\n",
      "Current loss:  5.426070213317871  | , previous best loss:  5.4331231117248535  | saving best model ...\n",
      "Current loss:  5.419033527374268  | , previous best loss:  5.426070213317871  | saving best model ...\n",
      "Current loss:  5.412015438079834  | , previous best loss:  5.419033527374268  | saving best model ...\n",
      "Current loss:  5.405013561248779  | , previous best loss:  5.412015438079834  | saving best model ...\n",
      "Current loss:  5.398028373718262  | , previous best loss:  5.405013561248779  | saving best model ...\n",
      "Current loss:  5.3910603523254395  | , previous best loss:  5.398028373718262  | saving best model ...\n",
      "Current loss:  5.3841094970703125  | , previous best loss:  5.3910603523254395  | saving best model ...\n",
      "Current loss:  5.377175807952881  | , previous best loss:  5.3841094970703125  | saving best model ...\n",
      "Current loss:  5.370259761810303  | , previous best loss:  5.377175807952881  | saving best model ...\n",
      "Current loss:  5.363360404968262  | , previous best loss:  5.370259761810303  | saving best model ...\n",
      "Current loss:  5.356478691101074  | , previous best loss:  5.363360404968262  | saving best model ...\n",
      "Current loss:  5.349615097045898  | , previous best loss:  5.356478691101074  | saving best model ...\n",
      "Current loss:  5.342769145965576  | , previous best loss:  5.349615097045898  | saving best model ...\n",
      "Current loss:  5.335941314697266  | , previous best loss:  5.342769145965576  | saving best model ...\n",
      "Current loss:  5.329130172729492  | , previous best loss:  5.335941314697266  | saving best model ...\n",
      "Current loss:  5.322338104248047  | , previous best loss:  5.329130172729492  | saving best model ...\n",
      "Current loss:  5.315563678741455  | , previous best loss:  5.322338104248047  | saving best model ...\n",
      "Current loss:  5.308806896209717  | , previous best loss:  5.315563678741455  | saving best model ...\n",
      "Current loss:  5.302068710327148  | , previous best loss:  5.308806896209717  | saving best model ...\n",
      "Current loss:  5.295348644256592  | , previous best loss:  5.302068710327148  | saving best model ...\n",
      "Current loss:  5.288647174835205  | , previous best loss:  5.295348644256592  | saving best model ...\n",
      "Current loss:  5.28196382522583  | , previous best loss:  5.288647174835205  | saving best model ...\n",
      "Current loss:  5.275299549102783  | , previous best loss:  5.28196382522583  | saving best model ...\n",
      "Current loss:  5.26865291595459  | , previous best loss:  5.275299549102783  | saving best model ...\n",
      "Current loss:  5.262024879455566  | , previous best loss:  5.26865291595459  | saving best model ...\n",
      "Current loss:  5.255416393280029  | , previous best loss:  5.262024879455566  | saving best model ...\n",
      "Current loss:  5.248826026916504  | , previous best loss:  5.255416393280029  | saving best model ...\n",
      "Current loss:  5.242254734039307  | , previous best loss:  5.248826026916504  | saving best model ...\n",
      "Current loss:  5.235702037811279  | , previous best loss:  5.242254734039307  | saving best model ...\n",
      "Current loss:  5.229168891906738  | , previous best loss:  5.235702037811279  | saving best model ...\n",
      "Current loss:  5.222655296325684  | , previous best loss:  5.229168891906738  | saving best model ...\n",
      "Current loss:  5.216159820556641  | , previous best loss:  5.222655296325684  | saving best model ...\n",
      "Current loss:  5.209682941436768  | , previous best loss:  5.216159820556641  | saving best model ...\n",
      "Current loss:  5.2032270431518555  | , previous best loss:  5.209682941436768  | saving best model ...\n",
      "Current loss:  5.1967902183532715  | , previous best loss:  5.2032270431518555  | saving best model ...\n",
      "Current loss:  5.190371513366699  | , previous best loss:  5.1967902183532715  | saving best model ...\n",
      "Current loss:  5.18397331237793  | , previous best loss:  5.190371513366699  | saving best model ...\n",
      "Current loss:  5.177594184875488  | , previous best loss:  5.18397331237793  | saving best model ...\n",
      "Current loss:  5.171234607696533  | , previous best loss:  5.177594184875488  | saving best model ...\n",
      "Current loss:  5.164895057678223  | , previous best loss:  5.171234607696533  | saving best model ...\n",
      "Current loss:  5.158574104309082  | , previous best loss:  5.164895057678223  | saving best model ...\n",
      "Current loss:  5.1522746086120605  | , previous best loss:  5.158574104309082  | saving best model ...\n",
      "Current loss:  5.145993232727051  | , previous best loss:  5.1522746086120605  | saving best model ...\n",
      "Current loss:  5.13973331451416  | , previous best loss:  5.145993232727051  | saving best model ...\n",
      "Current loss:  5.133492469787598  | , previous best loss:  5.13973331451416  | saving best model ...\n",
      "Current loss:  5.12727165222168  | , previous best loss:  5.133492469787598  | saving best model ...\n",
      "Current loss:  5.1210713386535645  | , previous best loss:  5.12727165222168  | saving best model ...\n",
      "Current loss:  5.114890098571777  | , previous best loss:  5.1210713386535645  | saving best model ...\n",
      "Current loss:  5.108729362487793  | , previous best loss:  5.114890098571777  | saving best model ...\n",
      "Current loss:  5.102589130401611  | , previous best loss:  5.108729362487793  | saving best model ...\n",
      "Current loss:  5.096469879150391  | , previous best loss:  5.102589130401611  | saving best model ...\n",
      "Current loss:  5.090369701385498  | , previous best loss:  5.096469879150391  | saving best model ...\n",
      "Current loss:  5.084290027618408  | , previous best loss:  5.090369701385498  | saving best model ...\n",
      "Current loss:  5.078230381011963  | , previous best loss:  5.084290027618408  | saving best model ...\n",
      "Current loss:  5.072192192077637  | , previous best loss:  5.078230381011963  | saving best model ...\n",
      "Current loss:  5.066174030303955  | , previous best loss:  5.072192192077637  | saving best model ...\n",
      "Current loss:  5.060175895690918  | , previous best loss:  5.066174030303955  | saving best model ...\n",
      "Current loss:  5.054198741912842  | , previous best loss:  5.060175895690918  | saving best model ...\n",
      "Current loss:  5.048242092132568  | , previous best loss:  5.054198741912842  | saving best model ...\n",
      "Current loss:  5.042305946350098  | , previous best loss:  5.048242092132568  | saving best model ...\n",
      "Current loss:  5.036390781402588  | , previous best loss:  5.042305946350098  | saving best model ...\n",
      "Current loss:  5.030496120452881  | , previous best loss:  5.036390781402588  | saving best model ...\n",
      "Current loss:  5.024621486663818  | , previous best loss:  5.030496120452881  | saving best model ...\n",
      "Current loss:  5.018769264221191  | , previous best loss:  5.024621486663818  | saving best model ...\n",
      "Current loss:  5.012936115264893  | , previous best loss:  5.018769264221191  | saving best model ...\n",
      "Current loss:  5.007124900817871  | , previous best loss:  5.012936115264893  | saving best model ...\n",
      "Current loss:  5.0013346672058105  | , previous best loss:  5.007124900817871  | saving best model ...\n",
      "Current loss:  4.9955644607543945  | , previous best loss:  5.0013346672058105  | saving best model ...\n",
      "Current loss:  4.9898152351379395  | , previous best loss:  4.9955644607543945  | saving best model ...\n",
      "Current loss:  4.9840874671936035  | , previous best loss:  4.9898152351379395  | saving best model ...\n",
      "Current loss:  4.978379726409912  | , previous best loss:  4.9840874671936035  | saving best model ...\n",
      "Current loss:  4.972694396972656  | , previous best loss:  4.978379726409912  | saving best model ...\n",
      "Current loss:  4.967028617858887  | , previous best loss:  4.972694396972656  | saving best model ...\n",
      "Current loss:  4.961385250091553  | , previous best loss:  4.967028617858887  | saving best model ...\n",
      "Current loss:  4.955761909484863  | , previous best loss:  4.961385250091553  | saving best model ...\n",
      "Current loss:  4.950160026550293  | , previous best loss:  4.955761909484863  | saving best model ...\n",
      "Current loss:  4.944579124450684  | , previous best loss:  4.950160026550293  | saving best model ...\n",
      "Current loss:  4.939019680023193  | , previous best loss:  4.944579124450684  | saving best model ...\n",
      "Current loss:  4.933481216430664  | , previous best loss:  4.939019680023193  | saving best model ...\n",
      "Current loss:  4.927964687347412  | , previous best loss:  4.933481216430664  | saving best model ...\n",
      "Current loss:  4.922467231750488  | , previous best loss:  4.927964687347412  | saving best model ...\n",
      "Current loss:  4.916992664337158  | , previous best loss:  4.922467231750488  | saving best model ...\n",
      "Current loss:  4.911538600921631  | , previous best loss:  4.916992664337158  | saving best model ...\n",
      "Current loss:  4.906106472015381  | , previous best loss:  4.911538600921631  | saving best model ...\n",
      "Current loss:  4.900694370269775  | , previous best loss:  4.906106472015381  | saving best model ...\n",
      "Current loss:  4.8953046798706055  | , previous best loss:  4.900694370269775  | saving best model ...\n",
      "Current loss:  4.88993501663208  | , previous best loss:  4.8953046798706055  | saving best model ...\n",
      "Current loss:  4.884586811065674  | , previous best loss:  4.88993501663208  | saving best model ...\n",
      "Current loss:  4.879260063171387  | , previous best loss:  4.884586811065674  | saving best model ...\n",
      "Current loss:  4.873955249786377  | , previous best loss:  4.879260063171387  | saving best model ...\n",
      "Current loss:  4.86867094039917  | , previous best loss:  4.873955249786377  | saving best model ...\n",
      "Current loss:  4.86340856552124  | , previous best loss:  4.86867094039917  | saving best model ...\n",
      "Current loss:  4.858166694641113  | , previous best loss:  4.86340856552124  | saving best model ...\n",
      "Current loss:  4.8529462814331055  | , previous best loss:  4.858166694641113  | saving best model ...\n",
      "Current loss:  4.8477463722229  | , previous best loss:  4.8529462814331055  | saving best model ...\n",
      "Current loss:  4.842568397521973  | , previous best loss:  4.8477463722229  | saving best model ...\n",
      "Current loss:  4.837411880493164  | , previous best loss:  4.842568397521973  | saving best model ...\n",
      "Current loss:  4.832275390625  | , previous best loss:  4.837411880493164  | saving best model ...\n",
      "Current loss:  4.8271613121032715  | , previous best loss:  4.832275390625  | saving best model ...\n",
      "Current loss:  4.822068214416504  | , previous best loss:  4.8271613121032715  | saving best model ...\n",
      "Current loss:  4.816996097564697  | , previous best loss:  4.822068214416504  | saving best model ...\n",
      "Current loss:  4.811944961547852  | , previous best loss:  4.816996097564697  | saving best model ...\n",
      "Current loss:  4.806914806365967  | , previous best loss:  4.811944961547852  | saving best model ...\n",
      "Current loss:  4.801906585693359  | , previous best loss:  4.806914806365967  | saving best model ...\n",
      "Current loss:  4.796919345855713  | , previous best loss:  4.801906585693359  | saving best model ...\n",
      "Current loss:  4.791953086853027  | , previous best loss:  4.796919345855713  | saving best model ...\n",
      "Current loss:  4.787007808685303  | , previous best loss:  4.791953086853027  | saving best model ...\n",
      "Current loss:  4.782083988189697  | , previous best loss:  4.787007808685303  | saving best model ...\n",
      "Current loss:  4.7771806716918945  | , previous best loss:  4.782083988189697  | saving best model ...\n",
      "Current loss:  4.772298336029053  | , previous best loss:  4.7771806716918945  | saving best model ...\n",
      "Current loss:  4.76743745803833  | , previous best loss:  4.772298336029053  | saving best model ...\n",
      "Current loss:  4.762598037719727  | , previous best loss:  4.76743745803833  | saving best model ...\n",
      "Current loss:  4.757778644561768  | , previous best loss:  4.762598037719727  | saving best model ...\n",
      "Current loss:  4.7529802322387695  | , previous best loss:  4.757778644561768  | saving best model ...\n",
      "Current loss:  4.748203754425049  | , previous best loss:  4.7529802322387695  | saving best model ...\n",
      "Current loss:  4.743447303771973  | , previous best loss:  4.748203754425049  | saving best model ...\n",
      "Current loss:  4.738711357116699  | , previous best loss:  4.743447303771973  | saving best model ...\n",
      "Current loss:  4.733997344970703  | , previous best loss:  4.738711357116699  | saving best model ...\n",
      "Current loss:  4.72930383682251  | , previous best loss:  4.733997344970703  | saving best model ...\n",
      "Current loss:  4.724630832672119  | , previous best loss:  4.72930383682251  | saving best model ...\n",
      "Current loss:  4.7199788093566895  | , previous best loss:  4.724630832672119  | saving best model ...\n",
      "Current loss:  4.715347766876221  | , previous best loss:  4.7199788093566895  | saving best model ...\n",
      "Current loss:  4.710736274719238  | , previous best loss:  4.715347766876221  | saving best model ...\n",
      "Current loss:  4.706147193908691  | , previous best loss:  4.710736274719238  | saving best model ...\n",
      "Current loss:  4.701577186584473  | , previous best loss:  4.706147193908691  | saving best model ...\n",
      "Current loss:  4.697028160095215  | , previous best loss:  4.701577186584473  | saving best model ...\n",
      "Current loss:  4.692500114440918  | , previous best loss:  4.697028160095215  | saving best model ...\n",
      "Current loss:  4.687992572784424  | , previous best loss:  4.692500114440918  | saving best model ...\n",
      "Current loss:  4.683505535125732  | , previous best loss:  4.687992572784424  | saving best model ...\n",
      "Current loss:  4.679038047790527  | , previous best loss:  4.683505535125732  | saving best model ...\n",
      "Current loss:  4.674592018127441  | , previous best loss:  4.679038047790527  | saving best model ...\n",
      "Current loss:  4.670166015625  | , previous best loss:  4.674592018127441  | saving best model ...\n",
      "Current loss:  4.665759563446045  | , previous best loss:  4.670166015625  | saving best model ...\n",
      "Current loss:  4.661374568939209  | , previous best loss:  4.665759563446045  | saving best model ...\n",
      "Current loss:  4.657008647918701  | , previous best loss:  4.661374568939209  | saving best model ...\n",
      "Current loss:  4.6526641845703125  | , previous best loss:  4.657008647918701  | saving best model ...\n",
      "Current loss:  4.648338794708252  | , previous best loss:  4.6526641845703125  | saving best model ...\n",
      "Current loss:  4.644033908843994  | , previous best loss:  4.648338794708252  | saving best model ...\n",
      "Current loss:  4.639748573303223  | , previous best loss:  4.644033908843994  | saving best model ...\n",
      "Current loss:  4.635483741760254  | , previous best loss:  4.639748573303223  | saving best model ...\n",
      "Current loss:  4.63123893737793  | , previous best loss:  4.635483741760254  | saving best model ...\n",
      "Current loss:  4.627013683319092  | , previous best loss:  4.63123893737793  | saving best model ...\n",
      "Current loss:  4.622808456420898  | , previous best loss:  4.627013683319092  | saving best model ...\n",
      "Current loss:  4.618622779846191  | , previous best loss:  4.622808456420898  | saving best model ...\n",
      "Current loss:  4.614457130432129  | , previous best loss:  4.618622779846191  | saving best model ...\n",
      "Current loss:  4.610311031341553  | , previous best loss:  4.614457130432129  | saving best model ...\n",
      "Current loss:  4.606184005737305  | , previous best loss:  4.610311031341553  | saving best model ...\n",
      "Current loss:  4.602077007293701  | , previous best loss:  4.606184005737305  | saving best model ...\n",
      "Current loss:  4.597989082336426  | , previous best loss:  4.602077007293701  | saving best model ...\n",
      "Current loss:  4.593920707702637  | , previous best loss:  4.597989082336426  | saving best model ...\n",
      "Current loss:  4.589871883392334  | , previous best loss:  4.593920707702637  | saving best model ...\n",
      "Current loss:  4.585842132568359  | , previous best loss:  4.589871883392334  | saving best model ...\n",
      "Current loss:  4.5818328857421875  | , previous best loss:  4.585842132568359  | saving best model ...\n",
      "Current loss:  4.577841281890869  | , previous best loss:  4.5818328857421875  | saving best model ...\n",
      "Current loss:  4.573869705200195  | , previous best loss:  4.577841281890869  | saving best model ...\n",
      "Current loss:  4.56991720199585  | , previous best loss:  4.573869705200195  | saving best model ...\n",
      "Current loss:  4.565982818603516  | , previous best loss:  4.56991720199585  | saving best model ...\n",
      "Current loss:  4.562067985534668  | , previous best loss:  4.565982818603516  | saving best model ...\n",
      "Current loss:  4.558172225952148  | , previous best loss:  4.562067985534668  | saving best model ...\n",
      "Current loss:  4.554295063018799  | , previous best loss:  4.558172225952148  | saving best model ...\n",
      "Current loss:  4.550436496734619  | , previous best loss:  4.554295063018799  | saving best model ...\n",
      "Current loss:  4.546597003936768  | , previous best loss:  4.550436496734619  | saving best model ...\n",
      "Current loss:  4.542776107788086  | , previous best loss:  4.546597003936768  | saving best model ...\n",
      "Current loss:  4.538973331451416  | , previous best loss:  4.542776107788086  | saving best model ...\n",
      "Current loss:  4.535189151763916  | , previous best loss:  4.538973331451416  | saving best model ...\n",
      "Current loss:  4.531423568725586  | , previous best loss:  4.535189151763916  | saving best model ...\n",
      "Current loss:  4.527675151824951  | , previous best loss:  4.531423568725586  | saving best model ...\n",
      "Current loss:  4.523946762084961  | , previous best loss:  4.527675151824951  | saving best model ...\n",
      "Current loss:  4.520236015319824  | , previous best loss:  4.523946762084961  | saving best model ...\n",
      "Current loss:  4.516543388366699  | , previous best loss:  4.520236015319824  | saving best model ...\n",
      "Current loss:  4.5128679275512695  | , previous best loss:  4.516543388366699  | saving best model ...\n",
      "Current loss:  4.509211540222168  | , previous best loss:  4.5128679275512695  | saving best model ...\n",
      "Current loss:  4.50557279586792  | , previous best loss:  4.509211540222168  | saving best model ...\n",
      "Current loss:  4.501951694488525  | , previous best loss:  4.50557279586792  | saving best model ...\n",
      "Current loss:  4.498348712921143  | , previous best loss:  4.501951694488525  | saving best model ...\n",
      "Current loss:  4.494762897491455  | , previous best loss:  4.498348712921143  | saving best model ...\n",
      "Current loss:  4.491195201873779  | , previous best loss:  4.494762897491455  | saving best model ...\n",
      "Current loss:  4.487644672393799  | , previous best loss:  4.491195201873779  | saving best model ...\n",
      "Current loss:  4.484111309051514  | , previous best loss:  4.487644672393799  | saving best model ...\n",
      "Current loss:  4.48059606552124  | , previous best loss:  4.484111309051514  | saving best model ...\n",
      "Current loss:  4.477097511291504  | , previous best loss:  4.48059606552124  | saving best model ...\n",
      "Current loss:  4.473616600036621  | , previous best loss:  4.477097511291504  | saving best model ...\n",
      "Current loss:  4.470152854919434  | , previous best loss:  4.473616600036621  | saving best model ...\n",
      "Current loss:  4.466706275939941  | , previous best loss:  4.470152854919434  | saving best model ...\n",
      "Current loss:  4.463275909423828  | , previous best loss:  4.466706275939941  | saving best model ...\n",
      "Current loss:  4.459863185882568  | , previous best loss:  4.463275909423828  | saving best model ...\n",
      "Current loss:  4.456467151641846  | , previous best loss:  4.459863185882568  | saving best model ...\n",
      "Current loss:  4.45308780670166  | , previous best loss:  4.456467151641846  | saving best model ...\n",
      "Current loss:  4.449725151062012  | , previous best loss:  4.45308780670166  | saving best model ...\n",
      "Current loss:  4.446378707885742  | , previous best loss:  4.449725151062012  | saving best model ...\n",
      "Current loss:  4.443049430847168  | , previous best loss:  4.446378707885742  | saving best model ...\n",
      "Current loss:  4.439736366271973  | , previous best loss:  4.443049430847168  | saving best model ...\n",
      "Current loss:  4.4364399909973145  | , previous best loss:  4.439736366271973  | saving best model ...\n",
      "Current loss:  4.433159351348877  | , previous best loss:  4.4364399909973145  | saving best model ...\n",
      "Current loss:  4.429894924163818  | , previous best loss:  4.433159351348877  | saving best model ...\n",
      "Current loss:  4.426647186279297  | , previous best loss:  4.429894924163818  | saving best model ...\n",
      "Current loss:  4.423414707183838  | , previous best loss:  4.426647186279297  | saving best model ...\n",
      "Current loss:  4.420198440551758  | , previous best loss:  4.423414707183838  | saving best model ...\n",
      "Current loss:  4.416997909545898  | , previous best loss:  4.420198440551758  | saving best model ...\n",
      "Current loss:  4.413814067840576  | , previous best loss:  4.416997909545898  | saving best model ...\n",
      "Current loss:  4.41064453125  | , previous best loss:  4.413814067840576  | saving best model ...\n",
      "Current loss:  4.407491683959961  | , previous best loss:  4.41064453125  | saving best model ...\n",
      "Current loss:  4.404353618621826  | , previous best loss:  4.407491683959961  | saving best model ...\n",
      "Current loss:  4.401231288909912  | , previous best loss:  4.404353618621826  | saving best model ...\n",
      "Current loss:  4.398124694824219  | , previous best loss:  4.401231288909912  | saving best model ...\n",
      "Current loss:  4.39503288269043  | , previous best loss:  4.398124694824219  | saving best model ...\n",
      "Current loss:  4.391956329345703  | , previous best loss:  4.39503288269043  | saving best model ...\n",
      "Current loss:  4.388895034790039  | , previous best loss:  4.391956329345703  | saving best model ...\n",
      "Current loss:  4.385848045349121  | , previous best loss:  4.388895034790039  | saving best model ...\n",
      "Current loss:  4.382816791534424  | , previous best loss:  4.385848045349121  | saving best model ...\n",
      "Current loss:  4.379800796508789  | , previous best loss:  4.382816791534424  | saving best model ...\n",
      "Current loss:  4.376798629760742  | , previous best loss:  4.379800796508789  | saving best model ...\n",
      "Current loss:  4.373811721801758  | , previous best loss:  4.376798629760742  | saving best model ...\n",
      "Current loss:  4.370838642120361  | , previous best loss:  4.373811721801758  | saving best model ...\n",
      "Current loss:  4.367880821228027  | , previous best loss:  4.370838642120361  | saving best model ...\n",
      "Current loss:  4.364936828613281  | , previous best loss:  4.367880821228027  | saving best model ...\n",
      "Current loss:  4.3620076179504395  | , previous best loss:  4.364936828613281  | saving best model ...\n",
      "Current loss:  4.359092712402344  | , previous best loss:  4.3620076179504395  | saving best model ...\n",
      "Current loss:  4.356192111968994  | , previous best loss:  4.359092712402344  | saving best model ...\n",
      "Current loss:  4.353305339813232  | , previous best loss:  4.356192111968994  | saving best model ...\n",
      "Current loss:  4.3504319190979  | , previous best loss:  4.353305339813232  | saving best model ...\n",
      "Current loss:  4.347573757171631  | , previous best loss:  4.3504319190979  | saving best model ...\n",
      "Current loss:  4.344727993011475  | , previous best loss:  4.347573757171631  | saving best model ...\n",
      "Current loss:  4.341897010803223  | , previous best loss:  4.344727993011475  | saving best model ...\n",
      "Current loss:  4.3390793800354  | , previous best loss:  4.341897010803223  | saving best model ...\n",
      "Current loss:  4.336275577545166  | , previous best loss:  4.3390793800354  | saving best model ...\n",
      "Current loss:  4.333484649658203  | , previous best loss:  4.336275577545166  | saving best model ...\n",
      "Current loss:  4.33070707321167  | , previous best loss:  4.333484649658203  | saving best model ...\n",
      "Current loss:  4.327943325042725  | , previous best loss:  4.33070707321167  | saving best model ...\n",
      "Current loss:  4.325192451477051  | , previous best loss:  4.327943325042725  | saving best model ...\n",
      "Current loss:  4.322454929351807  | , previous best loss:  4.325192451477051  | saving best model ...\n",
      "Current loss:  4.319730281829834  | , previous best loss:  4.322454929351807  | saving best model ...\n",
      "Current loss:  4.317018985748291  | , previous best loss:  4.319730281829834  | saving best model ...\n",
      "Current loss:  4.314320087432861  | , previous best loss:  4.317018985748291  | saving best model ...\n",
      "Current loss:  4.311634063720703  | , previous best loss:  4.314320087432861  | saving best model ...\n",
      "Current loss:  4.308960914611816  | , previous best loss:  4.311634063720703  | saving best model ...\n",
      "Current loss:  4.306300640106201  | , previous best loss:  4.308960914611816  | saving best model ...\n",
      "Current loss:  4.303652763366699  | , previous best loss:  4.306300640106201  | saving best model ...\n",
      "Current loss:  4.301016807556152  | , previous best loss:  4.303652763366699  | saving best model ...\n",
      "Current loss:  4.298394203186035  | , previous best loss:  4.301016807556152  | saving best model ...\n",
      "Current loss:  4.295783042907715  | , previous best loss:  4.298394203186035  | saving best model ...\n",
      "Current loss:  4.293184757232666  | , previous best loss:  4.295783042907715  | saving best model ...\n",
      "Current loss:  4.290597438812256  | , previous best loss:  4.293184757232666  | saving best model ...\n",
      "Current loss:  4.288023471832275  | , previous best loss:  4.290597438812256  | saving best model ...\n",
      "Current loss:  4.285460948944092  | , previous best loss:  4.288023471832275  | saving best model ...\n",
      "Current loss:  4.282909870147705  | , previous best loss:  4.285460948944092  | saving best model ...\n",
      "Current loss:  4.280371189117432  | , previous best loss:  4.282909870147705  | saving best model ...\n",
      "Current loss:  4.277843952178955  | , previous best loss:  4.280371189117432  | saving best model ...\n",
      "Current loss:  4.275328159332275  | , previous best loss:  4.277843952178955  | saving best model ...\n",
      "Current loss:  4.272824287414551  | , previous best loss:  4.275328159332275  | saving best model ...\n",
      "Current loss:  4.270331382751465  | , previous best loss:  4.272824287414551  | saving best model ...\n",
      "Current loss:  4.267849922180176  | , previous best loss:  4.270331382751465  | saving best model ...\n",
      "Current loss:  4.265380382537842  | , previous best loss:  4.267849922180176  | saving best model ...\n",
      "Current loss:  4.262921333312988  | , previous best loss:  4.265380382537842  | saving best model ...\n",
      "Current loss:  4.260473728179932  | , previous best loss:  4.262921333312988  | saving best model ...\n",
      "Current loss:  4.258037567138672  | , previous best loss:  4.260473728179932  | saving best model ...\n",
      "Current loss:  4.255611896514893  | , previous best loss:  4.258037567138672  | saving best model ...\n",
      "Current loss:  4.253197193145752  | , previous best loss:  4.255611896514893  | saving best model ...\n",
      "Current loss:  4.25079345703125  | , previous best loss:  4.253197193145752  | saving best model ...\n",
      "Current loss:  4.2484002113342285  | , previous best loss:  4.25079345703125  | saving best model ...\n",
      "Current loss:  4.2460174560546875  | , previous best loss:  4.2484002113342285  | saving best model ...\n",
      "Current loss:  4.243646144866943  | , previous best loss:  4.2460174560546875  | saving best model ...\n",
      "Current loss:  4.241284370422363  | , previous best loss:  4.243646144866943  | saving best model ...\n",
      "Current loss:  4.238933086395264  | , previous best loss:  4.241284370422363  | saving best model ...\n",
      "Current loss:  4.2365922927856445  | , previous best loss:  4.238933086395264  | saving best model ...\n",
      "Current loss:  4.234261512756348  | , previous best loss:  4.2365922927856445  | saving best model ...\n",
      "Current loss:  4.2319416999816895  | , previous best loss:  4.234261512756348  | saving best model ...\n",
      "Current loss:  4.2296319007873535  | , previous best loss:  4.2319416999816895  | saving best model ...\n",
      "Current loss:  4.227331161499023  | , previous best loss:  4.2296319007873535  | saving best model ...\n",
      "Current loss:  4.225040912628174  | , previous best loss:  4.227331161499023  | saving best model ...\n",
      "Current loss:  4.2227606773376465  | , previous best loss:  4.225040912628174  | saving best model ...\n",
      "Current loss:  4.220490455627441  | , previous best loss:  4.2227606773376465  | saving best model ...\n",
      "Current loss:  4.218229293823242  | , previous best loss:  4.220490455627441  | saving best model ...\n",
      "Current loss:  4.215978622436523  | , previous best loss:  4.218229293823242  | saving best model ...\n",
      "Current loss:  4.2137370109558105  | , previous best loss:  4.215978622436523  | saving best model ...\n",
      "Current loss:  4.2115044593811035  | , previous best loss:  4.2137370109558105  | saving best model ...\n",
      "Current loss:  4.209281921386719  | , previous best loss:  4.2115044593811035  | saving best model ...\n",
      "Current loss:  4.20706844329834  | , previous best loss:  4.209281921386719  | saving best model ...\n",
      "Current loss:  4.204863548278809  | , previous best loss:  4.20706844329834  | saving best model ...\n",
      "Current loss:  4.202669143676758  | , previous best loss:  4.204863548278809  | saving best model ...\n",
      "Current loss:  4.2004828453063965  | , previous best loss:  4.202669143676758  | saving best model ...\n",
      "Current loss:  4.198306560516357  | , previous best loss:  4.2004828453063965  | saving best model ...\n",
      "Current loss:  4.196138858795166  | , previous best loss:  4.198306560516357  | saving best model ...\n",
      "Current loss:  4.193979263305664  | , previous best loss:  4.196138858795166  | saving best model ...\n",
      "Current loss:  4.191830158233643  | , previous best loss:  4.193979263305664  | saving best model ...\n",
      "Current loss:  4.189688682556152  | , previous best loss:  4.191830158233643  | saving best model ...\n",
      "Current loss:  4.18755578994751  | , previous best loss:  4.189688682556152  | saving best model ...\n",
      "Current loss:  4.185431480407715  | , previous best loss:  4.18755578994751  | saving best model ...\n",
      "Current loss:  4.183316230773926  | , previous best loss:  4.185431480407715  | saving best model ...\n",
      "Current loss:  4.181209087371826  | , previous best loss:  4.183316230773926  | saving best model ...\n",
      "Current loss:  4.179110527038574  | , previous best loss:  4.181209087371826  | saving best model ...\n",
      "Current loss:  4.17702054977417  | , previous best loss:  4.179110527038574  | saving best model ...\n",
      "Current loss:  4.174938678741455  | , previous best loss:  4.17702054977417  | saving best model ...\n",
      "Current loss:  4.172865390777588  | , previous best loss:  4.174938678741455  | saving best model ...\n",
      "Current loss:  4.1707987785339355  | , previous best loss:  4.172865390777588  | saving best model ...\n",
      "Current loss:  4.1687421798706055  | , previous best loss:  4.1707987785339355  | saving best model ...\n",
      "Current loss:  4.166691780090332  | , previous best loss:  4.1687421798706055  | saving best model ...\n",
      "Current loss:  4.1646504402160645  | , previous best loss:  4.166691780090332  | saving best model ...\n",
      "Current loss:  4.162616729736328  | , previous best loss:  4.1646504402160645  | saving best model ...\n",
      "Current loss:  4.160590648651123  | , previous best loss:  4.162616729736328  | saving best model ...\n",
      "Current loss:  4.158572673797607  | , previous best loss:  4.160590648651123  | saving best model ...\n",
      "Current loss:  4.156562328338623  | , previous best loss:  4.158572673797607  | saving best model ...\n",
      "Current loss:  4.15455961227417  | , previous best loss:  4.156562328338623  | saving best model ...\n",
      "Current loss:  4.15256404876709  | , previous best loss:  4.15455961227417  | saving best model ...\n",
      "Current loss:  4.150576114654541  | , previous best loss:  4.15256404876709  | saving best model ...\n",
      "Current loss:  4.148595809936523  | , previous best loss:  4.150576114654541  | saving best model ...\n",
      "Current loss:  4.146622180938721  | , previous best loss:  4.148595809936523  | saving best model ...\n",
      "Current loss:  4.144657135009766  | , previous best loss:  4.146622180938721  | saving best model ...\n",
      "Current loss:  4.142698287963867  | , previous best loss:  4.144657135009766  | saving best model ...\n",
      "Current loss:  4.1407470703125  | , previous best loss:  4.142698287963867  | saving best model ...\n",
      "Current loss:  4.138802528381348  | , previous best loss:  4.1407470703125  | saving best model ...\n",
      "Current loss:  4.136865615844727  | , previous best loss:  4.138802528381348  | saving best model ...\n",
      "Current loss:  4.13493537902832  | , previous best loss:  4.136865615844727  | saving best model ...\n",
      "Current loss:  4.133012294769287  | , previous best loss:  4.13493537902832  | saving best model ...\n",
      "Current loss:  4.131095886230469  | , previous best loss:  4.133012294769287  | saving best model ...\n",
      "Current loss:  4.129186630249023  | , previous best loss:  4.131095886230469  | saving best model ...\n",
      "Current loss:  4.127283096313477  | , previous best loss:  4.129186630249023  | saving best model ...\n",
      "Current loss:  4.125387191772461  | , previous best loss:  4.127283096313477  | saving best model ...\n",
      "Current loss:  4.12349796295166  | , previous best loss:  4.125387191772461  | saving best model ...\n",
      "Current loss:  4.121614933013916  | , previous best loss:  4.12349796295166  | saving best model ...\n",
      "Current loss:  4.119738578796387  | , previous best loss:  4.121614933013916  | saving best model ...\n",
      "Current loss:  4.117868423461914  | , previous best loss:  4.119738578796387  | saving best model ...\n",
      "Current loss:  4.115997791290283  | , previous best loss:  4.117868423461914  | saving best model ...\n",
      "Current loss:  4.114126682281494  | , previous best loss:  4.115997791290283  | saving best model ...\n",
      "Current loss:  4.112260818481445  | , previous best loss:  4.114126682281494  | saving best model ...\n",
      "Current loss:  4.110400199890137  | , previous best loss:  4.112260818481445  | saving best model ...\n",
      "Current loss:  4.108545303344727  | , previous best loss:  4.110400199890137  | saving best model ...\n",
      "Current loss:  4.106695175170898  | , previous best loss:  4.108545303344727  | saving best model ...\n",
      "Current loss:  4.104851722717285  | , previous best loss:  4.106695175170898  | saving best model ...\n",
      "Current loss:  4.103013038635254  | , previous best loss:  4.104851722717285  | saving best model ...\n",
      "Current loss:  4.101180076599121  | , previous best loss:  4.103013038635254  | saving best model ...\n",
      "Current loss:  4.0993523597717285  | , previous best loss:  4.101180076599121  | saving best model ...\n",
      "Current loss:  4.097530364990234  | , previous best loss:  4.0993523597717285  | saving best model ...\n",
      "Current loss:  4.095714569091797  | , previous best loss:  4.097530364990234  | saving best model ...\n",
      "Current loss:  4.0939040184021  | , previous best loss:  4.095714569091797  | saving best model ...\n",
      "Current loss:  4.092098712921143  | , previous best loss:  4.0939040184021  | saving best model ...\n",
      "Current loss:  4.090299129486084  | , previous best loss:  4.092098712921143  | saving best model ...\n",
      "Current loss:  4.088505268096924  | , previous best loss:  4.090299129486084  | saving best model ...\n",
      "Current loss:  4.086717128753662  | , previous best loss:  4.088505268096924  | saving best model ...\n",
      "Current loss:  4.084934711456299  | , previous best loss:  4.086717128753662  | saving best model ...\n",
      "Current loss:  4.083157539367676  | , previous best loss:  4.084934711456299  | saving best model ...\n",
      "Current loss:  4.081385612487793  | , previous best loss:  4.083157539367676  | saving best model ...\n",
      "Current loss:  4.07961893081665  | , previous best loss:  4.081385612487793  | saving best model ...\n",
      "Current loss:  4.0778584480285645  | , previous best loss:  4.07961893081665  | saving best model ...\n",
      "Current loss:  4.076103210449219  | , previous best loss:  4.0778584480285645  | saving best model ...\n",
      "Current loss:  4.0743536949157715  | , previous best loss:  4.076103210449219  | saving best model ...\n",
      "Current loss:  4.072608470916748  | , previous best loss:  4.0743536949157715  | saving best model ...\n",
      "Current loss:  4.070869445800781  | , previous best loss:  4.072608470916748  | saving best model ...\n",
      "Current loss:  4.0691351890563965  | , previous best loss:  4.070869445800781  | saving best model ...\n",
      "Current loss:  4.067406177520752  | , previous best loss:  4.0691351890563965  | saving best model ...\n",
      "Current loss:  4.065682411193848  | , previous best loss:  4.067406177520752  | saving best model ...\n",
      "Current loss:  4.063963890075684  | , previous best loss:  4.065682411193848  | saving best model ...\n",
      "Current loss:  4.06225061416626  | , previous best loss:  4.063963890075684  | saving best model ...\n",
      "Current loss:  4.060542106628418  | , previous best loss:  4.06225061416626  | saving best model ...\n",
      "Current loss:  4.058838844299316  | , previous best loss:  4.060542106628418  | saving best model ...\n",
      "Current loss:  4.057140350341797  | , previous best loss:  4.058838844299316  | saving best model ...\n",
      "Current loss:  4.055446624755859  | , previous best loss:  4.057140350341797  | saving best model ...\n",
      "Current loss:  4.053758144378662  | , previous best loss:  4.055446624755859  | saving best model ...\n",
      "Current loss:  4.052074909210205  | , previous best loss:  4.053758144378662  | saving best model ...\n",
      "Current loss:  4.050395965576172  | , previous best loss:  4.052074909210205  | saving best model ...\n",
      "Current loss:  4.048722267150879  | , previous best loss:  4.050395965576172  | saving best model ...\n",
      "Current loss:  4.047053337097168  | , previous best loss:  4.048722267150879  | saving best model ...\n",
      "Current loss:  4.045388221740723  | , previous best loss:  4.047053337097168  | saving best model ...\n",
      "Current loss:  4.043728828430176  | , previous best loss:  4.045388221740723  | saving best model ...\n",
      "Current loss:  4.042074203491211  | , previous best loss:  4.043728828430176  | saving best model ...\n",
      "Current loss:  4.0404229164123535  | , previous best loss:  4.042074203491211  | saving best model ...\n",
      "Current loss:  4.038776874542236  | , previous best loss:  4.0404229164123535  | saving best model ...\n",
      "Current loss:  4.037136077880859  | , previous best loss:  4.038776874542236  | saving best model ...\n",
      "Current loss:  4.035499095916748  | , previous best loss:  4.037136077880859  | saving best model ...\n",
      "Current loss:  4.033866882324219  | , previous best loss:  4.035499095916748  | saving best model ...\n",
      "Current loss:  4.032238960266113  | , previous best loss:  4.033866882324219  | saving best model ...\n",
      "Current loss:  4.030615329742432  | , previous best loss:  4.032238960266113  | saving best model ...\n",
      "Current loss:  4.028995990753174  | , previous best loss:  4.030615329742432  | saving best model ...\n",
      "Current loss:  4.02738094329834  | , previous best loss:  4.028995990753174  | saving best model ...\n",
      "Current loss:  4.025770664215088  | , previous best loss:  4.02738094329834  | saving best model ...\n",
      "Current loss:  4.024164199829102  | , previous best loss:  4.025770664215088  | saving best model ...\n",
      "Current loss:  4.022562026977539  | , previous best loss:  4.024164199829102  | saving best model ...\n",
      "Current loss:  4.020963668823242  | , previous best loss:  4.022562026977539  | saving best model ...\n",
      "Current loss:  4.0193705558776855  | , previous best loss:  4.020963668823242  | saving best model ...\n",
      "Current loss:  4.017780780792236  | , previous best loss:  4.0193705558776855  | saving best model ...\n",
      "Current loss:  4.016195297241211  | , previous best loss:  4.017780780792236  | saving best model ...\n",
      "Current loss:  4.014613628387451  | , previous best loss:  4.016195297241211  | saving best model ...\n",
      "Current loss:  4.013035774230957  | , previous best loss:  4.014613628387451  | saving best model ...\n",
      "Current loss:  4.011462211608887  | , previous best loss:  4.013035774230957  | saving best model ...\n",
      "Current loss:  4.009892463684082  | , previous best loss:  4.011462211608887  | saving best model ...\n",
      "Current loss:  4.008327007293701  | , previous best loss:  4.009892463684082  | saving best model ...\n",
      "Current loss:  4.006765365600586  | , previous best loss:  4.008327007293701  | saving best model ...\n",
      "Current loss:  4.005207061767578  | , previous best loss:  4.006765365600586  | saving best model ...\n",
      "Current loss:  4.003653049468994  | , previous best loss:  4.005207061767578  | saving best model ...\n",
      "Current loss:  4.002101898193359  | , previous best loss:  4.003653049468994  | saving best model ...\n",
      "Current loss:  4.000555515289307  | , previous best loss:  4.002101898193359  | saving best model ...\n",
      "Current loss:  3.9990129470825195  | , previous best loss:  4.000555515289307  | saving best model ...\n",
      "Current loss:  3.9974732398986816  | , previous best loss:  3.9990129470825195  | saving best model ...\n",
      "Current loss:  3.9959380626678467  | , previous best loss:  3.9974732398986816  | saving best model ...\n",
      "Current loss:  3.994405508041382  | , previous best loss:  3.9959380626678467  | saving best model ...\n",
      "Current loss:  3.99287748336792  | , previous best loss:  3.994405508041382  | saving best model ...\n",
      "Current loss:  3.9913523197174072  | , previous best loss:  3.99287748336792  | saving best model ...\n",
      "Current loss:  3.9898314476013184  | , previous best loss:  3.9913523197174072  | saving best model ...\n",
      "Current loss:  3.9883134365081787  | , previous best loss:  3.9898314476013184  | saving best model ...\n",
      "Current loss:  3.9867990016937256  | , previous best loss:  3.9883134365081787  | saving best model ...\n",
      "Current loss:  3.985288381576538  | , previous best loss:  3.9867990016937256  | saving best model ...\n",
      "Current loss:  3.983780860900879  | , previous best loss:  3.985288381576538  | saving best model ...\n",
      "Current loss:  3.982276439666748  | , previous best loss:  3.983780860900879  | saving best model ...\n",
      "Current loss:  3.980776309967041  | , previous best loss:  3.982276439666748  | saving best model ...\n",
      "Current loss:  3.979278802871704  | , previous best loss:  3.980776309967041  | saving best model ...\n",
      "Current loss:  3.9777848720550537  | , previous best loss:  3.979278802871704  | saving best model ...\n",
      "Current loss:  3.9762940406799316  | , previous best loss:  3.9777848720550537  | saving best model ...\n",
      "Current loss:  3.974806785583496  | , previous best loss:  3.9762940406799316  | saving best model ...\n",
      "Current loss:  3.9733221530914307  | , previous best loss:  3.974806785583496  | saving best model ...\n",
      "Current loss:  3.971841335296631  | , previous best loss:  3.9733221530914307  | saving best model ...\n",
      "Current loss:  3.9703638553619385  | , previous best loss:  3.971841335296631  | saving best model ...\n",
      "Current loss:  3.9688892364501953  | , previous best loss:  3.9703638553619385  | saving best model ...\n",
      "Current loss:  3.9674177169799805  | , previous best loss:  3.9688892364501953  | saving best model ...\n",
      "Current loss:  3.965949296951294  | , previous best loss:  3.9674177169799805  | saving best model ...\n",
      "Current loss:  3.9644839763641357  | , previous best loss:  3.965949296951294  | saving best model ...\n",
      "Current loss:  3.963021755218506  | , previous best loss:  3.9644839763641357  | saving best model ...\n",
      "Current loss:  3.961562395095825  | , previous best loss:  3.963021755218506  | saving best model ...\n",
      "Current loss:  3.960106372833252  | , previous best loss:  3.961562395095825  | saving best model ...\n",
      "Current loss:  3.958653211593628  | , previous best loss:  3.960106372833252  | saving best model ...\n",
      "Current loss:  3.9572031497955322  | , previous best loss:  3.958653211593628  | saving best model ...\n",
      "Current loss:  3.9557557106018066  | , previous best loss:  3.9572031497955322  | saving best model ...\n",
      "Current loss:  3.9543116092681885  | , previous best loss:  3.9557557106018066  | saving best model ...\n",
      "Current loss:  3.9528703689575195  | , previous best loss:  3.9543116092681885  | saving best model ...\n",
      "Current loss:  3.9514319896698  | , previous best loss:  3.9528703689575195  | saving best model ...\n",
      "Current loss:  3.9499964714050293  | , previous best loss:  3.9514319896698  | saving best model ...\n",
      "Current loss:  3.94856333732605  | , previous best loss:  3.9499964714050293  | saving best model ...\n",
      "Current loss:  3.947133779525757  | , previous best loss:  3.94856333732605  | saving best model ...\n",
      "Current loss:  3.945706605911255  | , previous best loss:  3.947133779525757  | saving best model ...\n",
      "Current loss:  3.944282054901123  | , previous best loss:  3.945706605911255  | saving best model ...\n",
      "Current loss:  3.9428603649139404  | , previous best loss:  3.944282054901123  | saving best model ...\n",
      "Current loss:  3.941441059112549  | , previous best loss:  3.9428603649139404  | saving best model ...\n",
      "Current loss:  3.9400250911712646  | , previous best loss:  3.941441059112549  | saving best model ...\n",
      "Current loss:  3.9386117458343506  | , previous best loss:  3.9400250911712646  | saving best model ...\n",
      "Current loss:  3.9372012615203857  | , previous best loss:  3.9386117458343506  | saving best model ...\n",
      "Current loss:  3.935792922973633  | , previous best loss:  3.9372012615203857  | saving best model ...\n",
      "Current loss:  3.934387683868408  | , previous best loss:  3.935792922973633  | saving best model ...\n",
      "Current loss:  3.9329843521118164  | , previous best loss:  3.934387683868408  | saving best model ...\n",
      "Current loss:  3.931583881378174  | , previous best loss:  3.9329843521118164  | saving best model ...\n",
      "Current loss:  3.9301869869232178  | , previous best loss:  3.931583881378174  | saving best model ...\n",
      "Current loss:  3.9287915229797363  | , previous best loss:  3.9301869869232178  | saving best model ...\n",
      "Current loss:  3.927398920059204  | , previous best loss:  3.9287915229797363  | saving best model ...\n",
      "Current loss:  3.926009178161621  | , previous best loss:  3.927398920059204  | saving best model ...\n",
      "Current loss:  3.924621343612671  | , previous best loss:  3.926009178161621  | saving best model ...\n",
      "Current loss:  3.923236846923828  | , previous best loss:  3.924621343612671  | saving best model ...\n",
      "Current loss:  3.92185378074646  | , previous best loss:  3.923236846923828  | saving best model ...\n",
      "Current loss:  3.920474052429199  | , previous best loss:  3.92185378074646  | saving best model ...\n",
      "Current loss:  3.9190967082977295  | , previous best loss:  3.920474052429199  | saving best model ...\n",
      "Current loss:  3.9177207946777344  | , previous best loss:  3.9190967082977295  | saving best model ...\n",
      "Current loss:  3.916348457336426  | , previous best loss:  3.9177207946777344  | saving best model ...\n",
      "Current loss:  3.91497802734375  | , previous best loss:  3.916348457336426  | saving best model ...\n",
      "Current loss:  3.9136104583740234  | , previous best loss:  3.91497802734375  | saving best model ...\n",
      "Current loss:  3.9122443199157715  | , previous best loss:  3.9136104583740234  | saving best model ...\n",
      "Current loss:  3.9108810424804688  | , previous best loss:  3.9122443199157715  | saving best model ...\n",
      "Current loss:  3.909520149230957  | , previous best loss:  3.9108810424804688  | saving best model ...\n",
      "Current loss:  3.9081618785858154  | , previous best loss:  3.909520149230957  | saving best model ...\n",
      "Current loss:  3.9068052768707275  | , previous best loss:  3.9081618785858154  | saving best model ...\n",
      "Current loss:  3.905451774597168  | , previous best loss:  3.9068052768707275  | saving best model ...\n",
      "Current loss:  3.904099702835083  | , previous best loss:  3.905451774597168  | saving best model ...\n",
      "Current loss:  3.9027504920959473  | , previous best loss:  3.904099702835083  | saving best model ...\n",
      "Current loss:  3.901402235031128  | , previous best loss:  3.9027504920959473  | saving best model ...\n",
      "Current loss:  3.900057554244995  | , previous best loss:  3.901402235031128  | saving best model ...\n",
      "Current loss:  3.898714780807495  | , previous best loss:  3.900057554244995  | saving best model ...\n",
      "Current loss:  3.8973746299743652  | , previous best loss:  3.898714780807495  | saving best model ...\n",
      "Current loss:  3.89603590965271  | , previous best loss:  3.8973746299743652  | saving best model ...\n",
      "Current loss:  3.894699811935425  | , previous best loss:  3.89603590965271  | saving best model ...\n",
      "Current loss:  3.8933656215667725  | , previous best loss:  3.894699811935425  | saving best model ...\n",
      "Current loss:  3.8920340538024902  | , previous best loss:  3.8933656215667725  | saving best model ...\n",
      "Current loss:  3.8907036781311035  | , previous best loss:  3.8920340538024902  | saving best model ...\n",
      "Current loss:  3.889375925064087  | , previous best loss:  3.8907036781311035  | saving best model ...\n",
      "Current loss:  3.8880505561828613  | , previous best loss:  3.889375925064087  | saving best model ...\n",
      "Current loss:  3.8867266178131104  | , previous best loss:  3.8880505561828613  | saving best model ...\n",
      "Current loss:  3.8854050636291504  | , previous best loss:  3.8867266178131104  | saving best model ...\n",
      "Current loss:  3.884084939956665  | , previous best loss:  3.8854050636291504  | saving best model ...\n",
      "Current loss:  3.882767677307129  | , previous best loss:  3.884084939956665  | saving best model ...\n",
      "Current loss:  3.8814516067504883  | , previous best loss:  3.882767677307129  | saving best model ...\n",
      "Current loss:  3.8801376819610596  | , previous best loss:  3.8814516067504883  | saving best model ...\n",
      "Current loss:  3.878826141357422  | , previous best loss:  3.8801376819610596  | saving best model ...\n",
      "Current loss:  3.877516269683838  | , previous best loss:  3.878826141357422  | saving best model ...\n",
      "Current loss:  3.8762083053588867  | , previous best loss:  3.877516269683838  | saving best model ...\n",
      "Current loss:  3.8749022483825684  | , previous best loss:  3.8762083053588867  | saving best model ...\n",
      "Current loss:  3.873598337173462  | , previous best loss:  3.8749022483825684  | saving best model ...\n",
      "Current loss:  3.8722963333129883  | , previous best loss:  3.873598337173462  | saving best model ...\n",
      "Current loss:  3.8709959983825684  | , previous best loss:  3.8722963333129883  | saving best model ...\n",
      "Current loss:  3.8696980476379395  | , previous best loss:  3.8709959983825684  | saving best model ...\n",
      "Current loss:  3.8684020042419434  | , previous best loss:  3.8696980476379395  | saving best model ...\n",
      "Current loss:  3.8671071529388428  | , previous best loss:  3.8684020042419434  | saving best model ...\n",
      "Current loss:  3.8658154010772705  | , previous best loss:  3.8671071529388428  | saving best model ...\n",
      "Current loss:  3.8645243644714355  | , previous best loss:  3.8658154010772705  | saving best model ...\n",
      "Current loss:  3.8632357120513916  | , previous best loss:  3.8645243644714355  | saving best model ...\n",
      "Current loss:  3.8619492053985596  | , previous best loss:  3.8632357120513916  | saving best model ...\n",
      "Current loss:  3.860664129257202  | , previous best loss:  3.8619492053985596  | saving best model ...\n",
      "Current loss:  3.8593809604644775  | , previous best loss:  3.860664129257202  | saving best model ...\n",
      "Current loss:  3.858100175857544  | , previous best loss:  3.8593809604644775  | saving best model ...\n",
      "Current loss:  3.856820821762085  | , previous best loss:  3.858100175857544  | saving best model ...\n",
      "Current loss:  3.8555424213409424  | , previous best loss:  3.856820821762085  | saving best model ...\n",
      "Current loss:  3.854267120361328  | , previous best loss:  3.8555424213409424  | saving best model ...\n",
      "Current loss:  3.8529932498931885  | , previous best loss:  3.854267120361328  | saving best model ...\n",
      "Current loss:  3.8517212867736816  | , previous best loss:  3.8529932498931885  | saving best model ...\n",
      "Current loss:  3.8504512310028076  | , previous best loss:  3.8517212867736816  | saving best model ...\n",
      "Current loss:  3.84918212890625  | , previous best loss:  3.8504512310028076  | saving best model ...\n",
      "Current loss:  3.8479158878326416  | , previous best loss:  3.84918212890625  | saving best model ...\n",
      "Current loss:  3.8466503620147705  | , previous best loss:  3.8479158878326416  | saving best model ...\n",
      "Current loss:  3.8453872203826904  | , previous best loss:  3.8466503620147705  | saving best model ...\n",
      "Current loss:  3.844125747680664  | , previous best loss:  3.8453872203826904  | saving best model ...\n",
      "Current loss:  3.8428657054901123  | , previous best loss:  3.844125747680664  | saving best model ...\n",
      "Current loss:  3.8416078090667725  | , previous best loss:  3.8428657054901123  | saving best model ...\n",
      "Current loss:  3.8403513431549072  | , previous best loss:  3.8416078090667725  | saving best model ...\n",
      "Current loss:  3.839096784591675  | , previous best loss:  3.8403513431549072  | saving best model ...\n",
      "Current loss:  3.837843656539917  | , previous best loss:  3.839096784591675  | saving best model ...\n",
      "Current loss:  3.836592674255371  | , previous best loss:  3.837843656539917  | saving best model ...\n",
      "Current loss:  3.8353431224823  | , previous best loss:  3.836592674255371  | saving best model ...\n",
      "Current loss:  3.834095001220703  | , previous best loss:  3.8353431224823  | saving best model ...\n",
      "Current loss:  3.8328490257263184  | , previous best loss:  3.834095001220703  | saving best model ...\n",
      "Current loss:  3.831604480743408  | , previous best loss:  3.8328490257263184  | saving best model ...\n",
      "Current loss:  3.8303616046905518  | , previous best loss:  3.831604480743408  | saving best model ...\n",
      "Current loss:  3.82912015914917  | , previous best loss:  3.8303616046905518  | saving best model ...\n",
      "Current loss:  3.827880382537842  | , previous best loss:  3.82912015914917  | saving best model ...\n",
      "Current loss:  3.8266425132751465  | , previous best loss:  3.827880382537842  | saving best model ...\n",
      "Current loss:  3.8254058361053467  | , previous best loss:  3.8266425132751465  | saving best model ...\n",
      "Current loss:  3.824171543121338  | , previous best loss:  3.8254058361053467  | saving best model ...\n",
      "Current loss:  3.8229386806488037  | , previous best loss:  3.824171543121338  | saving best model ...\n",
      "Current loss:  3.821706771850586  | , previous best loss:  3.8229386806488037  | saving best model ...\n",
      "Current loss:  3.820476770401001  | , previous best loss:  3.821706771850586  | saving best model ...\n",
      "Current loss:  3.819248676300049  | , previous best loss:  3.820476770401001  | saving best model ...\n",
      "Current loss:  3.818021774291992  | , previous best loss:  3.819248676300049  | saving best model ...\n",
      "Current loss:  3.81679630279541  | , previous best loss:  3.818021774291992  | saving best model ...\n",
      "Current loss:  3.815572738647461  | , previous best loss:  3.81679630279541  | saving best model ...\n",
      "Current loss:  3.8143508434295654  | , previous best loss:  3.815572738647461  | saving best model ...\n",
      "Current loss:  3.8131301403045654  | , previous best loss:  3.8143508434295654  | saving best model ...\n",
      "Current loss:  3.8119113445281982  | , previous best loss:  3.8131301403045654  | saving best model ...\n",
      "Current loss:  3.8106937408447266  | , previous best loss:  3.8119113445281982  | saving best model ...\n",
      "Current loss:  3.8094780445098877  | , previous best loss:  3.8106937408447266  | saving best model ...\n",
      "Current loss:  3.8082633018493652  | , previous best loss:  3.8094780445098877  | saving best model ...\n",
      "Current loss:  3.8070504665374756  | , previous best loss:  3.8082633018493652  | saving best model ...\n",
      "Current loss:  3.8058390617370605  | , previous best loss:  3.8070504665374756  | saving best model ...\n",
      "Current loss:  3.804629325866699  | , previous best loss:  3.8058390617370605  | saving best model ...\n",
      "Current loss:  3.8034212589263916  | , previous best loss:  3.804629325866699  | saving best model ...\n",
      "Current loss:  3.8022141456604004  | , previous best loss:  3.8034212589263916  | saving best model ...\n",
      "Current loss:  3.801008939743042  | , previous best loss:  3.8022141456604004  | saving best model ...\n",
      "Current loss:  3.7998046875  | , previous best loss:  3.801008939743042  | saving best model ...\n",
      "Current loss:  3.798602819442749  | , previous best loss:  3.7998046875  | saving best model ...\n",
      "Current loss:  3.7974014282226562  | , previous best loss:  3.798602819442749  | saving best model ...\n",
      "Current loss:  3.7962019443511963  | , previous best loss:  3.7974014282226562  | saving best model ...\n",
      "Current loss:  3.795003890991211  | , previous best loss:  3.7962019443511963  | saving best model ...\n",
      "Current loss:  3.793807029724121  | , previous best loss:  3.795003890991211  | saving best model ...\n",
      "Current loss:  3.792612314224243  | , previous best loss:  3.793807029724121  | saving best model ...\n",
      "Current loss:  3.7914185523986816  | , previous best loss:  3.792612314224243  | saving best model ...\n",
      "Current loss:  3.7902262210845947  | , previous best loss:  3.7914185523986816  | saving best model ...\n",
      "Current loss:  3.7890353202819824  | , previous best loss:  3.7902262210845947  | saving best model ...\n",
      "Current loss:  3.7878458499908447  | , previous best loss:  3.7890353202819824  | saving best model ...\n",
      "Current loss:  3.7866580486297607  | , previous best loss:  3.7878458499908447  | saving best model ...\n",
      "Current loss:  3.785471200942993  | , previous best loss:  3.7866580486297607  | saving best model ...\n",
      "Current loss:  3.7842860221862793  | , previous best loss:  3.785471200942993  | saving best model ...\n",
      "Current loss:  3.78310227394104  | , previous best loss:  3.7842860221862793  | saving best model ...\n",
      "Current loss:  3.7819199562072754  | , previous best loss:  3.78310227394104  | saving best model ...\n",
      "Current loss:  3.780738592147827  | , previous best loss:  3.7819199562072754  | saving best model ...\n",
      "Current loss:  3.7795591354370117  | , previous best loss:  3.780738592147827  | saving best model ...\n",
      "Current loss:  3.778380870819092  | , previous best loss:  3.7795591354370117  | saving best model ...\n",
      "Current loss:  3.7772040367126465  | , previous best loss:  3.778380870819092  | saving best model ...\n",
      "Current loss:  3.776028633117676  | , previous best loss:  3.7772040367126465  | saving best model ...\n",
      "Current loss:  3.7748544216156006  | , previous best loss:  3.776028633117676  | saving best model ...\n",
      "Current loss:  3.773681640625  | , previous best loss:  3.7748544216156006  | saving best model ...\n",
      "Current loss:  3.772509813308716  | , previous best loss:  3.773681640625  | saving best model ...\n",
      "Current loss:  3.7713398933410645  | , previous best loss:  3.772509813308716  | saving best model ...\n",
      "Current loss:  3.7701711654663086  | , previous best loss:  3.7713398933410645  | saving best model ...\n",
      "Current loss:  3.7690038681030273  | , previous best loss:  3.7701711654663086  | saving best model ...\n",
      "Current loss:  3.7678380012512207  | , previous best loss:  3.7690038681030273  | saving best model ...\n",
      "Current loss:  3.7666726112365723  | , previous best loss:  3.7678380012512207  | saving best model ...\n",
      "Current loss:  3.765509605407715  | , previous best loss:  3.7666726112365723  | saving best model ...\n",
      "Current loss:  3.7643473148345947  | , previous best loss:  3.765509605407715  | saving best model ...\n",
      "Current loss:  3.7631866931915283  | , previous best loss:  3.7643473148345947  | saving best model ...\n",
      "Current loss:  3.7620270252227783  | , previous best loss:  3.7631866931915283  | saving best model ...\n",
      "Current loss:  3.760868787765503  | , previous best loss:  3.7620270252227783  | saving best model ...\n",
      "Current loss:  3.759711980819702  | , previous best loss:  3.760868787765503  | saving best model ...\n",
      "Current loss:  3.758556604385376  | , previous best loss:  3.759711980819702  | saving best model ...\n",
      "Current loss:  3.7574024200439453  | , previous best loss:  3.758556604385376  | saving best model ...\n",
      "Current loss:  3.756248950958252  | , previous best loss:  3.7574024200439453  | saving best model ...\n",
      "Current loss:  3.755096912384033  | , previous best loss:  3.756248950958252  | saving best model ...\n",
      "Current loss:  3.7539467811584473  | , previous best loss:  3.755096912384033  | saving best model ...\n",
      "Current loss:  3.7527973651885986  | , previous best loss:  3.7539467811584473  | saving best model ...\n",
      "Current loss:  3.7516496181488037  | , previous best loss:  3.7527973651885986  | saving best model ...\n",
      "Current loss:  3.750502824783325  | , previous best loss:  3.7516496181488037  | saving best model ...\n",
      "Current loss:  3.7493574619293213  | , previous best loss:  3.750502824783325  | saving best model ...\n",
      "Current loss:  3.748213768005371  | , previous best loss:  3.7493574619293213  | saving best model ...\n",
      "Current loss:  3.747070789337158  | , previous best loss:  3.748213768005371  | saving best model ...\n",
      "Current loss:  3.745929002761841  | , previous best loss:  3.747070789337158  | saving best model ...\n",
      "Current loss:  3.744788885116577  | , previous best loss:  3.745929002761841  | saving best model ...\n",
      "Current loss:  3.743649959564209  | , previous best loss:  3.744788885116577  | saving best model ...\n",
      "Current loss:  3.74251127243042  | , previous best loss:  3.743649959564209  | saving best model ...\n",
      "Current loss:  3.741374969482422  | , previous best loss:  3.74251127243042  | saving best model ...\n",
      "Current loss:  3.7402400970458984  | , previous best loss:  3.741374969482422  | saving best model ...\n",
      "Current loss:  3.739105463027954  | , previous best loss:  3.7402400970458984  | saving best model ...\n",
      "Current loss:  3.7379722595214844  | , previous best loss:  3.739105463027954  | saving best model ...\n",
      "Current loss:  3.7368404865264893  | , previous best loss:  3.7379722595214844  | saving best model ...\n",
      "Current loss:  3.7357101440429688  | , previous best loss:  3.7368404865264893  | saving best model ...\n",
      "Current loss:  3.7345802783966064  | , previous best loss:  3.7357101440429688  | saving best model ...\n",
      "Current loss:  3.733452558517456  | , previous best loss:  3.7345802783966064  | saving best model ...\n",
      "Current loss:  3.732325553894043  | , previous best loss:  3.733452558517456  | saving best model ...\n",
      "Current loss:  3.7311997413635254  | , previous best loss:  3.732325553894043  | saving best model ...\n",
      "Current loss:  3.7300751209259033  | , previous best loss:  3.7311997413635254  | saving best model ...\n",
      "Current loss:  3.7289514541625977  | , previous best loss:  3.7300751209259033  | saving best model ...\n",
      "Current loss:  3.727829694747925  | , previous best loss:  3.7289514541625977  | saving best model ...\n",
      "Current loss:  3.72670841217041  | , previous best loss:  3.727829694747925  | saving best model ...\n",
      "Current loss:  3.725588798522949  | , previous best loss:  3.72670841217041  | saving best model ...\n",
      "Current loss:  3.7244699001312256  | , previous best loss:  3.725588798522949  | saving best model ...\n",
      "Current loss:  3.7233524322509766  | , previous best loss:  3.7244699001312256  | saving best model ...\n",
      "Current loss:  3.722236394882202  | , previous best loss:  3.7233524322509766  | saving best model ...\n",
      "Current loss:  3.7211215496063232  | , previous best loss:  3.722236394882202  | saving best model ...\n",
      "Current loss:  3.7200074195861816  | , previous best loss:  3.7211215496063232  | saving best model ...\n",
      "Current loss:  3.7188942432403564  | , previous best loss:  3.7200074195861816  | saving best model ...\n",
      "Current loss:  3.717782735824585  | , previous best loss:  3.7188942432403564  | saving best model ...\n",
      "Current loss:  3.716672420501709  | , previous best loss:  3.717782735824585  | saving best model ...\n",
      "Current loss:  3.715562582015991  | , previous best loss:  3.716672420501709  | saving best model ...\n",
      "Current loss:  3.7144546508789062  | , previous best loss:  3.715562582015991  | saving best model ...\n",
      "Current loss:  3.7133476734161377  | , previous best loss:  3.7144546508789062  | saving best model ...\n",
      "Current loss:  3.7122416496276855  | , previous best loss:  3.7133476734161377  | saving best model ...\n",
      "Current loss:  3.711136817932129  | , previous best loss:  3.7122416496276855  | saving best model ...\n",
      "Current loss:  3.710033416748047  | , previous best loss:  3.711136817932129  | saving best model ...\n",
      "Current loss:  3.708930730819702  | , previous best loss:  3.710033416748047  | saving best model ...\n",
      "Current loss:  3.707828998565674  | , previous best loss:  3.708930730819702  | saving best model ...\n",
      "Current loss:  3.70672869682312  | , previous best loss:  3.707828998565674  | saving best model ...\n",
      "Current loss:  3.705629825592041  | , previous best loss:  3.70672869682312  | saving best model ...\n",
      "Current loss:  3.70453143119812  | , previous best loss:  3.705629825592041  | saving best model ...\n",
      "Current loss:  3.703434467315674  | , previous best loss:  3.70453143119812  | saving best model ...\n",
      "Current loss:  3.702338933944702  | , previous best loss:  3.703434467315674  | saving best model ...\n",
      "Current loss:  3.7012441158294678  | , previous best loss:  3.702338933944702  | saving best model ...\n",
      "Current loss:  3.700150966644287  | , previous best loss:  3.7012441158294678  | saving best model ...\n",
      "Current loss:  3.6990580558776855  | , previous best loss:  3.700150966644287  | saving best model ...\n",
      "Current loss:  3.697967052459717  | , previous best loss:  3.6990580558776855  | saving best model ...\n",
      "Current loss:  3.6968765258789062  | , previous best loss:  3.697967052459717  | saving best model ...\n",
      "Current loss:  3.6957874298095703  | , previous best loss:  3.6968765258789062  | saving best model ...\n",
      "Current loss:  3.6946990489959717  | , previous best loss:  3.6957874298095703  | saving best model ...\n",
      "Current loss:  3.6936118602752686  | , previous best loss:  3.6946990489959717  | saving best model ...\n",
      "Current loss:  3.69252610206604  | , previous best loss:  3.6936118602752686  | saving best model ...\n",
      "Current loss:  3.6914408206939697  | , previous best loss:  3.69252610206604  | saving best model ...\n",
      "Current loss:  3.690357208251953  | , previous best loss:  3.6914408206939697  | saving best model ...\n",
      "Current loss:  3.689274311065674  | , previous best loss:  3.690357208251953  | saving best model ...\n",
      "Current loss:  3.688192844390869  | , previous best loss:  3.689274311065674  | saving best model ...\n",
      "Current loss:  3.687112331390381  | , previous best loss:  3.688192844390869  | saving best model ...\n",
      "Current loss:  3.686032772064209  | , previous best loss:  3.687112331390381  | saving best model ...\n",
      "Current loss:  3.6849544048309326  | , previous best loss:  3.686032772064209  | saving best model ...\n",
      "Current loss:  3.6838767528533936  | , previous best loss:  3.6849544048309326  | saving best model ...\n",
      "Current loss:  3.682800531387329  | , previous best loss:  3.6838767528533936  | saving best model ...\n",
      "Current loss:  3.681725263595581  | , previous best loss:  3.682800531387329  | saving best model ...\n",
      "Current loss:  3.6806507110595703  | , previous best loss:  3.681725263595581  | saving best model ...\n",
      "Current loss:  3.6795778274536133  | , previous best loss:  3.6806507110595703  | saving best model ...\n",
      "Current loss:  3.6785054206848145  | , previous best loss:  3.6795778274536133  | saving best model ...\n",
      "Current loss:  3.6774346828460693  | , previous best loss:  3.6785054206848145  | saving best model ...\n",
      "Current loss:  3.6763641834259033  | , previous best loss:  3.6774346828460693  | saving best model ...\n",
      "Current loss:  3.675295352935791  | , previous best loss:  3.6763641834259033  | saving best model ...\n",
      "Current loss:  3.674227476119995  | , previous best loss:  3.675295352935791  | saving best model ...\n",
      "Current loss:  3.6731605529785156  | , previous best loss:  3.674227476119995  | saving best model ...\n",
      "Current loss:  3.6720943450927734  | , previous best loss:  3.6731605529785156  | saving best model ...\n",
      "Current loss:  3.671029806137085  | , previous best loss:  3.6720943450927734  | saving best model ...\n",
      "Current loss:  3.669965982437134  | , previous best loss:  3.671029806137085  | saving best model ...\n",
      "Current loss:  3.66890287399292  | , previous best loss:  3.669965982437134  | saving best model ...\n",
      "Current loss:  3.6678409576416016  | , previous best loss:  3.66890287399292  | saving best model ...\n",
      "Current loss:  3.6667802333831787  | , previous best loss:  3.6678409576416016  | saving best model ...\n",
      "Current loss:  3.665720224380493  | , previous best loss:  3.6667802333831787  | saving best model ...\n",
      "Current loss:  3.6646618843078613  | , previous best loss:  3.665720224380493  | saving best model ...\n",
      "Current loss:  3.6636040210723877  | , previous best loss:  3.6646618843078613  | saving best model ...\n",
      "Current loss:  3.6625473499298096  | , previous best loss:  3.6636040210723877  | saving best model ...\n",
      "Current loss:  3.661491632461548  | , previous best loss:  3.6625473499298096  | saving best model ...\n",
      "Current loss:  3.6604371070861816  | , previous best loss:  3.661491632461548  | saving best model ...\n",
      "Current loss:  3.6593828201293945  | , previous best loss:  3.6604371070861816  | saving best model ...\n",
      "Current loss:  3.658329963684082  | , previous best loss:  3.6593828201293945  | saving best model ...\n",
      "Current loss:  3.6572787761688232  | , previous best loss:  3.658329963684082  | saving best model ...\n",
      "Current loss:  3.6562275886535645  | , previous best loss:  3.6572787761688232  | saving best model ...\n",
      "Current loss:  3.6551778316497803  | , previous best loss:  3.6562275886535645  | saving best model ...\n",
      "Current loss:  3.6541287899017334  | , previous best loss:  3.6551778316497803  | saving best model ...\n",
      "Current loss:  3.6530814170837402  | , previous best loss:  3.6541287899017334  | saving best model ...\n",
      "Current loss:  3.6520347595214844  | , previous best loss:  3.6530814170837402  | saving best model ...\n",
      "Current loss:  3.6509885787963867  | , previous best loss:  3.6520347595214844  | saving best model ...\n",
      "Current loss:  3.6499438285827637  | , previous best loss:  3.6509885787963867  | saving best model ...\n",
      "Current loss:  3.648899793624878  | , previous best loss:  3.6499438285827637  | saving best model ...\n",
      "Current loss:  3.647857427597046  | , previous best loss:  3.648899793624878  | saving best model ...\n",
      "Current loss:  3.646815299987793  | , previous best loss:  3.647857427597046  | saving best model ...\n",
      "Current loss:  3.6457743644714355  | , previous best loss:  3.646815299987793  | saving best model ...\n",
      "Current loss:  3.6447341442108154  | , previous best loss:  3.6457743644714355  | saving best model ...\n",
      "Current loss:  3.643695116043091  | , previous best loss:  3.6447341442108154  | saving best model ...\n",
      "Current loss:  3.6426572799682617  | , previous best loss:  3.643695116043091  | saving best model ...\n",
      "Current loss:  3.64162015914917  | , previous best loss:  3.6426572799682617  | saving best model ...\n",
      "Current loss:  3.6405839920043945  | , previous best loss:  3.64162015914917  | saving best model ...\n",
      "Current loss:  3.6395490169525146  | , previous best loss:  3.6405839920043945  | saving best model ...\n",
      "Current loss:  3.6385152339935303  | , previous best loss:  3.6395490169525146  | saving best model ...\n",
      "Current loss:  3.637481451034546  | , previous best loss:  3.6385152339935303  | saving best model ...\n",
      "Current loss:  3.6364493370056152  | , previous best loss:  3.637481451034546  | saving best model ...\n",
      "Current loss:  3.6354176998138428  | , previous best loss:  3.6364493370056152  | saving best model ...\n",
      "Current loss:  3.634387254714966  | , previous best loss:  3.6354176998138428  | saving best model ...\n",
      "Current loss:  3.6333577632904053  | , previous best loss:  3.634387254714966  | saving best model ...\n",
      "Current loss:  3.6323297023773193  | , previous best loss:  3.6333577632904053  | saving best model ...\n",
      "Current loss:  3.6313016414642334  | , previous best loss:  3.6323297023773193  | saving best model ...\n",
      "Current loss:  3.6302754878997803  | , previous best loss:  3.6313016414642334  | saving best model ...\n",
      "Current loss:  3.6292495727539062  | , previous best loss:  3.6302754878997803  | saving best model ...\n",
      "Current loss:  3.6282246112823486  | , previous best loss:  3.6292495727539062  | saving best model ...\n",
      "Current loss:  3.6272008419036865  | , previous best loss:  3.6282246112823486  | saving best model ...\n",
      "Current loss:  3.6261777877807617  | , previous best loss:  3.6272008419036865  | saving best model ...\n",
      "Current loss:  3.6251556873321533  | , previous best loss:  3.6261777877807617  | saving best model ...\n",
      "Current loss:  3.6241350173950195  | , previous best loss:  3.6251556873321533  | saving best model ...\n",
      "Current loss:  3.623114824295044  | , previous best loss:  3.6241350173950195  | saving best model ...\n",
      "Current loss:  3.6220955848693848  | , previous best loss:  3.623114824295044  | saving best model ...\n",
      "Current loss:  3.621077060699463  | , previous best loss:  3.6220955848693848  | saving best model ...\n",
      "Current loss:  3.6200597286224365  | , previous best loss:  3.621077060699463  | saving best model ...\n",
      "Current loss:  3.6190435886383057  | , previous best loss:  3.6200597286224365  | saving best model ...\n",
      "Current loss:  3.618028163909912  | , previous best loss:  3.6190435886383057  | saving best model ...\n",
      "Current loss:  3.617013454437256  | , previous best loss:  3.618028163909912  | saving best model ...\n",
      "Current loss:  3.615999460220337  | , previous best loss:  3.617013454437256  | saving best model ...\n",
      "Current loss:  3.6149871349334717  | , previous best loss:  3.615999460220337  | saving best model ...\n",
      "Current loss:  3.6139748096466064  | , previous best loss:  3.6149871349334717  | saving best model ...\n",
      "Current loss:  3.612964153289795  | , previous best loss:  3.6139748096466064  | saving best model ...\n",
      "Current loss:  3.6119539737701416  | , previous best loss:  3.612964153289795  | saving best model ...\n",
      "Current loss:  3.6109447479248047  | , previous best loss:  3.6119539737701416  | saving best model ...\n",
      "Current loss:  3.609936237335205  | , previous best loss:  3.6109447479248047  | saving best model ...\n",
      "Current loss:  3.608928918838501  | , previous best loss:  3.609936237335205  | saving best model ...\n",
      "Current loss:  3.6079225540161133  | , previous best loss:  3.608928918838501  | saving best model ...\n",
      "Current loss:  3.606916904449463  | , previous best loss:  3.6079225540161133  | saving best model ...\n",
      "Current loss:  3.605912446975708  | , previous best loss:  3.606916904449463  | saving best model ...\n",
      "Current loss:  3.6049082279205322  | , previous best loss:  3.605912446975708  | saving best model ...\n",
      "Current loss:  3.603905200958252  | , previous best loss:  3.6049082279205322  | saving best model ...\n",
      "Current loss:  3.602903366088867  | , previous best loss:  3.603905200958252  | saving best model ...\n",
      "Current loss:  3.601902484893799  | , previous best loss:  3.602903366088867  | saving best model ...\n",
      "Current loss:  3.6009020805358887  | , previous best loss:  3.601902484893799  | saving best model ...\n",
      "Current loss:  3.599902868270874  | , previous best loss:  3.6009020805358887  | saving best model ...\n",
      "Current loss:  3.5989043712615967  | , previous best loss:  3.599902868270874  | saving best model ...\n",
      "Current loss:  3.5979065895080566  | , previous best loss:  3.5989043712615967  | saving best model ...\n",
      "Current loss:  3.596909761428833  | , previous best loss:  3.5979065895080566  | saving best model ...\n",
      "Current loss:  3.595913887023926  | , previous best loss:  3.596909761428833  | saving best model ...\n",
      "Current loss:  3.594919204711914  | , previous best loss:  3.595913887023926  | saving best model ...\n",
      "Current loss:  3.5939249992370605  | , previous best loss:  3.594919204711914  | saving best model ...\n",
      "Current loss:  3.5929317474365234  | , previous best loss:  3.5939249992370605  | saving best model ...\n",
      "Current loss:  3.5919389724731445  | , previous best loss:  3.5929317474365234  | saving best model ...\n",
      "Current loss:  3.5909476280212402  | , previous best loss:  3.5919389724731445  | saving best model ...\n",
      "Current loss:  3.5899569988250732  | , previous best loss:  3.5909476280212402  | saving best model ...\n",
      "Current loss:  3.5889670848846436  | , previous best loss:  3.5899569988250732  | saving best model ...\n",
      "Current loss:  3.5879783630371094  | , previous best loss:  3.5889670848846436  | saving best model ...\n",
      "Current loss:  3.5869898796081543  | , previous best loss:  3.5879783630371094  | saving best model ...\n",
      "Current loss:  3.586002826690674  | , previous best loss:  3.5869898796081543  | saving best model ...\n",
      "Current loss:  3.5850164890289307  | , previous best loss:  3.586002826690674  | saving best model ...\n",
      "Current loss:  3.584030866622925  | , previous best loss:  3.5850164890289307  | saving best model ...\n",
      "Current loss:  3.5830466747283936  | , previous best loss:  3.584030866622925  | saving best model ...\n",
      "Current loss:  3.5820629596710205  | , previous best loss:  3.5830466747283936  | saving best model ...\n",
      "Current loss:  3.5810794830322266  | , previous best loss:  3.5820629596710205  | saving best model ...\n",
      "Current loss:  3.580097198486328  | , previous best loss:  3.5810794830322266  | saving best model ...\n",
      "Current loss:  3.5791163444519043  | , previous best loss:  3.580097198486328  | saving best model ...\n",
      "Current loss:  3.5781357288360596  | , previous best loss:  3.5791163444519043  | saving best model ...\n",
      "Current loss:  3.5771563053131104  | , previous best loss:  3.5781357288360596  | saving best model ...\n",
      "Current loss:  3.5761773586273193  | , previous best loss:  3.5771563053131104  | saving best model ...\n",
      "Current loss:  3.575199604034424  | , previous best loss:  3.5761773586273193  | saving best model ...\n",
      "Current loss:  3.5742223262786865  | , previous best loss:  3.575199604034424  | saving best model ...\n",
      "Current loss:  3.5732462406158447  | , previous best loss:  3.5742223262786865  | saving best model ...\n",
      "Current loss:  3.5722711086273193  | , previous best loss:  3.5732462406158447  | saving best model ...\n",
      "Current loss:  3.571296453475952  | , previous best loss:  3.5722711086273193  | saving best model ...\n",
      "Current loss:  3.5703227519989014  | , previous best loss:  3.571296453475952  | saving best model ...\n",
      "Current loss:  3.569349765777588  | , previous best loss:  3.5703227519989014  | saving best model ...\n",
      "Current loss:  3.56837797164917  | , previous best loss:  3.569349765777588  | saving best model ...\n",
      "Current loss:  3.567406177520752  | , previous best loss:  3.56837797164917  | saving best model ...\n",
      "Current loss:  3.5664360523223877  | , previous best loss:  3.567406177520752  | saving best model ...\n",
      "Current loss:  3.5654664039611816  | , previous best loss:  3.5664360523223877  | saving best model ...\n",
      "Current loss:  3.564497470855713  | , previous best loss:  3.5654664039611816  | saving best model ...\n",
      "Current loss:  3.5635294914245605  | , previous best loss:  3.564497470855713  | saving best model ...\n",
      "Current loss:  3.5625627040863037  | , previous best loss:  3.5635294914245605  | saving best model ...\n",
      "Current loss:  3.561596393585205  | , previous best loss:  3.5625627040863037  | saving best model ...\n",
      "Current loss:  3.5606307983398438  | , previous best loss:  3.561596393585205  | saving best model ...\n",
      "Current loss:  3.5596659183502197  | , previous best loss:  3.5606307983398438  | saving best model ...\n",
      "Current loss:  3.5587024688720703  | , previous best loss:  3.5596659183502197  | saving best model ...\n",
      "Current loss:  3.5577392578125  | , previous best loss:  3.5587024688720703  | saving best model ...\n",
      "Current loss:  3.556776762008667  | , previous best loss:  3.5577392578125  | saving best model ...\n",
      "Current loss:  3.5558156967163086  | , previous best loss:  3.556776762008667  | saving best model ...\n",
      "Current loss:  3.5548551082611084  | , previous best loss:  3.5558156967163086  | saving best model ...\n",
      "Current loss:  3.5538952350616455  | , previous best loss:  3.5548551082611084  | saving best model ...\n",
      "Current loss:  3.55293607711792  | , previous best loss:  3.5538952350616455  | saving best model ...\n",
      "Current loss:  3.55197811126709  | , previous best loss:  3.55293607711792  | saving best model ...\n",
      "Current loss:  3.551021099090576  | , previous best loss:  3.55197811126709  | saving best model ...\n",
      "Current loss:  3.5500643253326416  | , previous best loss:  3.551021099090576  | saving best model ...\n",
      "Current loss:  3.5491085052490234  | , previous best loss:  3.5500643253326416  | saving best model ...\n",
      "Current loss:  3.5481534004211426  | , previous best loss:  3.5491085052490234  | saving best model ...\n",
      "Current loss:  3.547199010848999  | , previous best loss:  3.5481534004211426  | saving best model ...\n",
      "Current loss:  3.546245574951172  | , previous best loss:  3.547199010848999  | saving best model ...\n",
      "Current loss:  3.5452933311462402  | , previous best loss:  3.546245574951172  | saving best model ...\n",
      "Current loss:  3.5443413257598877  | , previous best loss:  3.5452933311462402  | saving best model ...\n",
      "Current loss:  3.5433905124664307  | , previous best loss:  3.5443413257598877  | saving best model ...\n",
      "Current loss:  3.5424399375915527  | , previous best loss:  3.5433905124664307  | saving best model ...\n",
      "Current loss:  3.5414907932281494  | , previous best loss:  3.5424399375915527  | saving best model ...\n",
      "Current loss:  3.540541887283325  | , previous best loss:  3.5414907932281494  | saving best model ...\n",
      "Current loss:  3.5395941734313965  | , previous best loss:  3.540541887283325  | saving best model ...\n",
      "Current loss:  3.538647413253784  | , previous best loss:  3.5395941734313965  | saving best model ...\n",
      "Current loss:  3.537700653076172  | , previous best loss:  3.538647413253784  | saving best model ...\n",
      "Current loss:  3.536755323410034  | , previous best loss:  3.537700653076172  | saving best model ...\n",
      "Current loss:  3.5358104705810547  | , previous best loss:  3.536755323410034  | saving best model ...\n",
      "Current loss:  3.5348668098449707  | , previous best loss:  3.5358104705810547  | saving best model ...\n",
      "Current loss:  3.533923625946045  | , previous best loss:  3.5348668098449707  | saving best model ...\n",
      "Current loss:  3.5329811573028564  | , previous best loss:  3.533923625946045  | saving best model ...\n",
      "Current loss:  3.5320394039154053  | , previous best loss:  3.5329811573028564  | saving best model ...\n",
      "Current loss:  3.5310986042022705  | , previous best loss:  3.5320394039154053  | saving best model ...\n",
      "Current loss:  3.530158519744873  | , previous best loss:  3.5310986042022705  | saving best model ...\n",
      "Current loss:  3.529219150543213  | , previous best loss:  3.530158519744873  | saving best model ...\n",
      "Current loss:  3.528280735015869  | , previous best loss:  3.529219150543213  | saving best model ...\n",
      "Current loss:  3.5273430347442627  | , previous best loss:  3.528280735015869  | saving best model ...\n",
      "Current loss:  3.5264062881469727  | , previous best loss:  3.5273430347442627  | saving best model ...\n",
      "Current loss:  3.525470018386841  | , previous best loss:  3.5264062881469727  | saving best model ...\n",
      "Current loss:  3.5245347023010254  | , previous best loss:  3.525470018386841  | saving best model ...\n",
      "Current loss:  3.523599863052368  | , previous best loss:  3.5245347023010254  | saving best model ...\n",
      "Current loss:  3.5226662158966064  | , previous best loss:  3.523599863052368  | saving best model ...\n",
      "Current loss:  3.521732807159424  | , previous best loss:  3.5226662158966064  | saving best model ...\n",
      "Current loss:  3.5208001136779785  | , previous best loss:  3.521732807159424  | saving best model ...\n",
      "Current loss:  3.519868850708008  | , previous best loss:  3.5208001136779785  | saving best model ...\n",
      "Current loss:  3.518937826156616  | , previous best loss:  3.519868850708008  | saving best model ...\n",
      "Current loss:  3.51800799369812  | , previous best loss:  3.518937826156616  | saving best model ...\n",
      "Current loss:  3.517078399658203  | , previous best loss:  3.51800799369812  | saving best model ...\n",
      "Current loss:  3.5161499977111816  | , previous best loss:  3.517078399658203  | saving best model ...\n",
      "Current loss:  3.5152220726013184  | , previous best loss:  3.5161499977111816  | saving best model ...\n",
      "Current loss:  3.5142946243286133  | , previous best loss:  3.5152220726013184  | saving best model ...\n",
      "Current loss:  3.5133683681488037  | , previous best loss:  3.5142946243286133  | saving best model ...\n",
      "Current loss:  3.5124428272247314  | , previous best loss:  3.5133683681488037  | saving best model ...\n",
      "Current loss:  3.5115182399749756  | , previous best loss:  3.5124428272247314  | saving best model ...\n",
      "Current loss:  3.510593891143799  | , previous best loss:  3.5115182399749756  | saving best model ...\n",
      "Current loss:  3.5096707344055176  | , previous best loss:  3.510593891143799  | saving best model ...\n",
      "Current loss:  3.5087480545043945  | , previous best loss:  3.5096707344055176  | saving best model ...\n",
      "Current loss:  3.507826328277588  | , previous best loss:  3.5087480545043945  | saving best model ...\n",
      "Current loss:  3.5069050788879395  | , previous best loss:  3.507826328277588  | saving best model ...\n",
      "Current loss:  3.5059847831726074  | , previous best loss:  3.5069050788879395  | saving best model ...\n",
      "Current loss:  3.505065441131592  | , previous best loss:  3.5059847831726074  | saving best model ...\n",
      "Current loss:  3.504146099090576  | , previous best loss:  3.505065441131592  | saving best model ...\n",
      "Current loss:  3.503228187561035  | , previous best loss:  3.504146099090576  | saving best model ...\n",
      "Current loss:  3.5023109912872314  | , previous best loss:  3.503228187561035  | saving best model ...\n",
      "Current loss:  3.501394748687744  | , previous best loss:  3.5023109912872314  | saving best model ...\n",
      "Current loss:  3.500478982925415  | , previous best loss:  3.501394748687744  | saving best model ...\n",
      "Current loss:  3.4995641708374023  | , previous best loss:  3.500478982925415  | saving best model ...\n",
      "Current loss:  3.498650312423706  | , previous best loss:  3.4995641708374023  | saving best model ...\n",
      "Current loss:  3.497736930847168  | , previous best loss:  3.498650312423706  | saving best model ...\n",
      "Current loss:  3.496824264526367  | , previous best loss:  3.497736930847168  | saving best model ...\n",
      "Current loss:  3.4959123134613037  | , previous best loss:  3.496824264526367  | saving best model ...\n",
      "Current loss:  3.4950015544891357  | , previous best loss:  3.4959123134613037  | saving best model ...\n",
      "Current loss:  3.494091033935547  | , previous best loss:  3.4950015544891357  | saving best model ...\n",
      "Current loss:  3.4931814670562744  | , previous best loss:  3.494091033935547  | saving best model ...\n",
      "Current loss:  3.4922726154327393  | , previous best loss:  3.4931814670562744  | saving best model ...\n",
      "Current loss:  3.4913642406463623  | , previous best loss:  3.4922726154327393  | saving best model ...\n",
      "Current loss:  3.4904568195343018  | , previous best loss:  3.4913642406463623  | saving best model ...\n",
      "Current loss:  3.4895503520965576  | , previous best loss:  3.4904568195343018  | saving best model ...\n",
      "Current loss:  3.4886443614959717  | , previous best loss:  3.4895503520965576  | saving best model ...\n",
      "Current loss:  3.487739324569702  | , previous best loss:  3.4886443614959717  | saving best model ...\n",
      "Current loss:  3.486834764480591  | , previous best loss:  3.487739324569702  | saving best model ...\n",
      "Current loss:  3.485931158065796  | , previous best loss:  3.486834764480591  | saving best model ...\n",
      "Current loss:  3.485028028488159  | , previous best loss:  3.485931158065796  | saving best model ...\n",
      "Current loss:  3.4841253757476807  | , previous best loss:  3.485028028488159  | saving best model ...\n",
      "Current loss:  3.4832236766815186  | , previous best loss:  3.4841253757476807  | saving best model ...\n",
      "Current loss:  3.4823226928710938  | , previous best loss:  3.4832236766815186  | saving best model ...\n",
      "Current loss:  3.4814226627349854  | , previous best loss:  3.4823226928710938  | saving best model ...\n",
      "Current loss:  3.4805233478546143  | , previous best loss:  3.4814226627349854  | saving best model ...\n",
      "Current loss:  3.4796242713928223  | , previous best loss:  3.4805233478546143  | saving best model ...\n",
      "Current loss:  3.478726625442505  | , previous best loss:  3.4796242713928223  | saving best model ...\n",
      "Current loss:  3.4778292179107666  | , previous best loss:  3.478726625442505  | saving best model ...\n",
      "Current loss:  3.4769320487976074  | , previous best loss:  3.4778292179107666  | saving best model ...\n",
      "Current loss:  3.476036310195923  | , previous best loss:  3.4769320487976074  | saving best model ...\n",
      "Current loss:  3.4751410484313965  | , previous best loss:  3.476036310195923  | saving best model ...\n",
      "Current loss:  3.4742465019226074  | , previous best loss:  3.4751410484313965  | saving best model ...\n",
      "Current loss:  3.4733526706695557  | , previous best loss:  3.4742465019226074  | saving best model ...\n",
      "Current loss:  3.4724600315093994  | , previous best loss:  3.4733526706695557  | saving best model ...\n",
      "Current loss:  3.471567392349243  | , previous best loss:  3.4724600315093994  | saving best model ...\n",
      "Current loss:  3.470675468444824  | , previous best loss:  3.471567392349243  | saving best model ...\n",
      "Current loss:  3.4697844982147217  | , previous best loss:  3.470675468444824  | saving best model ...\n",
      "Current loss:  3.4688942432403564  | , previous best loss:  3.4697844982147217  | saving best model ...\n",
      "Current loss:  3.4680047035217285  | , previous best loss:  3.4688942432403564  | saving best model ...\n",
      "Current loss:  3.467116117477417  | , previous best loss:  3.4680047035217285  | saving best model ...\n",
      "Current loss:  3.4662275314331055  | , previous best loss:  3.467116117477417  | saving best model ...\n",
      "Current loss:  3.4653401374816895  | , previous best loss:  3.4662275314331055  | saving best model ...\n",
      "Current loss:  3.4644532203674316  | , previous best loss:  3.4653401374816895  | saving best model ...\n",
      "Current loss:  3.4635674953460693  | , previous best loss:  3.4644532203674316  | saving best model ...\n",
      "Current loss:  3.462681531906128  | , previous best loss:  3.4635674953460693  | saving best model ...\n",
      "Current loss:  3.461796522140503  | , previous best loss:  3.462681531906128  | saving best model ...\n",
      "Current loss:  3.4609127044677734  | , previous best loss:  3.461796522140503  | saving best model ...\n",
      "Current loss:  3.460029363632202  | , previous best loss:  3.4609127044677734  | saving best model ...\n",
      "Current loss:  3.4591469764709473  | , previous best loss:  3.460029363632202  | saving best model ...\n",
      "Current loss:  3.4582648277282715  | , previous best loss:  3.4591469764709473  | saving best model ...\n",
      "Current loss:  3.457383632659912  | , previous best loss:  3.4582648277282715  | saving best model ...\n",
      "Current loss:  3.456502914428711  | , previous best loss:  3.457383632659912  | saving best model ...\n",
      "Current loss:  3.455623149871826  | , previous best loss:  3.456502914428711  | saving best model ...\n",
      "Current loss:  3.4547438621520996  | , previous best loss:  3.455623149871826  | saving best model ...\n",
      "Current loss:  3.4538657665252686  | , previous best loss:  3.4547438621520996  | saving best model ...\n",
      "Current loss:  3.4529871940612793  | , previous best loss:  3.4538657665252686  | saving best model ...\n",
      "Current loss:  3.4521098136901855  | , previous best loss:  3.4529871940612793  | saving best model ...\n",
      "Current loss:  3.451233386993408  | , previous best loss:  3.4521098136901855  | saving best model ...\n",
      "Current loss:  3.4503579139709473  | , previous best loss:  3.451233386993408  | saving best model ...\n",
      "Current loss:  3.4494824409484863  | , previous best loss:  3.4503579139709473  | saving best model ...\n",
      "Current loss:  3.448608160018921  | , previous best loss:  3.4494824409484863  | saving best model ...\n",
      "Current loss:  3.4477343559265137  | , previous best loss:  3.448608160018921  | saving best model ...\n",
      "Current loss:  3.4468612670898438  | , previous best loss:  3.4477343559265137  | saving best model ...\n",
      "Current loss:  3.445988655090332  | , previous best loss:  3.4468612670898438  | saving best model ...\n",
      "Current loss:  3.445117473602295  | , previous best loss:  3.445988655090332  | saving best model ...\n",
      "Current loss:  3.444246292114258  | , previous best loss:  3.445117473602295  | saving best model ...\n",
      "Current loss:  3.443375825881958  | , previous best loss:  3.444246292114258  | saving best model ...\n",
      "Current loss:  3.4425058364868164  | , previous best loss:  3.443375825881958  | saving best model ...\n",
      "Current loss:  3.4416372776031494  | , previous best loss:  3.4425058364868164  | saving best model ...\n",
      "Current loss:  3.4407687187194824  | , previous best loss:  3.4416372776031494  | saving best model ...\n",
      "Current loss:  3.439901113510132  | , previous best loss:  3.4407687187194824  | saving best model ...\n",
      "Current loss:  3.4390342235565186  | , previous best loss:  3.439901113510132  | saving best model ...\n",
      "Current loss:  3.4381675720214844  | , previous best loss:  3.4390342235565186  | saving best model ...\n",
      "Current loss:  3.4373018741607666  | , previous best loss:  3.4381675720214844  | saving best model ...\n",
      "Current loss:  3.4364371299743652  | , previous best loss:  3.4373018741607666  | saving best model ...\n",
      "Current loss:  3.435572624206543  | , previous best loss:  3.4364371299743652  | saving best model ...\n",
      "Current loss:  3.434708595275879  | , previous best loss:  3.435572624206543  | saving best model ...\n",
      "Current loss:  3.4338455200195312  | , previous best loss:  3.434708595275879  | saving best model ...\n",
      "Current loss:  3.4329833984375  | , previous best loss:  3.4338455200195312  | saving best model ...\n",
      "Current loss:  3.432121515274048  | , previous best loss:  3.4329833984375  | saving best model ...\n",
      "Current loss:  3.431260824203491  | , previous best loss:  3.432121515274048  | saving best model ...\n",
      "Current loss:  3.4303998947143555  | , previous best loss:  3.431260824203491  | saving best model ...\n",
      "Current loss:  3.4295406341552734  | , previous best loss:  3.4303998947143555  | saving best model ...\n",
      "Current loss:  3.4286818504333496  | , previous best loss:  3.4295406341552734  | saving best model ...\n",
      "Current loss:  3.427823066711426  | , previous best loss:  3.4286818504333496  | saving best model ...\n",
      "Current loss:  3.4269652366638184  | , previous best loss:  3.427823066711426  | saving best model ...\n",
      "Current loss:  3.4261083602905273  | , previous best loss:  3.4269652366638184  | saving best model ...\n",
      "Current loss:  3.4252519607543945  | , previous best loss:  3.4261083602905273  | saving best model ...\n",
      "Current loss:  3.424396276473999  | , previous best loss:  3.4252519607543945  | saving best model ...\n",
      "Current loss:  3.423541307449341  | , previous best loss:  3.424396276473999  | saving best model ...\n",
      "Current loss:  3.4226865768432617  | , previous best loss:  3.423541307449341  | saving best model ...\n",
      "Current loss:  3.421832799911499  | , previous best loss:  3.4226865768432617  | saving best model ...\n",
      "Current loss:  3.4209797382354736  | , previous best loss:  3.421832799911499  | saving best model ...\n",
      "Current loss:  3.4201271533966064  | , previous best loss:  3.4209797382354736  | saving best model ...\n",
      "Current loss:  3.4192757606506348  | , previous best loss:  3.4201271533966064  | saving best model ...\n",
      "Current loss:  3.418424606323242  | , previous best loss:  3.4192757606506348  | saving best model ...\n",
      "Current loss:  3.4175736904144287  | , previous best loss:  3.418424606323242  | saving best model ...\n",
      "Current loss:  3.41672420501709  | , previous best loss:  3.4175736904144287  | saving best model ...\n",
      "Current loss:  3.41587495803833  | , previous best loss:  3.41672420501709  | saving best model ...\n",
      "Current loss:  3.4150264263153076  | , previous best loss:  3.41587495803833  | saving best model ...\n",
      "Current loss:  3.4141783714294434  | , previous best loss:  3.4150264263153076  | saving best model ...\n",
      "Current loss:  3.4133312702178955  | , previous best loss:  3.4141783714294434  | saving best model ...\n",
      "Current loss:  3.4124844074249268  | , previous best loss:  3.4133312702178955  | saving best model ...\n",
      "Current loss:  3.4116384983062744  | , previous best loss:  3.4124844074249268  | saving best model ...\n",
      "Current loss:  3.4107933044433594  | , previous best loss:  3.4116384983062744  | saving best model ...\n",
      "Current loss:  3.4099485874176025  | , previous best loss:  3.4107933044433594  | saving best model ...\n",
      "Current loss:  3.409104824066162  | , previous best loss:  3.4099485874176025  | saving best model ...\n",
      "Current loss:  3.408261299133301  | , previous best loss:  3.409104824066162  | saving best model ...\n",
      "Current loss:  3.4074184894561768  | , previous best loss:  3.408261299133301  | saving best model ...\n",
      "Current loss:  3.406576633453369  | , previous best loss:  3.4074184894561768  | saving best model ...\n",
      "Current loss:  3.4057350158691406  | , previous best loss:  3.406576633453369  | saving best model ...\n",
      "Current loss:  3.4048943519592285  | , previous best loss:  3.4057350158691406  | saving best model ...\n",
      "Current loss:  3.4040541648864746  | , previous best loss:  3.4048943519592285  | saving best model ...\n",
      "Current loss:  3.403214454650879  | , previous best loss:  3.4040541648864746  | saving best model ...\n",
      "Current loss:  3.4023754596710205  | , previous best loss:  3.403214454650879  | saving best model ...\n",
      "Current loss:  3.4015376567840576  | , previous best loss:  3.4023754596710205  | saving best model ...\n",
      "Current loss:  3.4006996154785156  | , previous best loss:  3.4015376567840576  | saving best model ...\n",
      "Current loss:  3.3998630046844482  | , previous best loss:  3.4006996154785156  | saving best model ...\n",
      "Current loss:  3.39902663230896  | , previous best loss:  3.3998630046844482  | saving best model ...\n",
      "Current loss:  3.398190975189209  | , previous best loss:  3.39902663230896  | saving best model ...\n",
      "Current loss:  3.3973560333251953  | , previous best loss:  3.398190975189209  | saving best model ...\n",
      "Current loss:  3.39652156829834  | , previous best loss:  3.3973560333251953  | saving best model ...\n",
      "Current loss:  3.3956878185272217  | , previous best loss:  3.39652156829834  | saving best model ...\n",
      "Current loss:  3.39485502243042  | , previous best loss:  3.3956878185272217  | saving best model ...\n",
      "Current loss:  3.394022226333618  | , previous best loss:  3.39485502243042  | saving best model ...\n",
      "Current loss:  3.3931901454925537  | , previous best loss:  3.394022226333618  | saving best model ...\n",
      "Current loss:  3.3923590183258057  | , previous best loss:  3.3931901454925537  | saving best model ...\n",
      "Current loss:  3.391528606414795  | , previous best loss:  3.3923590183258057  | saving best model ...\n",
      "Current loss:  3.3906984329223633  | , previous best loss:  3.391528606414795  | saving best model ...\n",
      "Current loss:  3.389869213104248  | , previous best loss:  3.3906984329223633  | saving best model ...\n",
      "Current loss:  3.389040470123291  | , previous best loss:  3.389869213104248  | saving best model ...\n",
      "Current loss:  3.3882124423980713  | , previous best loss:  3.389040470123291  | saving best model ...\n",
      "Current loss:  3.387385368347168  | , previous best loss:  3.3882124423980713  | saving best model ...\n",
      "Current loss:  3.3865582942962646  | , previous best loss:  3.387385368347168  | saving best model ...\n",
      "Current loss:  3.3857321739196777  | , previous best loss:  3.3865582942962646  | saving best model ...\n",
      "Current loss:  3.384906768798828  | , previous best loss:  3.3857321739196777  | saving best model ...\n",
      "Current loss:  3.3840811252593994  | , previous best loss:  3.384906768798828  | saving best model ...\n",
      "Current loss:  3.3832571506500244  | , previous best loss:  3.3840811252593994  | saving best model ...\n",
      "Current loss:  3.3824336528778076  | , previous best loss:  3.3832571506500244  | saving best model ...\n",
      "Current loss:  3.381610870361328  | , previous best loss:  3.3824336528778076  | saving best model ...\n",
      "Current loss:  3.3807883262634277  | , previous best loss:  3.381610870361328  | saving best model ...\n",
      "Current loss:  3.3799662590026855  | , previous best loss:  3.3807883262634277  | saving best model ...\n",
      "Current loss:  3.3791451454162598  | , previous best loss:  3.3799662590026855  | saving best model ...\n",
      "Current loss:  3.3783247470855713  | , previous best loss:  3.3791451454162598  | saving best model ...\n",
      "Current loss:  3.377504587173462  | , previous best loss:  3.3783247470855713  | saving best model ...\n",
      "Current loss:  3.376685380935669  | , previous best loss:  3.377504587173462  | saving best model ...\n",
      "Current loss:  3.375866651535034  | , previous best loss:  3.376685380935669  | saving best model ...\n",
      "Current loss:  3.375048875808716  | , previous best loss:  3.375866651535034  | saving best model ...\n",
      "Current loss:  3.3742311000823975  | , previous best loss:  3.375048875808716  | saving best model ...\n",
      "Current loss:  3.3734140396118164  | , previous best loss:  3.3742311000823975  | saving best model ...\n",
      "Current loss:  3.372598171234131  | , previous best loss:  3.3734140396118164  | saving best model ...\n",
      "Current loss:  3.3717825412750244  | , previous best loss:  3.372598171234131  | saving best model ...\n",
      "Current loss:  3.3709676265716553  | , previous best loss:  3.3717825412750244  | saving best model ...\n",
      "Current loss:  3.3701534271240234  | , previous best loss:  3.3709676265716553  | saving best model ...\n",
      "Current loss:  3.3693394660949707  | , previous best loss:  3.3701534271240234  | saving best model ...\n",
      "Current loss:  3.368525981903076  | , previous best loss:  3.3693394660949707  | saving best model ...\n",
      "Current loss:  3.3677139282226562  | , previous best loss:  3.368525981903076  | saving best model ...\n",
      "Current loss:  3.3669018745422363  | , previous best loss:  3.3677139282226562  | saving best model ...\n",
      "Current loss:  3.3660905361175537  | , previous best loss:  3.3669018745422363  | saving best model ...\n",
      "Current loss:  3.3652803897857666  | , previous best loss:  3.3660905361175537  | saving best model ...\n",
      "Current loss:  3.3644697666168213  | , previous best loss:  3.3652803897857666  | saving best model ...\n",
      "Current loss:  3.3636600971221924  | , previous best loss:  3.3644697666168213  | saving best model ...\n",
      "Current loss:  3.36285138130188  | , previous best loss:  3.3636600971221924  | saving best model ...\n",
      "Current loss:  3.3620431423187256  | , previous best loss:  3.36285138130188  | saving best model ...\n",
      "Current loss:  3.3612358570098877  | , previous best loss:  3.3620431423187256  | saving best model ...\n",
      "Current loss:  3.36042857170105  | , previous best loss:  3.3612358570098877  | saving best model ...\n",
      "Current loss:  3.359622001647949  | , previous best loss:  3.36042857170105  | saving best model ...\n",
      "Current loss:  3.358816385269165  | , previous best loss:  3.359622001647949  | saving best model ...\n",
      "Current loss:  3.358011245727539  | , previous best loss:  3.358816385269165  | saving best model ...\n",
      "Current loss:  3.3572068214416504  | , previous best loss:  3.358011245727539  | saving best model ...\n",
      "Current loss:  3.3564023971557617  | , previous best loss:  3.3572068214416504  | saving best model ...\n",
      "Current loss:  3.3555996417999268  | , previous best loss:  3.3564023971557617  | saving best model ...\n",
      "Current loss:  3.3547966480255127  | , previous best loss:  3.3555996417999268  | saving best model ...\n",
      "Current loss:  3.353994369506836  | , previous best loss:  3.3547966480255127  | saving best model ...\n",
      "Current loss:  3.3531930446624756  | , previous best loss:  3.353994369506836  | saving best model ...\n",
      "Current loss:  3.3523921966552734  | , previous best loss:  3.3531930446624756  | saving best model ...\n",
      "Current loss:  3.3515915870666504  | , previous best loss:  3.3523921966552734  | saving best model ...\n",
      "Current loss:  3.3507919311523438  | , previous best loss:  3.3515915870666504  | saving best model ...\n",
      "Current loss:  3.3499927520751953  | , previous best loss:  3.3507919311523438  | saving best model ...\n",
      "Current loss:  3.349194288253784  | , previous best loss:  3.3499927520751953  | saving best model ...\n",
      "Current loss:  3.3483965396881104  | , previous best loss:  3.349194288253784  | saving best model ...\n",
      "Current loss:  3.3475987911224365  | , previous best loss:  3.3483965396881104  | saving best model ...\n",
      "Current loss:  3.3468027114868164  | , previous best loss:  3.3475987911224365  | saving best model ...\n",
      "Current loss:  3.346005916595459  | , previous best loss:  3.3468027114868164  | saving best model ...\n",
      "Current loss:  3.3452107906341553  | , previous best loss:  3.346005916595459  | saving best model ...\n",
      "Current loss:  3.3444159030914307  | , previous best loss:  3.3452107906341553  | saving best model ...\n",
      "Current loss:  3.343621253967285  | , previous best loss:  3.3444159030914307  | saving best model ...\n",
      "Current loss:  3.342827796936035  | , previous best loss:  3.343621253967285  | saving best model ...\n",
      "Current loss:  3.3420345783233643  | , previous best loss:  3.342827796936035  | saving best model ...\n",
      "Current loss:  3.3412418365478516  | , previous best loss:  3.3420345783233643  | saving best model ...\n",
      "Current loss:  3.3404502868652344  | , previous best loss:  3.3412418365478516  | saving best model ...\n",
      "Current loss:  3.3396589756011963  | , previous best loss:  3.3404502868652344  | saving best model ...\n",
      "Current loss:  3.3388681411743164  | , previous best loss:  3.3396589756011963  | saving best model ...\n",
      "Current loss:  3.3380777835845947  | , previous best loss:  3.3388681411743164  | saving best model ...\n",
      "Current loss:  3.3372883796691895  | , previous best loss:  3.3380777835845947  | saving best model ...\n",
      "Current loss:  3.3364994525909424  | , previous best loss:  3.3372883796691895  | saving best model ...\n",
      "Current loss:  3.3357114791870117  | , previous best loss:  3.3364994525909424  | saving best model ...\n",
      "Current loss:  3.334923267364502  | , previous best loss:  3.3357114791870117  | saving best model ...\n",
      "Current loss:  3.3341362476348877  | , previous best loss:  3.334923267364502  | saving best model ...\n",
      "Current loss:  3.3333499431610107  | , previous best loss:  3.3341362476348877  | saving best model ...\n",
      "Current loss:  3.332563638687134  | , previous best loss:  3.3333499431610107  | saving best model ...\n",
      "Current loss:  3.3317782878875732  | , previous best loss:  3.332563638687134  | saving best model ...\n",
      "Current loss:  3.33099365234375  | , previous best loss:  3.3317782878875732  | saving best model ...\n",
      "Current loss:  3.330209493637085  | , previous best loss:  3.33099365234375  | saving best model ...\n",
      "Current loss:  3.3294260501861572  | , previous best loss:  3.330209493637085  | saving best model ...\n",
      "Current loss:  3.3286426067352295  | , previous best loss:  3.3294260501861572  | saving best model ...\n",
      "Current loss:  3.327859878540039  | , previous best loss:  3.3286426067352295  | saving best model ...\n",
      "Current loss:  3.327078104019165  | , previous best loss:  3.327859878540039  | saving best model ...\n",
      "Current loss:  3.326296806335449  | , previous best loss:  3.327078104019165  | saving best model ...\n",
      "Current loss:  3.3255162239074707  | , previous best loss:  3.326296806335449  | saving best model ...\n",
      "Current loss:  3.3247363567352295  | , previous best loss:  3.3255162239074707  | saving best model ...\n",
      "Current loss:  3.3239564895629883  | , previous best loss:  3.3247363567352295  | saving best model ...\n",
      "Current loss:  3.3231780529022217  | , previous best loss:  3.3239564895629883  | saving best model ...\n",
      "Current loss:  3.322399377822876  | , previous best loss:  3.3231780529022217  | saving best model ...\n",
      "Current loss:  3.321622133255005  | , previous best loss:  3.322399377822876  | saving best model ...\n",
      "Current loss:  3.3208444118499756  | , previous best loss:  3.321622133255005  | saving best model ...\n",
      "Current loss:  3.320067882537842  | , previous best loss:  3.3208444118499756  | saving best model ...\n",
      "Current loss:  3.3192920684814453  | , previous best loss:  3.320067882537842  | saving best model ...\n",
      "Current loss:  3.318516254425049  | , previous best loss:  3.3192920684814453  | saving best model ...\n",
      "Current loss:  3.317741632461548  | , previous best loss:  3.318516254425049  | saving best model ...\n",
      "Current loss:  3.316967487335205  | , previous best loss:  3.317741632461548  | saving best model ...\n",
      "Current loss:  3.3161935806274414  | , previous best loss:  3.316967487335205  | saving best model ...\n",
      "Current loss:  3.315420389175415  | , previous best loss:  3.3161935806274414  | saving best model ...\n",
      "Current loss:  3.314647674560547  | , previous best loss:  3.315420389175415  | saving best model ...\n",
      "Current loss:  3.313875913619995  | , previous best loss:  3.314647674560547  | saving best model ...\n",
      "Current loss:  3.3131048679351807  | , previous best loss:  3.313875913619995  | saving best model ...\n",
      "Current loss:  3.312333822250366  | , previous best loss:  3.3131048679351807  | saving best model ...\n",
      "Current loss:  3.311563491821289  | , previous best loss:  3.312333822250366  | saving best model ...\n",
      "Current loss:  3.310793876647949  | , previous best loss:  3.311563491821289  | saving best model ...\n",
      "Current loss:  3.3100249767303467  | , previous best loss:  3.310793876647949  | saving best model ...\n",
      "Current loss:  3.309256076812744  | , previous best loss:  3.3100249767303467  | saving best model ...\n",
      "Current loss:  3.308488368988037  | , previous best loss:  3.309256076812744  | saving best model ...\n",
      "Current loss:  3.307720899581909  | , previous best loss:  3.308488368988037  | saving best model ...\n",
      "Current loss:  3.3069543838500977  | , previous best loss:  3.307720899581909  | saving best model ...\n",
      "Current loss:  3.3061881065368652  | , previous best loss:  3.3069543838500977  | saving best model ...\n",
      "Current loss:  3.30542254447937  | , previous best loss:  3.3061881065368652  | saving best model ...\n",
      "Current loss:  3.3046576976776123  | , previous best loss:  3.30542254447937  | saving best model ...\n",
      "Current loss:  3.3038930892944336  | , previous best loss:  3.3046576976776123  | saving best model ...\n",
      "Current loss:  3.303129196166992  | , previous best loss:  3.3038930892944336  | saving best model ...\n",
      "Current loss:  3.302365779876709  | , previous best loss:  3.303129196166992  | saving best model ...\n",
      "Current loss:  3.301603078842163  | , previous best loss:  3.302365779876709  | saving best model ...\n",
      "Current loss:  3.3008408546447754  | , previous best loss:  3.301603078842163  | saving best model ...\n",
      "Current loss:  3.300079107284546  | , previous best loss:  3.3008408546447754  | saving best model ...\n",
      "Current loss:  3.299318313598633  | , previous best loss:  3.300079107284546  | saving best model ...\n",
      "Current loss:  3.2985575199127197  | , previous best loss:  3.299318313598633  | saving best model ...\n",
      "Current loss:  3.297797918319702  | , previous best loss:  3.2985575199127197  | saving best model ...\n",
      "Current loss:  3.2970383167266846  | , previous best loss:  3.297797918319702  | saving best model ...\n",
      "Current loss:  3.2962796688079834  | , previous best loss:  3.2970383167266846  | saving best model ...\n",
      "Current loss:  3.2955212593078613  | , previous best loss:  3.2962796688079834  | saving best model ...\n",
      "Current loss:  3.2947638034820557  | , previous best loss:  3.2955212593078613  | saving best model ...\n",
      "Current loss:  3.294006824493408  | , previous best loss:  3.2947638034820557  | saving best model ...\n",
      "Current loss:  3.293250322341919  | , previous best loss:  3.294006824493408  | saving best model ...\n",
      "Current loss:  3.292494297027588  | , previous best loss:  3.293250322341919  | saving best model ...\n",
      "Current loss:  3.291738986968994  | , previous best loss:  3.292494297027588  | saving best model ...\n",
      "Current loss:  3.2909841537475586  | , previous best loss:  3.291738986968994  | saving best model ...\n",
      "Current loss:  3.2902297973632812  | , previous best loss:  3.2909841537475586  | saving best model ...\n",
      "Current loss:  3.2894763946533203  | , previous best loss:  3.2902297973632812  | saving best model ...\n",
      "Current loss:  3.2887227535247803  | , previous best loss:  3.2894763946533203  | saving best model ...\n",
      "Current loss:  3.2879703044891357  | , previous best loss:  3.2887227535247803  | saving best model ...\n",
      "Current loss:  3.2872185707092285  | , previous best loss:  3.2879703044891357  | saving best model ...\n",
      "Current loss:  3.2864670753479004  | , previous best loss:  3.2872185707092285  | saving best model ...\n",
      "Current loss:  3.2857160568237305  | , previous best loss:  3.2864670753479004  | saving best model ...\n",
      "Current loss:  3.284965991973877  | , previous best loss:  3.2857160568237305  | saving best model ...\n",
      "Current loss:  3.2842159271240234  | , previous best loss:  3.284965991973877  | saving best model ...\n",
      "Current loss:  3.2834668159484863  | , previous best loss:  3.2842159271240234  | saving best model ...\n",
      "Current loss:  3.2827179431915283  | , previous best loss:  3.2834668159484863  | saving best model ...\n",
      "Current loss:  3.2819700241088867  | , previous best loss:  3.2827179431915283  | saving best model ...\n",
      "Current loss:  3.281222343444824  | , previous best loss:  3.2819700241088867  | saving best model ...\n",
      "Current loss:  3.280475378036499  | , previous best loss:  3.281222343444824  | saving best model ...\n",
      "Current loss:  3.2797293663024902  | , previous best loss:  3.280475378036499  | saving best model ...\n",
      "Current loss:  3.2789831161499023  | , previous best loss:  3.2797293663024902  | saving best model ...\n",
      "Current loss:  3.278237819671631  | , previous best loss:  3.2789831161499023  | saving best model ...\n",
      "Current loss:  3.2774930000305176  | , previous best loss:  3.278237819671631  | saving best model ...\n",
      "Current loss:  3.2767491340637207  | , previous best loss:  3.2774930000305176  | saving best model ...\n",
      "Current loss:  3.2760050296783447  | , previous best loss:  3.2767491340637207  | saving best model ...\n",
      "Current loss:  3.2752621173858643  | , previous best loss:  3.2760050296783447  | saving best model ...\n",
      "Current loss:  3.274519205093384  | , previous best loss:  3.2752621173858643  | saving best model ...\n",
      "Current loss:  3.2737772464752197  | , previous best loss:  3.274519205093384  | saving best model ...\n",
      "Current loss:  3.273036003112793  | , previous best loss:  3.2737772464752197  | saving best model ...\n",
      "Current loss:  3.2722952365875244  | , previous best loss:  3.273036003112793  | saving best model ...\n",
      "Current loss:  3.271554708480835  | , previous best loss:  3.2722952365875244  | saving best model ...\n",
      "Current loss:  3.2708146572113037  | , previous best loss:  3.271554708480835  | saving best model ...\n",
      "Current loss:  3.270075798034668  | , previous best loss:  3.2708146572113037  | saving best model ...\n",
      "Current loss:  3.2693369388580322  | , previous best loss:  3.270075798034668  | saving best model ...\n",
      "Current loss:  3.268599033355713  | , previous best loss:  3.2693369388580322  | saving best model ...\n",
      "Current loss:  3.2678611278533936  | , previous best loss:  3.268599033355713  | saving best model ...\n",
      "Current loss:  3.2671239376068115  | , previous best loss:  3.2678611278533936  | saving best model ...\n",
      "Current loss:  3.266387701034546  | , previous best loss:  3.2671239376068115  | saving best model ...\n",
      "Current loss:  3.2656517028808594  | , previous best loss:  3.266387701034546  | saving best model ...\n",
      "Current loss:  3.264916181564331  | , previous best loss:  3.2656517028808594  | saving best model ...\n",
      "Current loss:  3.26418137550354  | , previous best loss:  3.264916181564331  | saving best model ...\n",
      "Current loss:  3.2634470462799072  | , previous best loss:  3.26418137550354  | saving best model ...\n",
      "Current loss:  3.2627131938934326  | , previous best loss:  3.2634470462799072  | saving best model ...\n",
      "Current loss:  3.2619800567626953  | , previous best loss:  3.2627131938934326  | saving best model ...\n",
      "Current loss:  3.261247158050537  | , previous best loss:  3.2619800567626953  | saving best model ...\n",
      "Current loss:  3.2605152130126953  | , previous best loss:  3.261247158050537  | saving best model ...\n",
      "Current loss:  3.2597832679748535  | , previous best loss:  3.2605152130126953  | saving best model ...\n",
      "Current loss:  3.2590525150299072  | , previous best loss:  3.2597832679748535  | saving best model ...\n",
      "Current loss:  3.25832200050354  | , previous best loss:  3.2590525150299072  | saving best model ...\n",
      "Current loss:  3.257591962814331  | , previous best loss:  3.25832200050354  | saving best model ...\n",
      "Current loss:  3.2568626403808594  | , previous best loss:  3.257591962814331  | saving best model ...\n",
      "Current loss:  3.256133556365967  | , previous best loss:  3.2568626403808594  | saving best model ...\n",
      "Current loss:  3.2554051876068115  | , previous best loss:  3.256133556365967  | saving best model ...\n",
      "Current loss:  3.2546770572662354  | , previous best loss:  3.2554051876068115  | saving best model ...\n",
      "Current loss:  3.253950357437134  | , previous best loss:  3.2546770572662354  | saving best model ...\n",
      "Current loss:  3.253223180770874  | , previous best loss:  3.253950357437134  | saving best model ...\n",
      "Current loss:  3.2524969577789307  | , previous best loss:  3.253223180770874  | saving best model ...\n",
      "Current loss:  3.2517712116241455  | , previous best loss:  3.2524969577789307  | saving best model ...\n",
      "Current loss:  3.2510464191436768  | , previous best loss:  3.2517712116241455  | saving best model ...\n",
      "Current loss:  3.250321865081787  | , previous best loss:  3.2510464191436768  | saving best model ...\n",
      "Current loss:  3.2495973110198975  | , previous best loss:  3.250321865081787  | saving best model ...\n",
      "Current loss:  3.2488739490509033  | , previous best loss:  3.2495973110198975  | saving best model ...\n",
      "Current loss:  3.2481508255004883  | , previous best loss:  3.2488739490509033  | saving best model ...\n",
      "Current loss:  3.2474286556243896  | , previous best loss:  3.2481508255004883  | saving best model ...\n",
      "Current loss:  3.246706485748291  | , previous best loss:  3.2474286556243896  | saving best model ...\n",
      "Current loss:  3.2459850311279297  | , previous best loss:  3.246706485748291  | saving best model ...\n",
      "Current loss:  3.2452642917633057  | , previous best loss:  3.2459850311279297  | saving best model ...\n",
      "Current loss:  3.24454402923584  | , previous best loss:  3.2452642917633057  | saving best model ...\n",
      "Current loss:  3.243824005126953  | , previous best loss:  3.24454402923584  | saving best model ...\n",
      "Current loss:  3.2431046962738037  | , previous best loss:  3.243824005126953  | saving best model ...\n",
      "Current loss:  3.2423856258392334  | , previous best loss:  3.2431046962738037  | saving best model ...\n",
      "Current loss:  3.2416677474975586  | , previous best loss:  3.2423856258392334  | saving best model ...\n",
      "Current loss:  3.240950107574463  | , previous best loss:  3.2416677474975586  | saving best model ...\n",
      "Current loss:  3.2402329444885254  | , previous best loss:  3.240950107574463  | saving best model ...\n",
      "Current loss:  3.239516496658325  | , previous best loss:  3.2402329444885254  | saving best model ...\n",
      "Current loss:  3.238800048828125  | , previous best loss:  3.239516496658325  | saving best model ...\n",
      "Current loss:  3.238084316253662  | , previous best loss:  3.238800048828125  | saving best model ...\n",
      "Current loss:  3.2373692989349365  | , previous best loss:  3.238084316253662  | saving best model ...\n",
      "Current loss:  3.2366549968719482  | , previous best loss:  3.2373692989349365  | saving best model ...\n",
      "Current loss:  3.235941171646118  | , previous best loss:  3.2366549968719482  | saving best model ...\n",
      "Current loss:  3.235227584838867  | , previous best loss:  3.235941171646118  | saving best model ...\n",
      "Current loss:  3.2345142364501953  | , previous best loss:  3.235227584838867  | saving best model ...\n",
      "Current loss:  3.233802080154419  | , previous best loss:  3.2345142364501953  | saving best model ...\n",
      "Current loss:  3.2330899238586426  | , previous best loss:  3.233802080154419  | saving best model ...\n",
      "Current loss:  3.2323784828186035  | , previous best loss:  3.2330899238586426  | saving best model ...\n",
      "Current loss:  3.2316677570343018  | , previous best loss:  3.2323784828186035  | saving best model ...\n",
      "Current loss:  3.230957508087158  | , previous best loss:  3.2316677570343018  | saving best model ...\n",
      "Current loss:  3.2302474975585938  | , previous best loss:  3.230957508087158  | saving best model ...\n",
      "Current loss:  3.2295382022857666  | , previous best loss:  3.2302474975585938  | saving best model ...\n",
      "Current loss:  3.2288293838500977  | , previous best loss:  3.2295382022857666  | saving best model ...\n",
      "Current loss:  3.228121280670166  | , previous best loss:  3.2288293838500977  | saving best model ...\n",
      "Current loss:  3.2274131774902344  | , previous best loss:  3.228121280670166  | saving best model ...\n",
      "Current loss:  3.226706027984619  | , previous best loss:  3.2274131774902344  | saving best model ...\n",
      "Current loss:  3.225999355316162  | , previous best loss:  3.226706027984619  | saving best model ...\n",
      "Current loss:  3.225292921066284  | , previous best loss:  3.225999355316162  | saving best model ...\n",
      "Current loss:  3.2245874404907227  | , previous best loss:  3.225292921066284  | saving best model ...\n",
      "Current loss:  3.2238824367523193  | , previous best loss:  3.2245874404907227  | saving best model ...\n",
      "Current loss:  3.223177433013916  | , previous best loss:  3.2238824367523193  | saving best model ...\n",
      "Current loss:  3.222473382949829  | , previous best loss:  3.223177433013916  | saving best model ...\n",
      "Current loss:  3.2217700481414795  | , previous best loss:  3.222473382949829  | saving best model ...\n",
      "Current loss:  3.22106671333313  | , previous best loss:  3.2217700481414795  | saving best model ...\n",
      "Current loss:  3.2203640937805176  | , previous best loss:  3.22106671333313  | saving best model ...\n",
      "Current loss:  3.2196617126464844  | , previous best loss:  3.2203640937805176  | saving best model ...\n",
      "Current loss:  3.2189602851867676  | , previous best loss:  3.2196617126464844  | saving best model ...\n",
      "Current loss:  3.218259334564209  | , previous best loss:  3.2189602851867676  | saving best model ...\n",
      "Current loss:  3.2175588607788086  | , previous best loss:  3.218259334564209  | saving best model ...\n",
      "Current loss:  3.2168586254119873  | , previous best loss:  3.2175588607788086  | saving best model ...\n",
      "Current loss:  3.216158628463745  | , previous best loss:  3.2168586254119873  | saving best model ...\n",
      "Current loss:  3.2154603004455566  | , previous best loss:  3.216158628463745  | saving best model ...\n",
      "Current loss:  3.21476149559021  | , previous best loss:  3.2154603004455566  | saving best model ...\n",
      "Current loss:  3.2140636444091797  | , previous best loss:  3.21476149559021  | saving best model ...\n",
      "Current loss:  3.2133657932281494  | , previous best loss:  3.2140636444091797  | saving best model ...\n",
      "Current loss:  3.2126688957214355  | , previous best loss:  3.2133657932281494  | saving best model ...\n",
      "Current loss:  3.21197247505188  | , previous best loss:  3.2126688957214355  | saving best model ...\n",
      "Current loss:  3.2112762928009033  | , previous best loss:  3.21197247505188  | saving best model ...\n",
      "Current loss:  3.210580825805664  | , previous best loss:  3.2112762928009033  | saving best model ...\n",
      "Current loss:  3.209886074066162  | , previous best loss:  3.210580825805664  | saving best model ...\n",
      "Current loss:  3.20919132232666  | , previous best loss:  3.209886074066162  | saving best model ...\n",
      "Current loss:  3.2084975242614746  | , previous best loss:  3.20919132232666  | saving best model ...\n",
      "Current loss:  3.207803726196289  | , previous best loss:  3.2084975242614746  | saving best model ...\n",
      "Current loss:  3.207111120223999  | , previous best loss:  3.207803726196289  | saving best model ...\n",
      "Current loss:  3.206418514251709  | , previous best loss:  3.207111120223999  | saving best model ...\n",
      "Current loss:  3.2057266235351562  | , previous best loss:  3.206418514251709  | saving best model ...\n",
      "Current loss:  3.2050349712371826  | , previous best loss:  3.2057266235351562  | saving best model ...\n",
      "Current loss:  3.2043440341949463  | , previous best loss:  3.2050349712371826  | saving best model ...\n",
      "Current loss:  3.2036538124084473  | , previous best loss:  3.2043440341949463  | saving best model ...\n",
      "Current loss:  3.2029638290405273  | , previous best loss:  3.2036538124084473  | saving best model ...\n",
      "Current loss:  3.2022740840911865  | , previous best loss:  3.2029638290405273  | saving best model ...\n",
      "Current loss:  3.201585531234741  | , previous best loss:  3.2022740840911865  | saving best model ...\n",
      "Current loss:  3.2008965015411377  | , previous best loss:  3.201585531234741  | saving best model ...\n",
      "Current loss:  3.2002086639404297  | , previous best loss:  3.2008965015411377  | saving best model ...\n",
      "Current loss:  3.199521541595459  | , previous best loss:  3.2002086639404297  | saving best model ...\n",
      "Current loss:  3.19883394241333  | , previous best loss:  3.199521541595459  | saving best model ...\n",
      "Current loss:  3.198148012161255  | , previous best loss:  3.19883394241333  | saving best model ...\n",
      "Current loss:  3.1974618434906006  | , previous best loss:  3.198148012161255  | saving best model ...\n",
      "Current loss:  3.1967761516571045  | , previous best loss:  3.1974618434906006  | saving best model ...\n",
      "Current loss:  3.196091413497925  | , previous best loss:  3.1967761516571045  | saving best model ...\n",
      "Current loss:  3.195406436920166  | , previous best loss:  3.196091413497925  | saving best model ...\n",
      "Current loss:  3.1947226524353027  | , previous best loss:  3.195406436920166  | saving best model ...\n",
      "Current loss:  3.1940391063690186  | , previous best loss:  3.1947226524353027  | saving best model ...\n",
      "Current loss:  3.193356513977051  | , previous best loss:  3.1940391063690186  | saving best model ...\n",
      "Current loss:  3.192673444747925  | , previous best loss:  3.193356513977051  | saving best model ...\n",
      "Current loss:  3.1919918060302734  | , previous best loss:  3.192673444747925  | saving best model ...\n",
      "Current loss:  3.191310167312622  | , previous best loss:  3.1919918060302734  | saving best model ...\n",
      "Current loss:  3.190629005432129  | , previous best loss:  3.191310167312622  | saving best model ...\n",
      "Current loss:  3.189948320388794  | , previous best loss:  3.190629005432129  | saving best model ...\n",
      "Current loss:  3.189268112182617  | , previous best loss:  3.189948320388794  | saving best model ...\n",
      "Current loss:  3.1885883808135986  | , previous best loss:  3.189268112182617  | saving best model ...\n",
      "Current loss:  3.1879091262817383  | , previous best loss:  3.1885883808135986  | saving best model ...\n",
      "Current loss:  3.1872308254241943  | , previous best loss:  3.1879091262817383  | saving best model ...\n",
      "Current loss:  3.1865525245666504  | , previous best loss:  3.1872308254241943  | saving best model ...\n",
      "Current loss:  3.1858749389648438  | , previous best loss:  3.1865525245666504  | saving best model ...\n",
      "Current loss:  3.1851978302001953  | , previous best loss:  3.1858749389648438  | saving best model ...\n",
      "Current loss:  3.184520959854126  | , previous best loss:  3.1851978302001953  | saving best model ...\n",
      "Current loss:  3.183844804763794  | , previous best loss:  3.184520959854126  | saving best model ...\n",
      "Current loss:  3.18316912651062  | , previous best loss:  3.183844804763794  | saving best model ...\n",
      "Current loss:  3.1824936866760254  | , previous best loss:  3.18316912651062  | saving best model ...\n",
      "Current loss:  3.181818962097168  | , previous best loss:  3.1824936866760254  | saving best model ...\n",
      "Current loss:  3.181144952774048  | , previous best loss:  3.181818962097168  | saving best model ...\n",
      "Current loss:  3.1804707050323486  | , previous best loss:  3.181144952774048  | saving best model ...\n",
      "Current loss:  3.179797410964966  | , previous best loss:  3.1804707050323486  | saving best model ...\n",
      "Current loss:  3.1791248321533203  | , previous best loss:  3.179797410964966  | saving best model ...\n",
      "Current loss:  3.178452491760254  | , previous best loss:  3.1791248321533203  | saving best model ...\n",
      "Current loss:  3.1777806282043457  | , previous best loss:  3.178452491760254  | saving best model ...\n",
      "Current loss:  3.1771090030670166  | , previous best loss:  3.1777806282043457  | saving best model ...\n",
      "Current loss:  3.176438093185425  | , previous best loss:  3.1771090030670166  | saving best model ...\n",
      "Current loss:  3.175767421722412  | , previous best loss:  3.176438093185425  | saving best model ...\n",
      "Current loss:  3.175097703933716  | , previous best loss:  3.175767421722412  | saving best model ...\n",
      "Current loss:  3.1744279861450195  | , previous best loss:  3.175097703933716  | saving best model ...\n",
      "Current loss:  3.1737587451934814  | , previous best loss:  3.1744279861450195  | saving best model ...\n",
      "Current loss:  3.1730904579162598  | , previous best loss:  3.1737587451934814  | saving best model ...\n",
      "Current loss:  3.172422170639038  | , previous best loss:  3.1730904579162598  | saving best model ...\n",
      "Current loss:  3.171754837036133  | , previous best loss:  3.172422170639038  | saving best model ...\n",
      "Current loss:  3.1710877418518066  | , previous best loss:  3.171754837036133  | saving best model ...\n",
      "Current loss:  3.1704208850860596  | , previous best loss:  3.1710877418518066  | saving best model ...\n",
      "Current loss:  3.169754981994629  | , previous best loss:  3.1704208850860596  | saving best model ...\n",
      "Current loss:  3.169088840484619  | , previous best loss:  3.169754981994629  | saving best model ...\n",
      "Current loss:  3.168423652648926  | , previous best loss:  3.169088840484619  | saving best model ...\n",
      "Current loss:  3.1677587032318115  | , previous best loss:  3.168423652648926  | saving best model ...\n",
      "Current loss:  3.1670947074890137  | , previous best loss:  3.1677587032318115  | saving best model ...\n",
      "Current loss:  3.1664304733276367  | , previous best loss:  3.1670947074890137  | saving best model ...\n",
      "Current loss:  3.1657674312591553  | , previous best loss:  3.1664304733276367  | saving best model ...\n",
      "Current loss:  3.1651041507720947  | , previous best loss:  3.1657674312591553  | saving best model ...\n",
      "Current loss:  3.1644418239593506  | , previous best loss:  3.1651041507720947  | saving best model ...\n",
      "Current loss:  3.1637799739837646  | , previous best loss:  3.1644418239593506  | saving best model ...\n",
      "Current loss:  3.163118362426758  | , previous best loss:  3.1637799739837646  | saving best model ...\n",
      "Current loss:  3.162457227706909  | , previous best loss:  3.163118362426758  | saving best model ...\n",
      "Current loss:  3.1617965698242188  | , previous best loss:  3.162457227706909  | saving best model ...\n",
      "Current loss:  3.1611366271972656  | , previous best loss:  3.1617965698242188  | saving best model ...\n",
      "Current loss:  3.16047739982605  | , previous best loss:  3.1611366271972656  | saving best model ...\n",
      "Current loss:  3.159817934036255  | , previous best loss:  3.16047739982605  | saving best model ...\n",
      "Current loss:  3.1591591835021973  | , previous best loss:  3.159817934036255  | saving best model ...\n",
      "Current loss:  3.1585006713867188  | , previous best loss:  3.1591591835021973  | saving best model ...\n",
      "Current loss:  3.1578431129455566  | , previous best loss:  3.1585006713867188  | saving best model ...\n",
      "Current loss:  3.1571855545043945  | , previous best loss:  3.1578431129455566  | saving best model ...\n",
      "Current loss:  3.1565284729003906  | , previous best loss:  3.1571855545043945  | saving best model ...\n",
      "Current loss:  3.155872106552124  | , previous best loss:  3.1565284729003906  | saving best model ...\n",
      "Current loss:  3.1552159786224365  | , previous best loss:  3.155872106552124  | saving best model ...\n",
      "Current loss:  3.1545605659484863  | , previous best loss:  3.1552159786224365  | saving best model ...\n",
      "Current loss:  3.1539056301116943  | , previous best loss:  3.1545605659484863  | saving best model ...\n",
      "Current loss:  3.1532509326934814  | , previous best loss:  3.1539056301116943  | saving best model ...\n",
      "Current loss:  3.1525967121124268  | , previous best loss:  3.1532509326934814  | saving best model ...\n",
      "Current loss:  3.1519429683685303  | , previous best loss:  3.1525967121124268  | saving best model ...\n",
      "Current loss:  3.151289939880371  | , previous best loss:  3.1519429683685303  | saving best model ...\n",
      "Current loss:  3.150637149810791  | , previous best loss:  3.151289939880371  | saving best model ...\n",
      "Current loss:  3.149984836578369  | , previous best loss:  3.150637149810791  | saving best model ...\n",
      "Current loss:  3.1493330001831055  | , previous best loss:  3.149984836578369  | saving best model ...\n",
      "Current loss:  3.148681640625  | , previous best loss:  3.1493330001831055  | saving best model ...\n",
      "Current loss:  3.1480307579040527  | , previous best loss:  3.148681640625  | saving best model ...\n",
      "Current loss:  3.1473798751831055  | , previous best loss:  3.1480307579040527  | saving best model ...\n",
      "Current loss:  3.1467301845550537  | , previous best loss:  3.1473798751831055  | saving best model ...\n",
      "Current loss:  3.146080493927002  | , previous best loss:  3.1467301845550537  | saving best model ...\n",
      "Current loss:  3.1454310417175293  | , previous best loss:  3.146080493927002  | saving best model ...\n",
      "Current loss:  3.144782543182373  | , previous best loss:  3.1454310417175293  | saving best model ...\n",
      "Current loss:  3.144134283065796  | , previous best loss:  3.144782543182373  | saving best model ...\n",
      "Current loss:  3.143486261367798  | , previous best loss:  3.144134283065796  | saving best model ...\n",
      "Current loss:  3.142839193344116  | , previous best loss:  3.143486261367798  | saving best model ...\n",
      "Current loss:  3.1421923637390137  | , previous best loss:  3.142839193344116  | saving best model ...\n",
      "Current loss:  3.141545295715332  | , previous best loss:  3.1421923637390137  | saving best model ...\n",
      "Current loss:  3.140899658203125  | , previous best loss:  3.141545295715332  | saving best model ...\n",
      "Current loss:  3.1402535438537598  | , previous best loss:  3.140899658203125  | saving best model ...\n",
      "Current loss:  3.139608860015869  | , previous best loss:  3.1402535438537598  | saving best model ...\n",
      "Current loss:  3.1389644145965576  | , previous best loss:  3.139608860015869  | saving best model ...\n",
      "Current loss:  3.138319969177246  | , previous best loss:  3.1389644145965576  | saving best model ...\n",
      "Current loss:  3.1376760005950928  | , previous best loss:  3.138319969177246  | saving best model ...\n",
      "Current loss:  3.1370327472686768  | , previous best loss:  3.1376760005950928  | saving best model ...\n",
      "Current loss:  3.136389970779419  | , previous best loss:  3.1370327472686768  | saving best model ...\n",
      "Current loss:  3.1357474327087402  | , previous best loss:  3.136389970779419  | saving best model ...\n",
      "Current loss:  3.1351053714752197  | , previous best loss:  3.1357474327087402  | saving best model ...\n",
      "Current loss:  3.1344637870788574  | , previous best loss:  3.1351053714752197  | saving best model ...\n",
      "Current loss:  3.133822441101074  | , previous best loss:  3.1344637870788574  | saving best model ...\n",
      "Current loss:  3.133180856704712  | , previous best loss:  3.133822441101074  | saving best model ...\n",
      "Current loss:  3.1325385570526123  | , previous best loss:  3.133180856704712  | saving best model ...\n",
      "Current loss:  3.131896495819092  | , previous best loss:  3.1325385570526123  | saving best model ...\n",
      "Current loss:  3.1312549114227295  | , previous best loss:  3.131896495819092  | saving best model ...\n",
      "Current loss:  3.1306135654449463  | , previous best loss:  3.1312549114227295  | saving best model ...\n",
      "Current loss:  3.129971981048584  | , previous best loss:  3.1306135654449463  | saving best model ...\n",
      "Current loss:  3.129331350326538  | , previous best loss:  3.129971981048584  | saving best model ...\n",
      "Current loss:  3.128690719604492  | , previous best loss:  3.129331350326538  | saving best model ...\n",
      "Current loss:  3.1280503273010254  | , previous best loss:  3.128690719604492  | saving best model ...\n",
      "Current loss:  3.127410650253296  | , previous best loss:  3.1280503273010254  | saving best model ...\n",
      "Current loss:  3.1267716884613037  | , previous best loss:  3.127410650253296  | saving best model ...\n",
      "Current loss:  3.1261322498321533  | , previous best loss:  3.1267716884613037  | saving best model ...\n",
      "Current loss:  3.1254937648773193  | , previous best loss:  3.1261322498321533  | saving best model ...\n",
      "Current loss:  3.1248559951782227  | , previous best loss:  3.1254937648773193  | saving best model ...\n",
      "Current loss:  3.124217987060547  | , previous best loss:  3.1248559951782227  | saving best model ...\n",
      "Current loss:  3.1235804557800293  | , previous best loss:  3.124217987060547  | saving best model ...\n",
      "Current loss:  3.122943878173828  | , previous best loss:  3.1235804557800293  | saving best model ...\n",
      "Current loss:  3.122307062149048  | , previous best loss:  3.122943878173828  | saving best model ...\n",
      "Current loss:  3.121670722961426  | , previous best loss:  3.122307062149048  | saving best model ...\n",
      "Current loss:  3.121035099029541  | , previous best loss:  3.121670722961426  | saving best model ...\n",
      "Current loss:  3.1203994750976562  | , previous best loss:  3.121035099029541  | saving best model ...\n",
      "Current loss:  3.1197640895843506  | , previous best loss:  3.1203994750976562  | saving best model ...\n",
      "Current loss:  3.1191298961639404  | , previous best loss:  3.1197640895843506  | saving best model ...\n",
      "Current loss:  3.1184959411621094  | , previous best loss:  3.1191298961639404  | saving best model ...\n",
      "Current loss:  3.117861747741699  | , previous best loss:  3.1184959411621094  | saving best model ...\n",
      "Current loss:  3.1172289848327637  | , previous best loss:  3.117861747741699  | saving best model ...\n",
      "Current loss:  3.11659574508667  | , previous best loss:  3.1172289848327637  | saving best model ...\n",
      "Current loss:  3.1159634590148926  | , previous best loss:  3.11659574508667  | saving best model ...\n",
      "Current loss:  3.1153318881988525  | , previous best loss:  3.1159634590148926  | saving best model ...\n",
      "Current loss:  3.1147000789642334  | , previous best loss:  3.1153318881988525  | saving best model ...\n",
      "Current loss:  3.1140689849853516  | , previous best loss:  3.1147000789642334  | saving best model ...\n",
      "Current loss:  3.113438367843628  | , previous best loss:  3.1140689849853516  | saving best model ...\n",
      "Current loss:  3.1128084659576416  | , previous best loss:  3.113438367843628  | saving best model ...\n",
      "Current loss:  3.112178087234497  | , previous best loss:  3.1128084659576416  | saving best model ...\n",
      "Current loss:  3.111549139022827  | , previous best loss:  3.112178087234497  | saving best model ...\n",
      "Current loss:  3.110919952392578  | , previous best loss:  3.111549139022827  | saving best model ...\n",
      "Current loss:  3.1102914810180664  | , previous best loss:  3.110919952392578  | saving best model ...\n",
      "Current loss:  3.1096630096435547  | , previous best loss:  3.1102914810180664  | saving best model ...\n",
      "Current loss:  3.1090352535247803  | , previous best loss:  3.1096630096435547  | saving best model ...\n",
      "Current loss:  3.1084084510803223  | , previous best loss:  3.1090352535247803  | saving best model ...\n",
      "Current loss:  3.107781410217285  | , previous best loss:  3.1084084510803223  | saving best model ...\n",
      "Current loss:  3.1071550846099854  | , previous best loss:  3.107781410217285  | saving best model ...\n",
      "Current loss:  3.1065289974212646  | , previous best loss:  3.1071550846099854  | saving best model ...\n",
      "Current loss:  3.1059038639068604  | , previous best loss:  3.1065289974212646  | saving best model ...\n",
      "Current loss:  3.105278730392456  | , previous best loss:  3.1059038639068604  | saving best model ...\n",
      "Current loss:  3.104654312133789  | , previous best loss:  3.105278730392456  | saving best model ...\n",
      "Current loss:  3.104029893875122  | , previous best loss:  3.104654312133789  | saving best model ...\n",
      "Current loss:  3.1034059524536133  | , previous best loss:  3.104029893875122  | saving best model ...\n",
      "Current loss:  3.102782964706421  | , previous best loss:  3.1034059524536133  | saving best model ...\n",
      "Current loss:  3.1021599769592285  | , previous best loss:  3.102782964706421  | saving best model ...\n",
      "Current loss:  3.1015377044677734  | , previous best loss:  3.1021599769592285  | saving best model ...\n",
      "Current loss:  3.10091495513916  | , previous best loss:  3.1015377044677734  | saving best model ...\n",
      "Current loss:  3.1002936363220215  | , previous best loss:  3.10091495513916  | saving best model ...\n",
      "Current loss:  3.099672317504883  | , previous best loss:  3.1002936363220215  | saving best model ...\n",
      "Current loss:  3.0990514755249023  | , previous best loss:  3.099672317504883  | saving best model ...\n",
      "Current loss:  3.098431348800659  | , previous best loss:  3.0990514755249023  | saving best model ...\n",
      "Current loss:  3.097811460494995  | , previous best loss:  3.098431348800659  | saving best model ...\n",
      "Current loss:  3.0971920490264893  | , previous best loss:  3.097811460494995  | saving best model ...\n",
      "Current loss:  3.0965726375579834  | , previous best loss:  3.0971920490264893  | saving best model ...\n",
      "Current loss:  3.095954418182373  | , previous best loss:  3.0965726375579834  | saving best model ...\n",
      "Current loss:  3.0953357219696045  | , previous best loss:  3.095954418182373  | saving best model ...\n",
      "Current loss:  3.0947182178497314  | , previous best loss:  3.0953357219696045  | saving best model ...\n",
      "Current loss:  3.0941004753112793  | , previous best loss:  3.0947182178497314  | saving best model ...\n",
      "Current loss:  3.0934834480285645  | , previous best loss:  3.0941004753112793  | saving best model ...\n",
      "Current loss:  3.092866897583008  | , previous best loss:  3.0934834480285645  | saving best model ...\n",
      "Current loss:  3.0922510623931885  | , previous best loss:  3.092866897583008  | saving best model ...\n",
      "Current loss:  3.091635227203369  | , previous best loss:  3.0922510623931885  | saving best model ...\n",
      "Current loss:  3.091019868850708  | , previous best loss:  3.091635227203369  | saving best model ...\n",
      "Current loss:  3.090404987335205  | , previous best loss:  3.091019868850708  | saving best model ...\n",
      "Current loss:  3.0897903442382812  | , previous best loss:  3.090404987335205  | saving best model ...\n",
      "Current loss:  3.0891761779785156  | , previous best loss:  3.0897903442382812  | saving best model ...\n",
      "Current loss:  3.088562488555908  | , previous best loss:  3.0891761779785156  | saving best model ...\n",
      "Current loss:  3.087949752807617  | , previous best loss:  3.088562488555908  | saving best model ...\n",
      "Current loss:  3.087336778640747  | , previous best loss:  3.087949752807617  | saving best model ...\n",
      "Current loss:  3.086724042892456  | , previous best loss:  3.087336778640747  | saving best model ...\n",
      "Current loss:  3.0861120223999023  | , previous best loss:  3.086724042892456  | saving best model ...\n",
      "Current loss:  3.085500478744507  | , previous best loss:  3.0861120223999023  | saving best model ...\n",
      "Current loss:  3.0848896503448486  | , previous best loss:  3.085500478744507  | saving best model ...\n",
      "Current loss:  3.0842788219451904  | , previous best loss:  3.0848896503448486  | saving best model ...\n",
      "Current loss:  3.0836684703826904  | , previous best loss:  3.0842788219451904  | saving best model ...\n",
      "Current loss:  3.0830583572387695  | , previous best loss:  3.0836684703826904  | saving best model ...\n",
      "Current loss:  3.082448959350586  | , previous best loss:  3.0830583572387695  | saving best model ...\n",
      "Current loss:  3.0818397998809814  | , previous best loss:  3.082448959350586  | saving best model ...\n",
      "Current loss:  3.081230878829956  | , previous best loss:  3.0818397998809814  | saving best model ...\n",
      "Current loss:  3.080622673034668  | , previous best loss:  3.081230878829956  | saving best model ...\n",
      "Current loss:  3.080014944076538  | , previous best loss:  3.080622673034668  | saving best model ...\n",
      "Current loss:  3.079407215118408  | , previous best loss:  3.080014944076538  | saving best model ...\n",
      "Current loss:  3.0787999629974365  | , previous best loss:  3.079407215118408  | saving best model ...\n",
      "Current loss:  3.078193426132202  | , previous best loss:  3.0787999629974365  | saving best model ...\n",
      "Current loss:  3.077587366104126  | , previous best loss:  3.078193426132202  | saving best model ...\n",
      "Current loss:  3.076981544494629  | , previous best loss:  3.077587366104126  | saving best model ...\n",
      "Current loss:  3.0763754844665527  | , previous best loss:  3.076981544494629  | saving best model ...\n",
      "Current loss:  3.075770616531372  | , previous best loss:  3.0763754844665527  | saving best model ...\n",
      "Current loss:  3.0751659870147705  | , previous best loss:  3.075770616531372  | saving best model ...\n",
      "Current loss:  3.074561595916748  | , previous best loss:  3.0751659870147705  | saving best model ...\n",
      "Current loss:  3.073957681655884  | , previous best loss:  3.074561595916748  | saving best model ...\n",
      "Current loss:  3.0733542442321777  | , previous best loss:  3.073957681655884  | saving best model ...\n",
      "Current loss:  3.07275128364563  | , previous best loss:  3.0733542442321777  | saving best model ...\n",
      "Current loss:  3.072148323059082  | , previous best loss:  3.07275128364563  | saving best model ...\n",
      "Current loss:  3.0715460777282715  | , previous best loss:  3.072148323059082  | saving best model ...\n",
      "Current loss:  3.070944309234619  | , previous best loss:  3.0715460777282715  | saving best model ...\n",
      "Current loss:  3.070343017578125  | , previous best loss:  3.070944309234619  | saving best model ...\n",
      "Current loss:  3.069741725921631  | , previous best loss:  3.070343017578125  | saving best model ...\n",
      "Current loss:  3.069140911102295  | , previous best loss:  3.069741725921631  | saving best model ...\n",
      "Current loss:  3.068539619445801  | , previous best loss:  3.069140911102295  | saving best model ...\n",
      "Current loss:  3.0679378509521484  | , previous best loss:  3.068539619445801  | saving best model ...\n",
      "Current loss:  3.067335844039917  | , previous best loss:  3.0679378509521484  | saving best model ...\n",
      "Current loss:  3.0667343139648438  | , previous best loss:  3.067335844039917  | saving best model ...\n",
      "Current loss:  3.0661327838897705  | , previous best loss:  3.0667343139648438  | saving best model ...\n",
      "Current loss:  3.0655319690704346  | , previous best loss:  3.0661327838897705  | saving best model ...\n",
      "Current loss:  3.0649309158325195  | , previous best loss:  3.0655319690704346  | saving best model ...\n",
      "Current loss:  3.0643303394317627  | , previous best loss:  3.0649309158325195  | saving best model ...\n",
      "Current loss:  3.063730239868164  | , previous best loss:  3.0643303394317627  | saving best model ...\n",
      "Current loss:  3.0631303787231445  | , previous best loss:  3.063730239868164  | saving best model ...\n",
      "Current loss:  3.062530517578125  | , previous best loss:  3.0631303787231445  | saving best model ...\n",
      "Current loss:  3.0619313716888428  | , previous best loss:  3.062530517578125  | saving best model ...\n",
      "Current loss:  3.0613324642181396  | , previous best loss:  3.0619313716888428  | saving best model ...\n",
      "Current loss:  3.0607337951660156  | , previous best loss:  3.0613324642181396  | saving best model ...\n",
      "Current loss:  3.0601353645324707  | , previous best loss:  3.0607337951660156  | saving best model ...\n",
      "Current loss:  3.059536933898926  | , previous best loss:  3.0601353645324707  | saving best model ...\n",
      "Current loss:  3.0589375495910645  | , previous best loss:  3.059536933898926  | saving best model ...\n",
      "Current loss:  3.0583386421203613  | , previous best loss:  3.0589375495910645  | saving best model ...\n",
      "Current loss:  3.0577392578125  | , previous best loss:  3.0583386421203613  | saving best model ...\n",
      "Current loss:  3.057140350341797  | , previous best loss:  3.0577392578125  | saving best model ...\n",
      "Current loss:  3.056541681289673  | , previous best loss:  3.057140350341797  | saving best model ...\n",
      "Current loss:  3.055943250656128  | , previous best loss:  3.056541681289673  | saving best model ...\n",
      "Current loss:  3.055345296859741  | , previous best loss:  3.055943250656128  | saving best model ...\n",
      "Current loss:  3.0547475814819336  | , previous best loss:  3.055345296859741  | saving best model ...\n",
      "Current loss:  3.054150104522705  | , previous best loss:  3.0547475814819336  | saving best model ...\n",
      "Current loss:  3.0535526275634766  | , previous best loss:  3.054150104522705  | saving best model ...\n",
      "Current loss:  3.0529563426971436  | , previous best loss:  3.0535526275634766  | saving best model ...\n",
      "Current loss:  3.0523598194122314  | , previous best loss:  3.0529563426971436  | saving best model ...\n",
      "Current loss:  3.0517642498016357  | , previous best loss:  3.0523598194122314  | saving best model ...\n",
      "Current loss:  3.051168203353882  | , previous best loss:  3.0517642498016357  | saving best model ...\n",
      "Current loss:  3.0505728721618652  | , previous best loss:  3.051168203353882  | saving best model ...\n",
      "Current loss:  3.0499775409698486  | , previous best loss:  3.0505728721618652  | saving best model ...\n",
      "Current loss:  3.0493829250335693  | , previous best loss:  3.0499775409698486  | saving best model ...\n",
      "Current loss:  3.048788547515869  | , previous best loss:  3.0493829250335693  | saving best model ...\n",
      "Current loss:  3.0481948852539062  | , previous best loss:  3.048788547515869  | saving best model ...\n",
      "Current loss:  3.047600746154785  | , previous best loss:  3.0481948852539062  | saving best model ...\n",
      "Current loss:  3.0470077991485596  | , previous best loss:  3.047600746154785  | saving best model ...\n",
      "Current loss:  3.046414852142334  | , previous best loss:  3.0470077991485596  | saving best model ...\n",
      "Current loss:  3.045822858810425  | , previous best loss:  3.046414852142334  | saving best model ...\n",
      "Current loss:  3.0452308654785156  | , previous best loss:  3.045822858810425  | saving best model ...\n",
      "Current loss:  3.0446391105651855  | , previous best loss:  3.0452308654785156  | saving best model ...\n",
      "Current loss:  3.0440478324890137  | , previous best loss:  3.0446391105651855  | saving best model ...\n",
      "Current loss:  3.043457269668579  | , previous best loss:  3.0440478324890137  | saving best model ...\n",
      "Current loss:  3.0428671836853027  | , previous best loss:  3.043457269668579  | saving best model ...\n",
      "Current loss:  3.0422768592834473  | , previous best loss:  3.0428671836853027  | saving best model ...\n",
      "Current loss:  3.0416879653930664  | , previous best loss:  3.0422768592834473  | saving best model ...\n",
      "Current loss:  3.041098117828369  | , previous best loss:  3.0416879653930664  | saving best model ...\n",
      "Current loss:  3.0405094623565674  | , previous best loss:  3.041098117828369  | saving best model ...\n",
      "Current loss:  3.0399210453033447  | , previous best loss:  3.0405094623565674  | saving best model ...\n",
      "Current loss:  3.0393331050872803  | , previous best loss:  3.0399210453033447  | saving best model ...\n",
      "Current loss:  3.038745641708374  | , previous best loss:  3.0393331050872803  | saving best model ...\n",
      "Current loss:  3.038158655166626  | , previous best loss:  3.038745641708374  | saving best model ...\n",
      "Current loss:  3.037571907043457  | , previous best loss:  3.038158655166626  | saving best model ...\n",
      "Current loss:  3.036985397338867  | , previous best loss:  3.037571907043457  | saving best model ...\n",
      "Current loss:  3.0363996028900146  | , previous best loss:  3.036985397338867  | saving best model ...\n",
      "Current loss:  3.0358142852783203  | , previous best loss:  3.0363996028900146  | saving best model ...\n",
      "Current loss:  3.035228967666626  | , previous best loss:  3.0358142852783203  | saving best model ...\n",
      "Current loss:  3.034644603729248  | , previous best loss:  3.035228967666626  | saving best model ...\n",
      "Current loss:  3.034060001373291  | , previous best loss:  3.034644603729248  | saving best model ...\n",
      "Current loss:  3.0334765911102295  | , previous best loss:  3.034060001373291  | saving best model ...\n",
      "Current loss:  3.0328927040100098  | , previous best loss:  3.0334765911102295  | saving best model ...\n",
      "Current loss:  3.0323100090026855  | , previous best loss:  3.0328927040100098  | saving best model ...\n",
      "Current loss:  3.0317270755767822  | , previous best loss:  3.0323100090026855  | saving best model ...\n",
      "Current loss:  3.0311450958251953  | , previous best loss:  3.0317270755767822  | saving best model ...\n",
      "Current loss:  3.0305628776550293  | , previous best loss:  3.0311450958251953  | saving best model ...\n",
      "Current loss:  3.0299816131591797  | , previous best loss:  3.0305628776550293  | saving best model ...\n",
      "Current loss:  3.02940034866333  | , previous best loss:  3.0299816131591797  | saving best model ...\n",
      "Current loss:  3.0288197994232178  | , previous best loss:  3.02940034866333  | saving best model ...\n",
      "Current loss:  3.0282392501831055  | , previous best loss:  3.0288197994232178  | saving best model ...\n",
      "Current loss:  3.0276594161987305  | , previous best loss:  3.0282392501831055  | saving best model ...\n",
      "Current loss:  3.0270800590515137  | , previous best loss:  3.0276594161987305  | saving best model ...\n",
      "Current loss:  3.026500701904297  | , previous best loss:  3.0270800590515137  | saving best model ...\n",
      "Current loss:  3.0259222984313965  | , previous best loss:  3.026500701904297  | saving best model ...\n",
      "Current loss:  3.025344133377075  | , previous best loss:  3.0259222984313965  | saving best model ...\n",
      "Current loss:  3.024766206741333  | , previous best loss:  3.025344133377075  | saving best model ...\n",
      "Current loss:  3.024188280105591  | , previous best loss:  3.024766206741333  | saving best model ...\n",
      "Current loss:  3.023611307144165  | , previous best loss:  3.024188280105591  | saving best model ...\n",
      "Current loss:  3.0230345726013184  | , previous best loss:  3.023611307144165  | saving best model ...\n",
      "Current loss:  3.022458553314209  | , previous best loss:  3.0230345726013184  | saving best model ...\n",
      "Current loss:  3.0218822956085205  | , previous best loss:  3.022458553314209  | saving best model ...\n",
      "Current loss:  3.0213067531585693  | , previous best loss:  3.0218822956085205  | saving best model ...\n",
      "Current loss:  3.0207316875457764  | , previous best loss:  3.0213067531585693  | saving best model ...\n",
      "Current loss:  3.0201566219329834  | , previous best loss:  3.0207316875457764  | saving best model ...\n",
      "Current loss:  3.019582509994507  | , previous best loss:  3.0201566219329834  | saving best model ...\n",
      "Current loss:  3.0190083980560303  | , previous best loss:  3.019582509994507  | saving best model ...\n",
      "Current loss:  3.018434524536133  | , previous best loss:  3.0190083980560303  | saving best model ...\n",
      "Current loss:  3.0178616046905518  | , previous best loss:  3.018434524536133  | saving best model ...\n",
      "Current loss:  3.0172884464263916  | , previous best loss:  3.0178616046905518  | saving best model ...\n",
      "Current loss:  3.0167160034179688  | , previous best loss:  3.0172884464263916  | saving best model ...\n",
      "Current loss:  3.016144037246704  | , previous best loss:  3.0167160034179688  | saving best model ...\n",
      "Current loss:  3.0155723094940186  | , previous best loss:  3.016144037246704  | saving best model ...\n",
      "Current loss:  3.0150012969970703  | , previous best loss:  3.0155723094940186  | saving best model ...\n",
      "Current loss:  3.0144295692443848  | , previous best loss:  3.0150012969970703  | saving best model ...\n",
      "Current loss:  3.0138590335845947  | , previous best loss:  3.0144295692443848  | saving best model ...\n",
      "Current loss:  3.013289213180542  | , previous best loss:  3.0138590335845947  | saving best model ...\n",
      "Current loss:  3.01271915435791  | , previous best loss:  3.013289213180542  | saving best model ...\n",
      "Current loss:  3.0121493339538574  | , previous best loss:  3.01271915435791  | saving best model ...\n",
      "Current loss:  3.0115807056427  | , previous best loss:  3.0121493339538574  | saving best model ...\n",
      "Current loss:  3.011012077331543  | , previous best loss:  3.0115807056427  | saving best model ...\n",
      "Current loss:  3.010443687438965  | , previous best loss:  3.011012077331543  | saving best model ...\n",
      "Current loss:  3.009875774383545  | , previous best loss:  3.010443687438965  | saving best model ...\n",
      "Current loss:  3.009307861328125  | , previous best loss:  3.009875774383545  | saving best model ...\n",
      "Current loss:  3.0087409019470215  | , previous best loss:  3.009307861328125  | saving best model ...\n",
      "Current loss:  3.008174419403076  | , previous best loss:  3.0087409019470215  | saving best model ...\n",
      "Current loss:  3.007607936859131  | , previous best loss:  3.008174419403076  | saving best model ...\n",
      "Current loss:  3.0070414543151855  | , previous best loss:  3.007607936859131  | saving best model ...\n",
      "Current loss:  3.006476402282715  | , previous best loss:  3.0070414543151855  | saving best model ...\n",
      "Current loss:  3.005910873413086  | , previous best loss:  3.006476402282715  | saving best model ...\n",
      "Current loss:  3.0053458213806152  | , previous best loss:  3.005910873413086  | saving best model ...\n",
      "Current loss:  3.004781484603882  | , previous best loss:  3.0053458213806152  | saving best model ...\n",
      "Current loss:  3.0042169094085693  | , previous best loss:  3.004781484603882  | saving best model ...\n",
      "Current loss:  3.0036532878875732  | , previous best loss:  3.0042169094085693  | saving best model ...\n",
      "Current loss:  3.0030899047851562  | , previous best loss:  3.0036532878875732  | saving best model ...\n",
      "Current loss:  3.00252628326416  | , previous best loss:  3.0030899047851562  | saving best model ...\n",
      "Current loss:  3.0019640922546387  | , previous best loss:  3.00252628326416  | saving best model ...\n",
      "Current loss:  3.001401662826538  | , previous best loss:  3.0019640922546387  | saving best model ...\n",
      "Current loss:  3.000839948654175  | , previous best loss:  3.001401662826538  | saving best model ...\n",
      "Current loss:  3.0002782344818115  | , previous best loss:  3.000839948654175  | saving best model ...\n",
      "Current loss:  2.9997167587280273  | , previous best loss:  3.0002782344818115  | saving best model ...\n",
      "Current loss:  2.9991559982299805  | , previous best loss:  2.9997167587280273  | saving best model ...\n",
      "Current loss:  2.9985954761505127  | , previous best loss:  2.9991559982299805  | saving best model ...\n",
      "Current loss:  2.9980356693267822  | , previous best loss:  2.9985954761505127  | saving best model ...\n",
      "Current loss:  2.9974756240844727  | , previous best loss:  2.9980356693267822  | saving best model ...\n",
      "Current loss:  2.9969165325164795  | , previous best loss:  2.9974756240844727  | saving best model ...\n",
      "Current loss:  2.9963572025299072  | , previous best loss:  2.9969165325164795  | saving best model ...\n",
      "Current loss:  2.9957988262176514  | , previous best loss:  2.9963572025299072  | saving best model ...\n",
      "Current loss:  2.9952399730682373  | , previous best loss:  2.9957988262176514  | saving best model ...\n",
      "Current loss:  2.9946820735931396  | , previous best loss:  2.9952399730682373  | saving best model ...\n",
      "Current loss:  2.994124412536621  | , previous best loss:  2.9946820735931396  | saving best model ...\n",
      "Current loss:  2.99356746673584  | , previous best loss:  2.994124412536621  | saving best model ...\n",
      "Current loss:  2.9930107593536377  | , previous best loss:  2.99356746673584  | saving best model ...\n",
      "Current loss:  2.9924540519714355  | , previous best loss:  2.9930107593536377  | saving best model ...\n",
      "Current loss:  2.9918980598449707  | , previous best loss:  2.9924540519714355  | saving best model ...\n",
      "Current loss:  2.991342306137085  | , previous best loss:  2.9918980598449707  | saving best model ...\n",
      "Current loss:  2.990786552429199  | , previous best loss:  2.991342306137085  | saving best model ...\n",
      "Current loss:  2.99023175239563  | , previous best loss:  2.990786552429199  | saving best model ...\n",
      "Current loss:  2.9896771907806396  | , previous best loss:  2.99023175239563  | saving best model ...\n",
      "Current loss:  2.9891228675842285  | , previous best loss:  2.9896771907806396  | saving best model ...\n",
      "Current loss:  2.9885687828063965  | , previous best loss:  2.9891228675842285  | saving best model ...\n",
      "Current loss:  2.9880149364471436  | , previous best loss:  2.9885687828063965  | saving best model ...\n",
      "Current loss:  2.987461805343628  | , previous best loss:  2.9880149364471436  | saving best model ...\n",
      "Current loss:  2.9869091510772705  | , previous best loss:  2.987461805343628  | saving best model ...\n",
      "Current loss:  2.986356258392334  | , previous best loss:  2.9869091510772705  | saving best model ...\n",
      "Current loss:  2.985804557800293  | , previous best loss:  2.986356258392334  | saving best model ...\n",
      "Current loss:  2.9852523803710938  | , previous best loss:  2.985804557800293  | saving best model ...\n",
      "Current loss:  2.984700918197632  | , previous best loss:  2.9852523803710938  | saving best model ...\n",
      "Current loss:  2.984149932861328  | , previous best loss:  2.984700918197632  | saving best model ...\n",
      "Current loss:  2.9835991859436035  | , previous best loss:  2.984149932861328  | saving best model ...\n",
      "Current loss:  2.983048915863037  | , previous best loss:  2.9835991859436035  | saving best model ...\n",
      "Current loss:  2.9824986457824707  | , previous best loss:  2.983048915863037  | saving best model ...\n",
      "Current loss:  2.9819486141204834  | , previous best loss:  2.9824986457824707  | saving best model ...\n",
      "Current loss:  2.9813997745513916  | , previous best loss:  2.9819486141204834  | saving best model ...\n",
      "Current loss:  2.9808504581451416  | , previous best loss:  2.9813997745513916  | saving best model ...\n",
      "Current loss:  2.980302333831787  | , previous best loss:  2.9808504581451416  | saving best model ...\n",
      "Current loss:  2.9797534942626953  | , previous best loss:  2.980302333831787  | saving best model ...\n",
      "Current loss:  2.979205846786499  | , previous best loss:  2.9797534942626953  | saving best model ...\n",
      "Current loss:  2.9786579608917236  | , previous best loss:  2.979205846786499  | saving best model ...\n",
      "Current loss:  2.9781105518341064  | , previous best loss:  2.9786579608917236  | saving best model ...\n",
      "Current loss:  2.9775636196136475  | , previous best loss:  2.9781105518341064  | saving best model ...\n",
      "Current loss:  2.977017641067505  | , previous best loss:  2.9775636196136475  | saving best model ...\n",
      "Current loss:  2.976471185684204  | , previous best loss:  2.977017641067505  | saving best model ...\n",
      "Current loss:  2.9759256839752197  | , previous best loss:  2.976471185684204  | saving best model ...\n",
      "Current loss:  2.9753799438476562  | , previous best loss:  2.9759256839752197  | saving best model ...\n",
      "Current loss:  2.97483491897583  | , previous best loss:  2.9753799438476562  | saving best model ...\n",
      "Current loss:  2.974290132522583  | , previous best loss:  2.97483491897583  | saving best model ...\n",
      "Current loss:  2.973745584487915  | , previous best loss:  2.974290132522583  | saving best model ...\n",
      "Current loss:  2.9732015132904053  | , previous best loss:  2.973745584487915  | saving best model ...\n",
      "Current loss:  2.9726572036743164  | , previous best loss:  2.9732015132904053  | saving best model ...\n",
      "Current loss:  2.9721133708953857  | , previous best loss:  2.9726572036743164  | saving best model ...\n",
      "Current loss:  2.9715700149536133  | , previous best loss:  2.9721133708953857  | saving best model ...\n",
      "Current loss:  2.971027135848999  | , previous best loss:  2.9715700149536133  | saving best model ...\n",
      "Current loss:  2.9704842567443848  | , previous best loss:  2.971027135848999  | saving best model ...\n",
      "Current loss:  2.9699418544769287  | , previous best loss:  2.9704842567443848  | saving best model ...\n",
      "Current loss:  2.969399929046631  | , previous best loss:  2.9699418544769287  | saving best model ...\n",
      "Current loss:  2.968858242034912  | , previous best loss:  2.969399929046631  | saving best model ...\n",
      "Current loss:  2.9683167934417725  | , previous best loss:  2.968858242034912  | saving best model ...\n",
      "Current loss:  2.967775583267212  | , previous best loss:  2.9683167934417725  | saving best model ...\n",
      "Current loss:  2.9672343730926514  | , previous best loss:  2.967775583267212  | saving best model ...\n",
      "Current loss:  2.9666945934295654  | , previous best loss:  2.9672343730926514  | saving best model ...\n",
      "Current loss:  2.9661545753479004  | , previous best loss:  2.9666945934295654  | saving best model ...\n",
      "Current loss:  2.9656147956848145  | , previous best loss:  2.9661545753479004  | saving best model ...\n",
      "Current loss:  2.9650750160217285  | , previous best loss:  2.9656147956848145  | saving best model ...\n",
      "Current loss:  2.964536190032959  | , previous best loss:  2.9650750160217285  | saving best model ...\n",
      "Current loss:  2.9639976024627686  | , previous best loss:  2.964536190032959  | saving best model ...\n",
      "Current loss:  2.963459014892578  | , previous best loss:  2.9639976024627686  | saving best model ...\n",
      "Current loss:  2.962921142578125  | , previous best loss:  2.963459014892578  | saving best model ...\n",
      "Current loss:  2.96238374710083  | , previous best loss:  2.962921142578125  | saving best model ...\n",
      "Current loss:  2.961846113204956  | , previous best loss:  2.96238374710083  | saving best model ...\n",
      "Current loss:  2.9613094329833984  | , previous best loss:  2.961846113204956  | saving best model ...\n",
      "Current loss:  2.9607725143432617  | , previous best loss:  2.9613094329833984  | saving best model ...\n",
      "Current loss:  2.9602363109588623  | , previous best loss:  2.9607725143432617  | saving best model ...\n",
      "Current loss:  2.959700107574463  | , previous best loss:  2.9602363109588623  | saving best model ...\n",
      "Current loss:  2.959164619445801  | , previous best loss:  2.959700107574463  | saving best model ...\n",
      "Current loss:  2.9586291313171387  | , previous best loss:  2.959164619445801  | saving best model ...\n",
      "Current loss:  2.9580941200256348  | , previous best loss:  2.9586291313171387  | saving best model ...\n",
      "Current loss:  2.957559585571289  | , previous best loss:  2.9580941200256348  | saving best model ...\n",
      "Current loss:  2.9570252895355225  | , previous best loss:  2.957559585571289  | saving best model ...\n",
      "Current loss:  2.956491470336914  | , previous best loss:  2.9570252895355225  | saving best model ...\n",
      "Current loss:  2.9559578895568848  | , previous best loss:  2.956491470336914  | saving best model ...\n",
      "Current loss:  2.9554245471954346  | , previous best loss:  2.9559578895568848  | saving best model ...\n",
      "Current loss:  2.9548916816711426  | , previous best loss:  2.9554245471954346  | saving best model ...\n",
      "Current loss:  2.9543590545654297  | , previous best loss:  2.9548916816711426  | saving best model ...\n",
      "Current loss:  2.953826904296875  | , previous best loss:  2.9543590545654297  | saving best model ...\n",
      "Current loss:  2.9532949924468994  | , previous best loss:  2.953826904296875  | saving best model ...\n",
      "Current loss:  2.952763080596924  | , previous best loss:  2.9532949924468994  | saving best model ...\n",
      "Current loss:  2.9522318840026855  | , previous best loss:  2.952763080596924  | saving best model ...\n",
      "Current loss:  2.9517011642456055  | , previous best loss:  2.9522318840026855  | saving best model ...\n",
      "Current loss:  2.9511704444885254  | , previous best loss:  2.9517011642456055  | saving best model ...\n",
      "Current loss:  2.9506404399871826  | , previous best loss:  2.9511704444885254  | saving best model ...\n",
      "Current loss:  2.95011043548584  | , previous best loss:  2.9506404399871826  | saving best model ...\n",
      "Current loss:  2.949580669403076  | , previous best loss:  2.95011043548584  | saving best model ...\n",
      "Current loss:  2.949051856994629  | , previous best loss:  2.949580669403076  | saving best model ...\n",
      "Current loss:  2.9485228061676025  | , previous best loss:  2.949051856994629  | saving best model ...\n",
      "Current loss:  2.9479942321777344  | , previous best loss:  2.9485228061676025  | saving best model ...\n",
      "Current loss:  2.9474661350250244  | , previous best loss:  2.9479942321777344  | saving best model ...\n",
      "Current loss:  2.9469382762908936  | , previous best loss:  2.9474661350250244  | saving best model ...\n",
      "Current loss:  2.946410894393921  | , previous best loss:  2.9469382762908936  | saving best model ...\n",
      "Current loss:  2.945883274078369  | , previous best loss:  2.946410894393921  | saving best model ...\n",
      "Current loss:  2.9453561305999756  | , previous best loss:  2.945883274078369  | saving best model ...\n",
      "Current loss:  2.9448301792144775  | , previous best loss:  2.9453561305999756  | saving best model ...\n",
      "Current loss:  2.9443039894104004  | , previous best loss:  2.9448301792144775  | saving best model ...\n",
      "Current loss:  2.9437777996063232  | , previous best loss:  2.9443039894104004  | saving best model ...\n",
      "Current loss:  2.9432523250579834  | , previous best loss:  2.9437777996063232  | saving best model ...\n",
      "Current loss:  2.9427270889282227  | , previous best loss:  2.9432523250579834  | saving best model ...\n",
      "Current loss:  2.942202091217041  | , previous best loss:  2.9427270889282227  | saving best model ...\n",
      "Current loss:  2.9416778087615967  | , previous best loss:  2.942202091217041  | saving best model ...\n",
      "Current loss:  2.9411532878875732  | , previous best loss:  2.9416778087615967  | saving best model ...\n",
      "Current loss:  2.940629720687866  | , previous best loss:  2.9411532878875732  | saving best model ...\n",
      "Current loss:  2.940106153488159  | , previous best loss:  2.940629720687866  | saving best model ...\n",
      "Current loss:  2.9395828247070312  | , previous best loss:  2.940106153488159  | saving best model ...\n",
      "Current loss:  2.9390599727630615  | , previous best loss:  2.9395828247070312  | saving best model ...\n",
      "Current loss:  2.938537120819092  | , previous best loss:  2.9390599727630615  | saving best model ...\n",
      "Current loss:  2.9380149841308594  | , previous best loss:  2.938537120819092  | saving best model ...\n",
      "Current loss:  2.937493324279785  | , previous best loss:  2.9380149841308594  | saving best model ...\n",
      "Current loss:  2.936971664428711  | , previous best loss:  2.937493324279785  | saving best model ...\n",
      "Current loss:  2.936450242996216  | , previous best loss:  2.936971664428711  | saving best model ...\n",
      "Current loss:  2.935929775238037  | , previous best loss:  2.936450242996216  | saving best model ...\n",
      "Current loss:  2.9354088306427  | , previous best loss:  2.935929775238037  | saving best model ...\n",
      "Current loss:  2.934889078140259  | , previous best loss:  2.9354088306427  | saving best model ...\n",
      "Current loss:  2.934368848800659  | , previous best loss:  2.934889078140259  | saving best model ...\n",
      "Current loss:  2.9338490962982178  | , previous best loss:  2.934368848800659  | saving best model ...\n",
      "Current loss:  2.9333300590515137  | , previous best loss:  2.9338490962982178  | saving best model ...\n",
      "Current loss:  2.9328110218048096  | , previous best loss:  2.9333300590515137  | saving best model ...\n",
      "Current loss:  2.9322926998138428  | , previous best loss:  2.9328110218048096  | saving best model ...\n",
      "Current loss:  2.931774139404297  | , previous best loss:  2.9322926998138428  | saving best model ...\n",
      "Current loss:  2.9312562942504883  | , previous best loss:  2.931774139404297  | saving best model ...\n",
      "Current loss:  2.930738687515259  | , previous best loss:  2.9312562942504883  | saving best model ...\n",
      "Current loss:  2.9302217960357666  | , previous best loss:  2.930738687515259  | saving best model ...\n",
      "Current loss:  2.929704427719116  | , previous best loss:  2.9302217960357666  | saving best model ...\n",
      "Current loss:  2.929187774658203  | , previous best loss:  2.929704427719116  | saving best model ...\n",
      "Current loss:  2.9286715984344482  | , previous best loss:  2.929187774658203  | saving best model ...\n",
      "Current loss:  2.9281554222106934  | , previous best loss:  2.9286715984344482  | saving best model ...\n",
      "Current loss:  2.927639961242676  | , previous best loss:  2.9281554222106934  | saving best model ...\n",
      "Current loss:  2.927124500274658  | , previous best loss:  2.927639961242676  | saving best model ...\n",
      "Current loss:  2.9266092777252197  | , previous best loss:  2.927124500274658  | saving best model ...\n",
      "Current loss:  2.9260950088500977  | , previous best loss:  2.9266092777252197  | saving best model ...\n",
      "Current loss:  2.9255802631378174  | , previous best loss:  2.9260950088500977  | saving best model ...\n",
      "Current loss:  2.9250664710998535  | , previous best loss:  2.9255802631378174  | saving best model ...\n",
      "Current loss:  2.9245526790618896  | , previous best loss:  2.9250664710998535  | saving best model ...\n",
      "Current loss:  2.924039125442505  | , previous best loss:  2.9245526790618896  | saving best model ...\n",
      "Current loss:  2.9235262870788574  | , previous best loss:  2.924039125442505  | saving best model ...\n",
      "Current loss:  2.923013687133789  | , previous best loss:  2.9235262870788574  | saving best model ...\n",
      "Current loss:  2.9225013256073  | , previous best loss:  2.923013687133789  | saving best model ...\n",
      "Current loss:  2.9219892024993896  | , previous best loss:  2.9225013256073  | saving best model ...\n",
      "Current loss:  2.9214773178100586  | , previous best loss:  2.9219892024993896  | saving best model ...\n",
      "Current loss:  2.9209659099578857  | , previous best loss:  2.9214773178100586  | saving best model ...\n",
      "Current loss:  2.920454740524292  | , previous best loss:  2.9209659099578857  | saving best model ...\n",
      "Current loss:  2.9199438095092773  | , previous best loss:  2.920454740524292  | saving best model ...\n",
      "Current loss:  2.91943359375  | , previous best loss:  2.9199438095092773  | saving best model ...\n",
      "Current loss:  2.9189236164093018  | , previous best loss:  2.91943359375  | saving best model ...\n",
      "Current loss:  2.9184136390686035  | , previous best loss:  2.9189236164093018  | saving best model ...\n",
      "Current loss:  2.9179039001464844  | , previous best loss:  2.9184136390686035  | saving best model ...\n",
      "Current loss:  2.9173951148986816  | , previous best loss:  2.9179039001464844  | saving best model ...\n",
      "Current loss:  2.916886329650879  | , previous best loss:  2.9173951148986816  | saving best model ...\n",
      "Current loss:  2.916377544403076  | , previous best loss:  2.916886329650879  | saving best model ...\n",
      "Current loss:  2.9158694744110107  | , previous best loss:  2.916377544403076  | saving best model ...\n",
      "Current loss:  2.9153614044189453  | , previous best loss:  2.9158694744110107  | saving best model ...\n",
      "Current loss:  2.914854049682617  | , previous best loss:  2.9153614044189453  | saving best model ...\n",
      "Current loss:  2.914346933364868  | , previous best loss:  2.914854049682617  | saving best model ...\n",
      "Current loss:  2.91383957862854  | , previous best loss:  2.914346933364868  | saving best model ...\n",
      "Current loss:  2.913332939147949  | , previous best loss:  2.91383957862854  | saving best model ...\n",
      "Current loss:  2.9128260612487793  | , previous best loss:  2.913332939147949  | saving best model ...\n",
      "Current loss:  2.9123153686523438  | , previous best loss:  2.9128260612487793  | saving best model ...\n",
      "Current loss:  2.911803960800171  | , previous best loss:  2.9123153686523438  | saving best model ...\n",
      "Current loss:  2.911292314529419  | , previous best loss:  2.911803960800171  | saving best model ...\n",
      "Current loss:  2.910780191421509  | , previous best loss:  2.911292314529419  | saving best model ...\n",
      "Current loss:  2.9102683067321777  | , previous best loss:  2.910780191421509  | saving best model ...\n",
      "Current loss:  2.9097561836242676  | , previous best loss:  2.9102683067321777  | saving best model ...\n",
      "Current loss:  2.9092445373535156  | , previous best loss:  2.9097561836242676  | saving best model ...\n",
      "Current loss:  2.9087326526641846  | , previous best loss:  2.9092445373535156  | saving best model ...\n",
      "Current loss:  2.9082212448120117  | , previous best loss:  2.9087326526641846  | saving best model ...\n",
      "Current loss:  2.9077093601226807  | , previous best loss:  2.9082212448120117  | saving best model ...\n",
      "Current loss:  2.907198190689087  | , previous best loss:  2.9077093601226807  | saving best model ...\n",
      "Current loss:  2.906686782836914  | , previous best loss:  2.907198190689087  | saving best model ...\n",
      "Current loss:  2.9061758518218994  | , previous best loss:  2.906686782836914  | saving best model ...\n",
      "Current loss:  2.9056646823883057  | , previous best loss:  2.9061758518218994  | saving best model ...\n",
      "Current loss:  2.905153512954712  | , previous best loss:  2.9056646823883057  | saving best model ...\n",
      "Current loss:  2.904642343521118  | , previous best loss:  2.905153512954712  | saving best model ...\n",
      "Current loss:  2.9041316509246826  | , previous best loss:  2.904642343521118  | saving best model ...\n",
      "Current loss:  2.9036190509796143  | , previous best loss:  2.9041316509246826  | saving best model ...\n",
      "Current loss:  2.9031033515930176  | , previous best loss:  2.9036190509796143  | saving best model ...\n",
      "Current loss:  2.9025869369506836  | , previous best loss:  2.9031033515930176  | saving best model ...\n",
      "Current loss:  2.9020702838897705  | , previous best loss:  2.9025869369506836  | saving best model ...\n",
      "Current loss:  2.901553153991699  | , previous best loss:  2.9020702838897705  | saving best model ...\n",
      "Current loss:  2.9010355472564697  | , previous best loss:  2.901553153991699  | saving best model ...\n",
      "Current loss:  2.9005184173583984  | , previous best loss:  2.9010355472564697  | saving best model ...\n",
      "Current loss:  2.9000003337860107  | , previous best loss:  2.9005184173583984  | saving best model ...\n",
      "Current loss:  2.8994827270507812  | , previous best loss:  2.9000003337860107  | saving best model ...\n",
      "Current loss:  2.8989646434783936  | , previous best loss:  2.8994827270507812  | saving best model ...\n",
      "Current loss:  2.898447036743164  | , previous best loss:  2.8989646434783936  | saving best model ...\n",
      "Current loss:  2.8979294300079346  | , previous best loss:  2.898447036743164  | saving best model ...\n",
      "Current loss:  2.897411584854126  | , previous best loss:  2.8979294300079346  | saving best model ...\n",
      "Current loss:  2.8968944549560547  | , previous best loss:  2.897411584854126  | saving best model ...\n",
      "Current loss:  2.8963770866394043  | , previous best loss:  2.8968944549560547  | saving best model ...\n",
      "Current loss:  2.895859956741333  | , previous best loss:  2.8963770866394043  | saving best model ...\n",
      "Current loss:  2.895343065261841  | , previous best loss:  2.895859956741333  | saving best model ...\n",
      "Current loss:  2.8948261737823486  | , previous best loss:  2.895343065261841  | saving best model ...\n",
      "Current loss:  2.8943097591400146  | , previous best loss:  2.8948261737823486  | saving best model ...\n",
      "Current loss:  2.8937931060791016  | , previous best loss:  2.8943097591400146  | saving best model ...\n",
      "Current loss:  2.8932769298553467  | , previous best loss:  2.8937931060791016  | saving best model ...\n",
      "Current loss:  2.89276123046875  | , previous best loss:  2.8932769298553467  | saving best model ...\n",
      "Current loss:  2.892245292663574  | , previous best loss:  2.89276123046875  | saving best model ...\n",
      "Current loss:  2.8917300701141357  | , previous best loss:  2.892245292663574  | saving best model ...\n",
      "Current loss:  2.8912153244018555  | , previous best loss:  2.8917300701141357  | saving best model ...\n",
      "Current loss:  2.890700101852417  | , previous best loss:  2.8912153244018555  | saving best model ...\n",
      "Current loss:  2.890186071395874  | , previous best loss:  2.890700101852417  | saving best model ...\n",
      "Current loss:  2.889671802520752  | , previous best loss:  2.890186071395874  | saving best model ...\n",
      "Current loss:  2.889158010482788  | , previous best loss:  2.889671802520752  | saving best model ...\n",
      "Current loss:  2.8886444568634033  | , previous best loss:  2.889158010482788  | saving best model ...\n",
      "Current loss:  2.8881311416625977  | , previous best loss:  2.8886444568634033  | saving best model ...\n",
      "Current loss:  2.88761830329895  | , previous best loss:  2.8881311416625977  | saving best model ...\n",
      "Current loss:  2.887105703353882  | , previous best loss:  2.88761830329895  | saving best model ...\n",
      "Current loss:  2.8865933418273926  | , previous best loss:  2.887105703353882  | saving best model ...\n",
      "Current loss:  2.8860812187194824  | , previous best loss:  2.8865933418273926  | saving best model ...\n",
      "Current loss:  2.8855698108673096  | , previous best loss:  2.8860812187194824  | saving best model ...\n",
      "Current loss:  2.8850584030151367  | , previous best loss:  2.8855698108673096  | saving best model ...\n",
      "Current loss:  2.884547472000122  | , previous best loss:  2.8850584030151367  | saving best model ...\n",
      "Current loss:  2.8840363025665283  | , previous best loss:  2.884547472000122  | saving best model ...\n",
      "Current loss:  2.88352632522583  | , previous best loss:  2.8840363025665283  | saving best model ...\n",
      "Current loss:  2.883016347885132  | , previous best loss:  2.88352632522583  | saving best model ...\n",
      "Current loss:  2.8825066089630127  | , previous best loss:  2.883016347885132  | saving best model ...\n",
      "Current loss:  2.881997585296631  | , previous best loss:  2.8825066089630127  | saving best model ...\n",
      "Current loss:  2.881488084793091  | , previous best loss:  2.881997585296631  | saving best model ...\n",
      "Current loss:  2.880979537963867  | , previous best loss:  2.881488084793091  | saving best model ...\n",
      "Current loss:  2.8804712295532227  | , previous best loss:  2.880979537963867  | saving best model ...\n",
      "Current loss:  2.8799631595611572  | , previous best loss:  2.8804712295532227  | saving best model ...\n",
      "Current loss:  2.87945556640625  | , previous best loss:  2.8799631595611572  | saving best model ...\n",
      "Current loss:  2.878948211669922  | , previous best loss:  2.87945556640625  | saving best model ...\n",
      "Current loss:  2.878441095352173  | , previous best loss:  2.878948211669922  | saving best model ...\n",
      "Current loss:  2.877933979034424  | , previous best loss:  2.878441095352173  | saving best model ...\n",
      "Current loss:  2.877427816390991  | , previous best loss:  2.877933979034424  | saving best model ...\n",
      "Current loss:  2.876922130584717  | , previous best loss:  2.877427816390991  | saving best model ...\n",
      "Current loss:  2.8764164447784424  | , previous best loss:  2.876922130584717  | saving best model ...\n",
      "Current loss:  2.875911235809326  | , previous best loss:  2.8764164447784424  | saving best model ...\n",
      "Current loss:  2.875405788421631  | , previous best loss:  2.875911235809326  | saving best model ...\n",
      "Current loss:  2.874901056289673  | , previous best loss:  2.875405788421631  | saving best model ...\n",
      "Current loss:  2.874396800994873  | , previous best loss:  2.874901056289673  | saving best model ...\n",
      "Current loss:  2.873892307281494  | , previous best loss:  2.874396800994873  | saving best model ...\n",
      "Current loss:  2.8733890056610107  | , previous best loss:  2.873892307281494  | saving best model ...\n",
      "Current loss:  2.8728854656219482  | , previous best loss:  2.8733890056610107  | saving best model ...\n",
      "Current loss:  2.872382402420044  | , previous best loss:  2.8728854656219482  | saving best model ...\n",
      "Current loss:  2.8718793392181396  | , previous best loss:  2.872382402420044  | saving best model ...\n",
      "Current loss:  2.8713772296905518  | , previous best loss:  2.8718793392181396  | saving best model ...\n",
      "Current loss:  2.8708748817443848  | , previous best loss:  2.8713772296905518  | saving best model ...\n",
      "Current loss:  2.870373010635376  | , previous best loss:  2.8708748817443848  | saving best model ...\n",
      "Current loss:  2.8698718547821045  | , previous best loss:  2.870373010635376  | saving best model ...\n",
      "Current loss:  2.869370937347412  | , previous best loss:  2.8698718547821045  | saving best model ...\n",
      "Current loss:  2.8688700199127197  | , previous best loss:  2.869370937347412  | saving best model ...\n",
      "Current loss:  2.8683695793151855  | , previous best loss:  2.8688700199127197  | saving best model ...\n",
      "Current loss:  2.8678696155548096  | , previous best loss:  2.8683695793151855  | saving best model ...\n",
      "Current loss:  2.8673696517944336  | , previous best loss:  2.8678696155548096  | saving best model ...\n",
      "Current loss:  2.8668699264526367  | , previous best loss:  2.8673696517944336  | saving best model ...\n",
      "Current loss:  2.866370677947998  | , previous best loss:  2.8668699264526367  | saving best model ...\n",
      "Current loss:  2.8658721446990967  | , previous best loss:  2.866370677947998  | saving best model ...\n",
      "Current loss:  2.8653736114501953  | , previous best loss:  2.8658721446990967  | saving best model ...\n",
      "Current loss:  2.864875316619873  | , previous best loss:  2.8653736114501953  | saving best model ...\n",
      "Current loss:  2.864377498626709  | , previous best loss:  2.864875316619873  | saving best model ...\n",
      "Current loss:  2.863879919052124  | , previous best loss:  2.864377498626709  | saving best model ...\n",
      "Current loss:  2.863382577896118  | , previous best loss:  2.863879919052124  | saving best model ...\n",
      "Current loss:  2.8628857135772705  | , previous best loss:  2.863382577896118  | saving best model ...\n",
      "Current loss:  2.862389326095581  | , previous best loss:  2.8628857135772705  | saving best model ...\n",
      "Current loss:  2.8618931770324707  | , previous best loss:  2.862389326095581  | saving best model ...\n",
      "Current loss:  2.8613972663879395  | , previous best loss:  2.8618931770324707  | saving best model ...\n",
      "Current loss:  2.8609015941619873  | , previous best loss:  2.8613972663879395  | saving best model ...\n",
      "Current loss:  2.8604061603546143  | , previous best loss:  2.8609015941619873  | saving best model ...\n",
      "Current loss:  2.8599114418029785  | , previous best loss:  2.8604061603546143  | saving best model ...\n",
      "Current loss:  2.8594167232513428  | , previous best loss:  2.8599114418029785  | saving best model ...\n",
      "Current loss:  2.8589224815368652  | , previous best loss:  2.8594167232513428  | saving best model ...\n",
      "Current loss:  2.858428478240967  | , previous best loss:  2.8589224815368652  | saving best model ...\n",
      "Current loss:  2.8579349517822266  | , previous best loss:  2.858428478240967  | saving best model ...\n",
      "Current loss:  2.8574414253234863  | , previous best loss:  2.8579349517822266  | saving best model ...\n",
      "Current loss:  2.8569486141204834  | , previous best loss:  2.8574414253234863  | saving best model ...\n",
      "Current loss:  2.8564560413360596  | , previous best loss:  2.8569486141204834  | saving best model ...\n",
      "Current loss:  2.8559634685516357  | , previous best loss:  2.8564560413360596  | saving best model ...\n",
      "Current loss:  2.85547137260437  | , previous best loss:  2.8559634685516357  | saving best model ...\n",
      "Current loss:  2.8549795150756836  | , previous best loss:  2.85547137260437  | saving best model ...\n",
      "Current loss:  2.8544881343841553  | , previous best loss:  2.8549795150756836  | saving best model ...\n",
      "Current loss:  2.853996992111206  | , previous best loss:  2.8544881343841553  | saving best model ...\n",
      "Current loss:  2.853506326675415  | , previous best loss:  2.853996992111206  | saving best model ...\n",
      "Current loss:  2.8530161380767822  | , previous best loss:  2.853506326675415  | saving best model ...\n",
      "Current loss:  2.8525257110595703  | , previous best loss:  2.8530161380767822  | saving best model ...\n",
      "Current loss:  2.852036237716675  | , previous best loss:  2.8525257110595703  | saving best model ...\n",
      "Current loss:  2.851546287536621  | , previous best loss:  2.852036237716675  | saving best model ...\n",
      "Current loss:  2.851057291030884  | , previous best loss:  2.851546287536621  | saving best model ...\n",
      "Current loss:  2.8505685329437256  | , previous best loss:  2.851057291030884  | saving best model ...\n",
      "Current loss:  2.8500802516937256  | , previous best loss:  2.8505685329437256  | saving best model ...\n",
      "Current loss:  2.8495917320251465  | , previous best loss:  2.8500802516937256  | saving best model ...\n",
      "Current loss:  2.8491039276123047  | , previous best loss:  2.8495917320251465  | saving best model ...\n",
      "Current loss:  2.848616123199463  | , previous best loss:  2.8491039276123047  | saving best model ...\n",
      "Current loss:  2.8481290340423584  | , previous best loss:  2.848616123199463  | saving best model ...\n",
      "Current loss:  2.847642421722412  | , previous best loss:  2.8481290340423584  | saving best model ...\n",
      "Current loss:  2.847155809402466  | , previous best loss:  2.847642421722412  | saving best model ...\n",
      "Current loss:  2.8466691970825195  | , previous best loss:  2.847155809402466  | saving best model ...\n",
      "Current loss:  2.8461833000183105  | , previous best loss:  2.8466691970825195  | saving best model ...\n",
      "Current loss:  2.8456978797912598  | , previous best loss:  2.8461833000183105  | saving best model ...\n",
      "Current loss:  2.845212459564209  | , previous best loss:  2.8456978797912598  | saving best model ...\n",
      "Current loss:  2.8447275161743164  | , previous best loss:  2.845212459564209  | saving best model ...\n",
      "Current loss:  2.8442423343658447  | , previous best loss:  2.8447275161743164  | saving best model ...\n",
      "Current loss:  2.8437583446502686  | , previous best loss:  2.8442423343658447  | saving best model ...\n",
      "Current loss:  2.8432743549346924  | , previous best loss:  2.8437583446502686  | saving best model ...\n",
      "Current loss:  2.842790365219116  | , previous best loss:  2.8432743549346924  | saving best model ...\n",
      "Current loss:  2.8423070907592773  | , previous best loss:  2.842790365219116  | saving best model ...\n",
      "Current loss:  2.8418238162994385  | , previous best loss:  2.8423070907592773  | saving best model ...\n",
      "Current loss:  2.8413407802581787  | , previous best loss:  2.8418238162994385  | saving best model ...\n",
      "Current loss:  2.8408584594726562  | , previous best loss:  2.8413407802581787  | saving best model ...\n",
      "Current loss:  2.840376377105713  | , previous best loss:  2.8408584594726562  | saving best model ...\n",
      "Current loss:  2.8398945331573486  | , previous best loss:  2.840376377105713  | saving best model ...\n",
      "Current loss:  2.8394129276275635  | , previous best loss:  2.8398945331573486  | saving best model ...\n",
      "Current loss:  2.8389315605163574  | , previous best loss:  2.8394129276275635  | saving best model ...\n",
      "Current loss:  2.8384509086608887  | , previous best loss:  2.8389315605163574  | saving best model ...\n",
      "Current loss:  2.83797025680542  | , previous best loss:  2.8384509086608887  | saving best model ...\n",
      "Current loss:  2.8374900817871094  | , previous best loss:  2.83797025680542  | saving best model ...\n",
      "Current loss:  2.837009906768799  | , previous best loss:  2.8374900817871094  | saving best model ...\n",
      "Current loss:  2.8365304470062256  | , previous best loss:  2.837009906768799  | saving best model ...\n",
      "Current loss:  2.8360509872436523  | , previous best loss:  2.8365304470062256  | saving best model ...\n",
      "Current loss:  2.8355720043182373  | , previous best loss:  2.8360509872436523  | saving best model ...\n",
      "Current loss:  2.8350934982299805  | , previous best loss:  2.8355720043182373  | saving best model ...\n",
      "Current loss:  2.8346149921417236  | , previous best loss:  2.8350934982299805  | saving best model ...\n",
      "Current loss:  2.834136962890625  | , previous best loss:  2.8346149921417236  | saving best model ...\n",
      "Current loss:  2.8336591720581055  | , previous best loss:  2.834136962890625  | saving best model ...\n",
      "Current loss:  2.833181619644165  | , previous best loss:  2.8336591720581055  | saving best model ...\n",
      "Current loss:  2.832704544067383  | , previous best loss:  2.833181619644165  | saving best model ...\n",
      "Current loss:  2.8322277069091797  | , previous best loss:  2.832704544067383  | saving best model ...\n",
      "Current loss:  2.8317511081695557  | , previous best loss:  2.8322277069091797  | saving best model ...\n",
      "Current loss:  2.83127498626709  | , previous best loss:  2.8317511081695557  | saving best model ...\n",
      "Current loss:  2.8307993412017822  | , previous best loss:  2.83127498626709  | saving best model ...\n",
      "Current loss:  2.8303236961364746  | , previous best loss:  2.8307993412017822  | saving best model ...\n",
      "Current loss:  2.829848289489746  | , previous best loss:  2.8303236961364746  | saving best model ...\n",
      "Current loss:  2.829373598098755  | , previous best loss:  2.829848289489746  | saving best model ...\n",
      "Current loss:  2.8288991451263428  | , previous best loss:  2.829373598098755  | saving best model ...\n",
      "Current loss:  2.8284244537353516  | , previous best loss:  2.8288991451263428  | saving best model ...\n",
      "Current loss:  2.8279507160186768  | , previous best loss:  2.8284244537353516  | saving best model ...\n",
      "Current loss:  2.827476739883423  | , previous best loss:  2.8279507160186768  | saving best model ...\n",
      "Current loss:  2.8270037174224854  | , previous best loss:  2.827476739883423  | saving best model ...\n",
      "Current loss:  2.8265304565429688  | , previous best loss:  2.8270037174224854  | saving best model ...\n",
      "Current loss:  2.8260579109191895  | , previous best loss:  2.8265304565429688  | saving best model ...\n",
      "Current loss:  2.82558536529541  | , previous best loss:  2.8260579109191895  | saving best model ...\n",
      "Current loss:  2.82511305809021  | , previous best loss:  2.82558536529541  | saving best model ...\n",
      "Current loss:  2.824641704559326  | , previous best loss:  2.82511305809021  | saving best model ...\n",
      "Current loss:  2.824169874191284  | , previous best loss:  2.824641704559326  | saving best model ...\n",
      "Current loss:  2.8236989974975586  | , previous best loss:  2.824169874191284  | saving best model ...\n",
      "Current loss:  2.823227882385254  | , previous best loss:  2.8236989974975586  | saving best model ...\n",
      "Current loss:  2.8227572441101074  | , previous best loss:  2.823227882385254  | saving best model ...\n",
      "Current loss:  2.82228684425354  | , previous best loss:  2.8227572441101074  | saving best model ...\n",
      "Current loss:  2.82181715965271  | , previous best loss:  2.82228684425354  | saving best model ...\n",
      "Current loss:  2.82134747505188  | , previous best loss:  2.82181715965271  | saving best model ...\n",
      "Current loss:  2.820878028869629  | , previous best loss:  2.82134747505188  | saving best model ...\n",
      "Current loss:  2.8204095363616943  | , previous best loss:  2.820878028869629  | saving best model ...\n",
      "Current loss:  2.8199403285980225  | , previous best loss:  2.8204095363616943  | saving best model ...\n",
      "Current loss:  2.819472074508667  | , previous best loss:  2.8199403285980225  | saving best model ...\n",
      "Current loss:  2.8190038204193115  | , previous best loss:  2.819472074508667  | saving best model ...\n",
      "Current loss:  2.8185360431671143  | , previous best loss:  2.8190038204193115  | saving best model ...\n",
      "Current loss:  2.818068742752075  | , previous best loss:  2.8185360431671143  | saving best model ...\n",
      "Current loss:  2.8176016807556152  | , previous best loss:  2.818068742752075  | saving best model ...\n",
      "Current loss:  2.8171348571777344  | , previous best loss:  2.8176016807556152  | saving best model ...\n",
      "Current loss:  2.8166685104370117  | , previous best loss:  2.8171348571777344  | saving best model ...\n",
      "Current loss:  2.81620192527771  | , previous best loss:  2.8166685104370117  | saving best model ...\n",
      "Current loss:  2.8157358169555664  | , previous best loss:  2.81620192527771  | saving best model ...\n",
      "Current loss:  2.81527042388916  | , previous best loss:  2.8157358169555664  | saving best model ...\n",
      "Current loss:  2.814805269241333  | , previous best loss:  2.81527042388916  | saving best model ...\n",
      "Current loss:  2.814340353012085  | , previous best loss:  2.814805269241333  | saving best model ...\n",
      "Current loss:  2.813875436782837  | , previous best loss:  2.814340353012085  | saving best model ...\n",
      "Current loss:  2.813411235809326  | , previous best loss:  2.813875436782837  | saving best model ...\n",
      "Current loss:  2.8129470348358154  | , previous best loss:  2.813411235809326  | saving best model ...\n",
      "Current loss:  2.812483310699463  | , previous best loss:  2.8129470348358154  | saving best model ...\n",
      "Current loss:  2.8120200634002686  | , previous best loss:  2.812483310699463  | saving best model ...\n",
      "Current loss:  2.811556577682495  | , previous best loss:  2.8120200634002686  | saving best model ...\n",
      "Current loss:  2.811094045639038  | , previous best loss:  2.811556577682495  | saving best model ...\n",
      "Current loss:  2.810631275177002  | , previous best loss:  2.811094045639038  | saving best model ...\n",
      "Current loss:  2.8101694583892822  | , previous best loss:  2.810631275177002  | saving best model ...\n",
      "Current loss:  2.8097071647644043  | , previous best loss:  2.8101694583892822  | saving best model ...\n",
      "Current loss:  2.8092458248138428  | , previous best loss:  2.8097071647644043  | saving best model ...\n",
      "Current loss:  2.808784246444702  | , previous best loss:  2.8092458248138428  | saving best model ...\n",
      "Current loss:  2.8083231449127197  | , previous best loss:  2.808784246444702  | saving best model ...\n",
      "Current loss:  2.8078625202178955  | , previous best loss:  2.8083231449127197  | saving best model ...\n",
      "Current loss:  2.8074021339416504  | , previous best loss:  2.8078625202178955  | saving best model ...\n",
      "Current loss:  2.8069419860839844  | , previous best loss:  2.8074021339416504  | saving best model ...\n",
      "Current loss:  2.8064823150634766  | , previous best loss:  2.8069419860839844  | saving best model ...\n",
      "Current loss:  2.806022882461548  | , previous best loss:  2.8064823150634766  | saving best model ...\n",
      "Current loss:  2.8055636882781982  | , previous best loss:  2.806022882461548  | saving best model ...\n",
      "Current loss:  2.805105209350586  | , previous best loss:  2.8055636882781982  | saving best model ...\n",
      "Current loss:  2.8046462535858154  | , previous best loss:  2.805105209350586  | saving best model ...\n",
      "Current loss:  2.8041882514953613  | , previous best loss:  2.8046462535858154  | saving best model ...\n",
      "Current loss:  2.8037302494049072  | , previous best loss:  2.8041882514953613  | saving best model ...\n",
      "Current loss:  2.8032727241516113  | , previous best loss:  2.8037302494049072  | saving best model ...\n",
      "Current loss:  2.8028151988983154  | , previous best loss:  2.8032727241516113  | saving best model ...\n",
      "Current loss:  2.8023581504821777  | , previous best loss:  2.8028151988983154  | saving best model ...\n",
      "Current loss:  2.8019015789031982  | , previous best loss:  2.8023581504821777  | saving best model ...\n",
      "Current loss:  2.8014450073242188  | , previous best loss:  2.8019015789031982  | saving best model ...\n",
      "Current loss:  2.8009889125823975  | , previous best loss:  2.8014450073242188  | saving best model ...\n",
      "Current loss:  2.8005332946777344  | , previous best loss:  2.8009889125823975  | saving best model ...\n",
      "Current loss:  2.8000776767730713  | , previous best loss:  2.8005332946777344  | saving best model ...\n",
      "Current loss:  2.7996225357055664  | , previous best loss:  2.8000776767730713  | saving best model ...\n",
      "Current loss:  2.7991673946380615  | , previous best loss:  2.7996225357055664  | saving best model ...\n",
      "Current loss:  2.798712730407715  | , previous best loss:  2.7991673946380615  | saving best model ...\n",
      "Current loss:  2.7982585430145264  | , previous best loss:  2.798712730407715  | saving best model ...\n",
      "Current loss:  2.797804355621338  | , previous best loss:  2.7982585430145264  | saving best model ...\n",
      "Current loss:  2.7973508834838867  | , previous best loss:  2.797804355621338  | saving best model ...\n",
      "Current loss:  2.7968974113464355  | , previous best loss:  2.7973508834838867  | saving best model ...\n",
      "Current loss:  2.7964444160461426  | , previous best loss:  2.7968974113464355  | saving best model ...\n",
      "Current loss:  2.7959916591644287  | , previous best loss:  2.7964444160461426  | saving best model ...\n",
      "Current loss:  2.795539140701294  | , previous best loss:  2.7959916591644287  | saving best model ...\n",
      "Current loss:  2.7950868606567383  | , previous best loss:  2.795539140701294  | saving best model ...\n",
      "Current loss:  2.794635057449341  | , previous best loss:  2.7950868606567383  | saving best model ...\n",
      "Current loss:  2.7941834926605225  | , previous best loss:  2.794635057449341  | saving best model ...\n",
      "Current loss:  2.793731927871704  | , previous best loss:  2.7941834926605225  | saving best model ...\n",
      "Current loss:  2.793281316757202  | , previous best loss:  2.793731927871704  | saving best model ...\n",
      "Current loss:  2.792830228805542  | , previous best loss:  2.793281316757202  | saving best model ...\n",
      "Current loss:  2.792379856109619  | , previous best loss:  2.792830228805542  | saving best model ...\n",
      "Current loss:  2.7919299602508545  | , previous best loss:  2.792379856109619  | saving best model ...\n",
      "Current loss:  2.791480541229248  | , previous best loss:  2.7919299602508545  | saving best model ...\n",
      "Current loss:  2.7910306453704834  | , previous best loss:  2.791480541229248  | saving best model ...\n",
      "Current loss:  2.790581226348877  | , previous best loss:  2.7910306453704834  | saving best model ...\n",
      "Current loss:  2.7901322841644287  | , previous best loss:  2.790581226348877  | saving best model ...\n",
      "Current loss:  2.7896840572357178  | , previous best loss:  2.7901322841644287  | saving best model ...\n",
      "Current loss:  2.7892353534698486  | , previous best loss:  2.7896840572357178  | saving best model ...\n",
      "Current loss:  2.788787603378296  | , previous best loss:  2.7892353534698486  | saving best model ...\n",
      "Current loss:  2.788339853286743  | , previous best loss:  2.788787603378296  | saving best model ...\n",
      "Current loss:  2.7878921031951904  | , previous best loss:  2.788339853286743  | saving best model ...\n",
      "Current loss:  2.787445545196533  | , previous best loss:  2.7878921031951904  | saving best model ...\n",
      "Current loss:  2.7869982719421387  | , previous best loss:  2.787445545196533  | saving best model ...\n",
      "Current loss:  2.7865519523620605  | , previous best loss:  2.7869982719421387  | saving best model ...\n",
      "Current loss:  2.7861053943634033  | , previous best loss:  2.7865519523620605  | saving best model ...\n",
      "Current loss:  2.7856595516204834  | , previous best loss:  2.7861053943634033  | saving best model ...\n",
      "Current loss:  2.7852141857147217  | , previous best loss:  2.7856595516204834  | saving best model ...\n",
      "Current loss:  2.784768581390381  | , previous best loss:  2.7852141857147217  | saving best model ...\n",
      "Current loss:  2.7843236923217773  | , previous best loss:  2.784768581390381  | saving best model ...\n",
      "Current loss:  2.7838785648345947  | , previous best loss:  2.7843236923217773  | saving best model ...\n",
      "Current loss:  2.7834346294403076  | , previous best loss:  2.7838785648345947  | saving best model ...\n",
      "Current loss:  2.7829902172088623  | , previous best loss:  2.7834346294403076  | saving best model ...\n",
      "Current loss:  2.782545804977417  | , previous best loss:  2.7829902172088623  | saving best model ...\n",
      "Current loss:  2.7821028232574463  | , previous best loss:  2.782545804977417  | saving best model ...\n",
      "Current loss:  2.7816591262817383  | , previous best loss:  2.7821028232574463  | saving best model ...\n",
      "Current loss:  2.7812163829803467  | , previous best loss:  2.7816591262817383  | saving best model ...\n",
      "Current loss:  2.780773401260376  | , previous best loss:  2.7812163829803467  | saving best model ...\n",
      "Current loss:  2.7803311347961426  | , previous best loss:  2.780773401260376  | saving best model ...\n",
      "Current loss:  2.779888868331909  | , previous best loss:  2.7803311347961426  | saving best model ...\n",
      "Current loss:  2.779447317123413  | , previous best loss:  2.779888868331909  | saving best model ...\n",
      "Current loss:  2.779005765914917  | , previous best loss:  2.779447317123413  | saving best model ...\n",
      "Current loss:  2.778564453125  | , previous best loss:  2.779005765914917  | saving best model ...\n",
      "Current loss:  2.778123617172241  | , previous best loss:  2.778564453125  | saving best model ...\n",
      "Current loss:  2.7776830196380615  | , previous best loss:  2.778123617172241  | saving best model ...\n",
      "Current loss:  2.777242422103882  | , previous best loss:  2.7776830196380615  | saving best model ...\n",
      "Current loss:  2.7768023014068604  | , previous best loss:  2.777242422103882  | saving best model ...\n",
      "Current loss:  2.776362657546997  | , previous best loss:  2.7768023014068604  | saving best model ...\n",
      "Current loss:  2.7759227752685547  | , previous best loss:  2.776362657546997  | saving best model ...\n",
      "Current loss:  2.7754836082458496  | , previous best loss:  2.7759227752685547  | saving best model ...\n",
      "Current loss:  2.7750449180603027  | , previous best loss:  2.7754836082458496  | saving best model ...\n",
      "Current loss:  2.774606227874756  | , previous best loss:  2.7750449180603027  | saving best model ...\n",
      "Current loss:  2.774168014526367  | , previous best loss:  2.774606227874756  | saving best model ...\n",
      "Current loss:  2.7737298011779785  | , previous best loss:  2.774168014526367  | saving best model ...\n",
      "Current loss:  2.773292064666748  | , previous best loss:  2.7737298011779785  | saving best model ...\n",
      "Current loss:  2.7728545665740967  | , previous best loss:  2.773292064666748  | saving best model ...\n",
      "Current loss:  2.7724175453186035  | , previous best loss:  2.7728545665740967  | saving best model ...\n",
      "Current loss:  2.7719805240631104  | , previous best loss:  2.7724175453186035  | saving best model ...\n",
      "Current loss:  2.7715439796447754  | , previous best loss:  2.7719805240631104  | saving best model ...\n",
      "Current loss:  2.7711074352264404  | , previous best loss:  2.7715439796447754  | saving best model ...\n",
      "Current loss:  2.7706713676452637  | , previous best loss:  2.7711074352264404  | saving best model ...\n",
      "Current loss:  2.770235300064087  | , previous best loss:  2.7706713676452637  | saving best model ...\n",
      "Current loss:  2.7697999477386475  | , previous best loss:  2.770235300064087  | saving best model ...\n",
      "Current loss:  2.769364833831787  | , previous best loss:  2.7697999477386475  | saving best model ...\n",
      "Current loss:  2.768929958343506  | , previous best loss:  2.769364833831787  | saving best model ...\n",
      "Current loss:  2.7684950828552246  | , previous best loss:  2.768929958343506  | saving best model ...\n",
      "Current loss:  2.7680609226226807  | , previous best loss:  2.7684950828552246  | saving best model ...\n",
      "Current loss:  2.7676267623901367  | , previous best loss:  2.7680609226226807  | saving best model ...\n",
      "Current loss:  2.767192840576172  | , previous best loss:  2.7676267623901367  | saving best model ...\n",
      "Current loss:  2.7667598724365234  | , previous best loss:  2.767192840576172  | saving best model ...\n",
      "Current loss:  2.766326665878296  | , previous best loss:  2.7667598724365234  | saving best model ...\n",
      "Current loss:  2.7658932209014893  | , previous best loss:  2.766326665878296  | saving best model ...\n",
      "Current loss:  2.765460729598999  | , previous best loss:  2.7658932209014893  | saving best model ...\n",
      "Current loss:  2.765028238296509  | , previous best loss:  2.765460729598999  | saving best model ...\n",
      "Current loss:  2.7645959854125977  | , previous best loss:  2.765028238296509  | saving best model ...\n",
      "Current loss:  2.764164447784424  | , previous best loss:  2.7645959854125977  | saving best model ...\n",
      "Current loss:  2.76373291015625  | , previous best loss:  2.764164447784424  | saving best model ...\n",
      "Current loss:  2.7633016109466553  | , previous best loss:  2.76373291015625  | saving best model ...\n",
      "Current loss:  2.7628707885742188  | , previous best loss:  2.7633016109466553  | saving best model ...\n",
      "Current loss:  2.7624399662017822  | , previous best loss:  2.7628707885742188  | saving best model ...\n",
      "Current loss:  2.762009382247925  | , previous best loss:  2.7624399662017822  | saving best model ...\n",
      "Current loss:  2.7615792751312256  | , previous best loss:  2.762009382247925  | saving best model ...\n",
      "Current loss:  2.7611494064331055  | , previous best loss:  2.7615792751312256  | saving best model ...\n",
      "Current loss:  2.7607202529907227  | , previous best loss:  2.7611494064331055  | saving best model ...\n",
      "Current loss:  2.7602908611297607  | , previous best loss:  2.7607202529907227  | saving best model ...\n",
      "Current loss:  2.759861707687378  | , previous best loss:  2.7602908611297607  | saving best model ...\n",
      "Current loss:  2.7594330310821533  | , previous best loss:  2.759861707687378  | saving best model ...\n",
      "Current loss:  2.7590043544769287  | , previous best loss:  2.7594330310821533  | saving best model ...\n",
      "Current loss:  2.7585763931274414  | , previous best loss:  2.7590043544769287  | saving best model ...\n",
      "Current loss:  2.758148431777954  | , previous best loss:  2.7585763931274414  | saving best model ...\n",
      "Current loss:  2.757720470428467  | , previous best loss:  2.758148431777954  | saving best model ...\n",
      "Current loss:  2.757293224334717  | , previous best loss:  2.757720470428467  | saving best model ...\n",
      "Current loss:  2.756865978240967  | , previous best loss:  2.757293224334717  | saving best model ...\n",
      "Current loss:  2.756439447402954  | , previous best loss:  2.756865978240967  | saving best model ...\n",
      "Current loss:  2.7560129165649414  | , previous best loss:  2.756439447402954  | saving best model ...\n",
      "Current loss:  2.755586624145508  | , previous best loss:  2.7560129165649414  | saving best model ...\n",
      "Current loss:  2.7551605701446533  | , previous best loss:  2.755586624145508  | saving best model ...\n",
      "Current loss:  2.754734754562378  | , previous best loss:  2.7551605701446533  | saving best model ...\n",
      "Current loss:  2.7543094158172607  | , previous best loss:  2.754734754562378  | saving best model ...\n",
      "Current loss:  2.7538843154907227  | , previous best loss:  2.7543094158172607  | saving best model ...\n",
      "Current loss:  2.7534594535827637  | , previous best loss:  2.7538843154907227  | saving best model ...\n",
      "Current loss:  2.7530345916748047  | , previous best loss:  2.7534594535827637  | saving best model ...\n",
      "Current loss:  2.752610445022583  | , previous best loss:  2.7530345916748047  | saving best model ...\n",
      "Current loss:  2.7521862983703613  | , previous best loss:  2.752610445022583  | saving best model ...\n",
      "Current loss:  2.751762628555298  | , previous best loss:  2.7521862983703613  | saving best model ...\n",
      "Current loss:  2.7513391971588135  | , previous best loss:  2.751762628555298  | saving best model ...\n",
      "Current loss:  2.750915765762329  | , previous best loss:  2.7513391971588135  | saving best model ...\n",
      "Current loss:  2.750492811203003  | , previous best loss:  2.750915765762329  | saving best model ...\n",
      "Current loss:  2.7500698566436768  | , previous best loss:  2.750492811203003  | saving best model ...\n",
      "Current loss:  2.749647617340088  | , previous best loss:  2.7500698566436768  | saving best model ...\n",
      "Current loss:  2.749225616455078  | , previous best loss:  2.749647617340088  | saving best model ...\n",
      "Current loss:  2.7488033771514893  | , previous best loss:  2.749225616455078  | saving best model ...\n",
      "Current loss:  2.7483816146850586  | , previous best loss:  2.7488033771514893  | saving best model ...\n",
      "Current loss:  2.7479605674743652  | , previous best loss:  2.7483816146850586  | saving best model ...\n",
      "Current loss:  2.747539520263672  | , previous best loss:  2.7479605674743652  | saving best model ...\n",
      "Current loss:  2.7471184730529785  | , previous best loss:  2.747539520263672  | saving best model ...\n",
      "Current loss:  2.7466976642608643  | , previous best loss:  2.7471184730529785  | saving best model ...\n",
      "Current loss:  2.7462775707244873  | , previous best loss:  2.7466976642608643  | saving best model ...\n",
      "Current loss:  2.7458577156066895  | , previous best loss:  2.7462775707244873  | saving best model ...\n",
      "Current loss:  2.7454380989074707  | , previous best loss:  2.7458577156066895  | saving best model ...\n",
      "Current loss:  2.745018243789673  | , previous best loss:  2.7454380989074707  | saving best model ...\n",
      "Current loss:  2.744598627090454  | , previous best loss:  2.745018243789673  | saving best model ...\n",
      "Current loss:  2.744180202484131  | , previous best loss:  2.744598627090454  | saving best model ...\n",
      "Current loss:  2.7437613010406494  | , previous best loss:  2.744180202484131  | saving best model ...\n",
      "Current loss:  2.743342638015747  | , previous best loss:  2.7437613010406494  | saving best model ...\n",
      "Current loss:  2.742924213409424  | , previous best loss:  2.743342638015747  | saving best model ...\n",
      "Current loss:  2.742506504058838  | , previous best loss:  2.742924213409424  | saving best model ...\n",
      "Current loss:  2.742088794708252  | , previous best loss:  2.742506504058838  | saving best model ...\n",
      "Current loss:  2.741671323776245  | , previous best loss:  2.742088794708252  | saving best model ...\n",
      "Current loss:  2.7412540912628174  | , previous best loss:  2.741671323776245  | saving best model ...\n",
      "Current loss:  2.740837335586548  | , previous best loss:  2.7412540912628174  | saving best model ...\n",
      "Current loss:  2.7404208183288574  | , previous best loss:  2.740837335586548  | saving best model ...\n",
      "Current loss:  2.740004301071167  | , previous best loss:  2.7404208183288574  | saving best model ...\n",
      "Current loss:  2.7395882606506348  | , previous best loss:  2.740004301071167  | saving best model ...\n",
      "Current loss:  2.7391724586486816  | , previous best loss:  2.7395882606506348  | saving best model ...\n",
      "Current loss:  2.7387568950653076  | , previous best loss:  2.7391724586486816  | saving best model ...\n",
      "Current loss:  2.7383413314819336  | , previous best loss:  2.7387568950653076  | saving best model ...\n",
      "Current loss:  2.7379262447357178  | , previous best loss:  2.7383413314819336  | saving best model ...\n",
      "Current loss:  2.737511396408081  | , previous best loss:  2.7379262447357178  | saving best model ...\n",
      "Current loss:  2.7370970249176025  | , previous best loss:  2.737511396408081  | saving best model ...\n",
      "Current loss:  2.736682653427124  | , previous best loss:  2.7370970249176025  | saving best model ...\n",
      "Current loss:  2.7362685203552246  | , previous best loss:  2.736682653427124  | saving best model ...\n",
      "Current loss:  2.7358548641204834  | , previous best loss:  2.7362685203552246  | saving best model ...\n",
      "Current loss:  2.735440969467163  | , previous best loss:  2.7358548641204834  | saving best model ...\n",
      "Current loss:  2.73502779006958  | , previous best loss:  2.735440969467163  | saving best model ...\n",
      "Current loss:  2.7346150875091553  | , previous best loss:  2.73502779006958  | saving best model ...\n",
      "Current loss:  2.7342021465301514  | , previous best loss:  2.7346150875091553  | saving best model ...\n",
      "Current loss:  2.7337894439697266  | , previous best loss:  2.7342021465301514  | saving best model ...\n",
      "Current loss:  2.733377456665039  | , previous best loss:  2.7337894439697266  | saving best model ...\n",
      "Current loss:  2.7329649925231934  | , previous best loss:  2.733377456665039  | saving best model ...\n",
      "Current loss:  2.732553720474243  | , previous best loss:  2.7329649925231934  | saving best model ...\n",
      "Current loss:  2.7321419715881348  | , previous best loss:  2.732553720474243  | saving best model ...\n",
      "Current loss:  2.7317304611206055  | , previous best loss:  2.7321419715881348  | saving best model ...\n",
      "Current loss:  2.7313196659088135  | , previous best loss:  2.7317304611206055  | saving best model ...\n",
      "Current loss:  2.7309091091156006  | , previous best loss:  2.7313196659088135  | saving best model ...\n",
      "Current loss:  2.730498790740967  | , previous best loss:  2.7309091091156006  | saving best model ...\n",
      "Current loss:  2.730088472366333  | , previous best loss:  2.730498790740967  | saving best model ...\n",
      "Current loss:  2.729678153991699  | , previous best loss:  2.730088472366333  | saving best model ...\n",
      "Current loss:  2.7292685508728027  | , previous best loss:  2.729678153991699  | saving best model ...\n",
      "Current loss:  2.7288589477539062  | , previous best loss:  2.7292685508728027  | saving best model ...\n",
      "Current loss:  2.728449821472168  | , previous best loss:  2.7288589477539062  | saving best model ...\n",
      "Current loss:  2.7280404567718506  | , previous best loss:  2.728449821472168  | saving best model ...\n",
      "Current loss:  2.7276318073272705  | , previous best loss:  2.7280404567718506  | saving best model ...\n",
      "Current loss:  2.7272231578826904  | , previous best loss:  2.7276318073272705  | saving best model ...\n",
      "Current loss:  2.7268149852752686  | , previous best loss:  2.7272231578826904  | saving best model ...\n",
      "Current loss:  2.7264068126678467  | , previous best loss:  2.7268149852752686  | saving best model ...\n",
      "Current loss:  2.725999355316162  | , previous best loss:  2.7264068126678467  | saving best model ...\n",
      "Current loss:  2.7255916595458984  | , previous best loss:  2.725999355316162  | saving best model ...\n",
      "Current loss:  2.725184679031372  | , previous best loss:  2.7255916595458984  | saving best model ...\n",
      "Current loss:  2.7247769832611084  | , previous best loss:  2.725184679031372  | saving best model ...\n",
      "Current loss:  2.7243707180023193  | , previous best loss:  2.7247769832611084  | saving best model ...\n",
      "Current loss:  2.723963975906372  | , previous best loss:  2.7243707180023193  | saving best model ...\n",
      "Current loss:  2.723557710647583  | , previous best loss:  2.723963975906372  | saving best model ...\n",
      "Current loss:  2.723151445388794  | , previous best loss:  2.723557710647583  | saving best model ...\n",
      "Current loss:  2.722745656967163  | , previous best loss:  2.723151445388794  | saving best model ...\n",
      "Current loss:  2.7223401069641113  | , previous best loss:  2.722745656967163  | saving best model ...\n",
      "Current loss:  2.7219347953796387  | , previous best loss:  2.7223401069641113  | saving best model ...\n",
      "Current loss:  2.721529483795166  | , previous best loss:  2.7219347953796387  | saving best model ...\n",
      "Current loss:  2.7211244106292725  | , previous best loss:  2.721529483795166  | saving best model ...\n",
      "Current loss:  2.7207202911376953  | , previous best loss:  2.7211244106292725  | saving best model ...\n",
      "Current loss:  2.720315456390381  | , previous best loss:  2.7207202911376953  | saving best model ...\n",
      "Current loss:  2.7199113368988037  | , previous best loss:  2.720315456390381  | saving best model ...\n",
      "Current loss:  2.7195069789886475  | , previous best loss:  2.7199113368988037  | saving best model ...\n",
      "Current loss:  2.7191033363342285  | , previous best loss:  2.7195069789886475  | saving best model ...\n",
      "Current loss:  2.718700408935547  | , previous best loss:  2.7191033363342285  | saving best model ...\n",
      "Current loss:  2.718296766281128  | , previous best loss:  2.718700408935547  | saving best model ...\n",
      "Current loss:  2.7178938388824463  | , previous best loss:  2.718296766281128  | saving best model ...\n",
      "Current loss:  2.717491388320923  | , previous best loss:  2.7178938388824463  | saving best model ...\n",
      "Current loss:  2.717088460922241  | , previous best loss:  2.717491388320923  | saving best model ...\n",
      "Current loss:  2.716686487197876  | , previous best loss:  2.717088460922241  | saving best model ...\n",
      "Current loss:  2.71628475189209  | , previous best loss:  2.716686487197876  | saving best model ...\n",
      "Current loss:  2.7158827781677246  | , previous best loss:  2.71628475189209  | saving best model ...\n",
      "Current loss:  2.7154810428619385  | , previous best loss:  2.7158827781677246  | saving best model ...\n",
      "Current loss:  2.7150800228118896  | , previous best loss:  2.7154810428619385  | saving best model ...\n",
      "Current loss:  2.7146787643432617  | , previous best loss:  2.7150800228118896  | saving best model ...\n",
      "Current loss:  2.714277744293213  | , previous best loss:  2.7146787643432617  | saving best model ...\n",
      "Current loss:  2.713876962661743  | , previous best loss:  2.714277744293213  | saving best model ...\n",
      "Current loss:  2.7134768962860107  | , previous best loss:  2.713876962661743  | saving best model ...\n",
      "Current loss:  2.71307635307312  | , previous best loss:  2.7134768962860107  | saving best model ...\n",
      "Current loss:  2.712676525115967  | , previous best loss:  2.71307635307312  | saving best model ...\n",
      "Current loss:  2.7122769355773926  | , previous best loss:  2.712676525115967  | saving best model ...\n",
      "Current loss:  2.7118771076202393  | , previous best loss:  2.7122769355773926  | saving best model ...\n",
      "Current loss:  2.7114779949188232  | , previous best loss:  2.7118771076202393  | saving best model ...\n",
      "Current loss:  2.7110791206359863  | , previous best loss:  2.7114779949188232  | saving best model ...\n",
      "Current loss:  2.7106804847717285  | , previous best loss:  2.7110791206359863  | saving best model ...\n",
      "Current loss:  2.7102816104888916  | , previous best loss:  2.7106804847717285  | saving best model ...\n",
      "Current loss:  2.709883213043213  | , previous best loss:  2.7102816104888916  | saving best model ...\n",
      "Current loss:  2.709484815597534  | , previous best loss:  2.709883213043213  | saving best model ...\n",
      "Current loss:  2.7090868949890137  | , previous best loss:  2.709484815597534  | saving best model ...\n",
      "Current loss:  2.7086896896362305  | , previous best loss:  2.7090868949890137  | saving best model ...\n",
      "Current loss:  2.708292007446289  | , previous best loss:  2.7086896896362305  | saving best model ...\n",
      "Current loss:  2.707894802093506  | , previous best loss:  2.708292007446289  | saving best model ...\n",
      "Current loss:  2.707498073577881  | , previous best loss:  2.707894802093506  | saving best model ...\n",
      "Current loss:  2.7071011066436768  | , previous best loss:  2.707498073577881  | saving best model ...\n",
      "Current loss:  2.706704616546631  | , previous best loss:  2.7071011066436768  | saving best model ...\n",
      "Current loss:  2.706308126449585  | , previous best loss:  2.706704616546631  | saving best model ...\n",
      "Current loss:  2.705911874771118  | , previous best loss:  2.706308126449585  | saving best model ...\n",
      "Current loss:  2.7055163383483887  | , previous best loss:  2.705911874771118  | saving best model ...\n",
      "Current loss:  2.705120325088501  | , previous best loss:  2.7055163383483887  | saving best model ...\n",
      "Current loss:  2.7047250270843506  | , previous best loss:  2.705120325088501  | saving best model ...\n",
      "Current loss:  2.704329490661621  | , previous best loss:  2.7047250270843506  | saving best model ...\n",
      "Current loss:  2.703934669494629  | , previous best loss:  2.704329490661621  | saving best model ...\n",
      "Current loss:  2.703540086746216  | , previous best loss:  2.703934669494629  | saving best model ...\n",
      "Current loss:  2.7031452655792236  | , previous best loss:  2.703540086746216  | saving best model ...\n",
      "Current loss:  2.7027506828308105  | , previous best loss:  2.7031452655792236  | saving best model ...\n",
      "Current loss:  2.7023568153381348  | , previous best loss:  2.7027506828308105  | saving best model ...\n",
      "Current loss:  2.701963186264038  | , previous best loss:  2.7023568153381348  | saving best model ...\n",
      "Current loss:  2.7015693187713623  | , previous best loss:  2.701963186264038  | saving best model ...\n",
      "Current loss:  2.7011756896972656  | , previous best loss:  2.7015693187713623  | saving best model ...\n",
      "Current loss:  2.7007827758789062  | , previous best loss:  2.7011756896972656  | saving best model ...\n",
      "Current loss:  2.7003893852233887  | , previous best loss:  2.7007827758789062  | saving best model ...\n",
      "Current loss:  2.6999964714050293  | , previous best loss:  2.7003893852233887  | saving best model ...\n",
      "Current loss:  2.699604034423828  | , previous best loss:  2.6999964714050293  | saving best model ...\n",
      "Current loss:  2.699211597442627  | , previous best loss:  2.699604034423828  | saving best model ...\n",
      "Current loss:  2.698819398880005  | , previous best loss:  2.699211597442627  | saving best model ...\n",
      "Current loss:  2.698427200317383  | , previous best loss:  2.698819398880005  | saving best model ...\n",
      "Current loss:  2.698035478591919  | , previous best loss:  2.698427200317383  | saving best model ...\n",
      "Current loss:  2.697643756866455  | , previous best loss:  2.698035478591919  | saving best model ...\n",
      "Current loss:  2.6972529888153076  | , previous best loss:  2.697643756866455  | saving best model ...\n",
      "Current loss:  2.696861505508423  | , previous best loss:  2.6972529888153076  | saving best model ...\n",
      "Current loss:  2.6964704990386963  | , previous best loss:  2.696861505508423  | saving best model ...\n",
      "Current loss:  2.696079969406128  | , previous best loss:  2.6964704990386963  | saving best model ...\n",
      "Current loss:  2.6956892013549805  | , previous best loss:  2.696079969406128  | saving best model ...\n",
      "Current loss:  2.6952991485595703  | , previous best loss:  2.6956892013549805  | saving best model ...\n",
      "Current loss:  2.694908857345581  | , previous best loss:  2.6952991485595703  | saving best model ...\n",
      "Current loss:  2.69451904296875  | , previous best loss:  2.694908857345581  | saving best model ...\n",
      "Current loss:  2.694129467010498  | , previous best loss:  2.69451904296875  | saving best model ...\n",
      "Current loss:  2.693739652633667  | , previous best loss:  2.694129467010498  | saving best model ...\n",
      "Current loss:  2.693350315093994  | , previous best loss:  2.693739652633667  | saving best model ...\n",
      "Current loss:  2.6929616928100586  | , previous best loss:  2.693350315093994  | saving best model ...\n",
      "Current loss:  2.692572593688965  | , previous best loss:  2.6929616928100586  | saving best model ...\n",
      "Current loss:  2.6921839714050293  | , previous best loss:  2.692572593688965  | saving best model ...\n",
      "Current loss:  2.691795587539673  | , previous best loss:  2.6921839714050293  | saving best model ...\n",
      "Current loss:  2.6914074420928955  | , previous best loss:  2.691795587539673  | saving best model ...\n",
      "Current loss:  2.691019296646118  | , previous best loss:  2.6914074420928955  | saving best model ...\n",
      "Current loss:  2.69063138961792  | , previous best loss:  2.691019296646118  | saving best model ...\n",
      "Current loss:  2.690243721008301  | , previous best loss:  2.69063138961792  | saving best model ...\n",
      "Current loss:  2.68985652923584  | , previous best loss:  2.690243721008301  | saving best model ...\n",
      "Current loss:  2.689469337463379  | , previous best loss:  2.68985652923584  | saving best model ...\n",
      "Current loss:  2.689082384109497  | , previous best loss:  2.689469337463379  | saving best model ...\n",
      "Current loss:  2.6886954307556152  | , previous best loss:  2.689082384109497  | saving best model ...\n",
      "Current loss:  2.6883084774017334  | , previous best loss:  2.6886954307556152  | saving best model ...\n",
      "Current loss:  2.6879220008850098  | , previous best loss:  2.6883084774017334  | saving best model ...\n",
      "Current loss:  2.6875362396240234  | , previous best loss:  2.6879220008850098  | saving best model ...\n",
      "Current loss:  2.687150001525879  | , previous best loss:  2.6875362396240234  | saving best model ...\n",
      "Current loss:  2.6867640018463135  | , previous best loss:  2.687150001525879  | saving best model ...\n",
      "Current loss:  2.6863784790039062  | , previous best loss:  2.6867640018463135  | saving best model ...\n",
      "Current loss:  2.685992956161499  | , previous best loss:  2.6863784790039062  | saving best model ...\n",
      "Current loss:  2.685607433319092  | , previous best loss:  2.685992956161499  | saving best model ...\n",
      "Current loss:  2.685222625732422  | , previous best loss:  2.685607433319092  | saving best model ...\n",
      "Current loss:  2.684837579727173  | , previous best loss:  2.685222625732422  | saving best model ...\n",
      "Current loss:  2.684452772140503  | , previous best loss:  2.684837579727173  | saving best model ...\n",
      "Current loss:  2.6840689182281494  | , previous best loss:  2.684452772140503  | saving best model ...\n",
      "Current loss:  2.6836845874786377  | , previous best loss:  2.6840689182281494  | saving best model ...\n",
      "Current loss:  2.683300256729126  | , previous best loss:  2.6836845874786377  | saving best model ...\n",
      "Current loss:  2.6829164028167725  | , previous best loss:  2.683300256729126  | saving best model ...\n",
      "Current loss:  2.682532787322998  | , previous best loss:  2.6829164028167725  | saving best model ...\n",
      "Current loss:  2.6821494102478027  | , previous best loss:  2.682532787322998  | saving best model ...\n",
      "Current loss:  2.6817657947540283  | , previous best loss:  2.6821494102478027  | saving best model ...\n",
      "Current loss:  2.681382656097412  | , previous best loss:  2.6817657947540283  | saving best model ...\n",
      "Current loss:  2.680999517440796  | , previous best loss:  2.681382656097412  | saving best model ...\n",
      "Current loss:  2.680617094039917  | , previous best loss:  2.680999517440796  | saving best model ...\n",
      "Current loss:  2.680234670639038  | , previous best loss:  2.680617094039917  | saving best model ...\n",
      "Current loss:  2.67985200881958  | , previous best loss:  2.680234670639038  | saving best model ...\n",
      "Current loss:  2.6794700622558594  | , previous best loss:  2.67985200881958  | saving best model ...\n",
      "Current loss:  2.6790878772735596  | , previous best loss:  2.6794700622558594  | saving best model ...\n",
      "Current loss:  2.678706169128418  | , previous best loss:  2.6790878772735596  | saving best model ...\n",
      "Current loss:  2.6783246994018555  | , previous best loss:  2.678706169128418  | saving best model ...\n",
      "Current loss:  2.6779427528381348  | , previous best loss:  2.6783246994018555  | saving best model ...\n",
      "Current loss:  2.6775617599487305  | , previous best loss:  2.6779427528381348  | saving best model ...\n",
      "Current loss:  2.677180767059326  | , previous best loss:  2.6775617599487305  | saving best model ...\n",
      "Current loss:  2.676799774169922  | , previous best loss:  2.677180767059326  | saving best model ...\n",
      "Current loss:  2.676419258117676  | , previous best loss:  2.676799774169922  | saving best model ...\n",
      "Current loss:  2.6760385036468506  | , previous best loss:  2.676419258117676  | saving best model ...\n",
      "Current loss:  2.6756582260131836  | , previous best loss:  2.6760385036468506  | saving best model ...\n",
      "Current loss:  2.6752781867980957  | , previous best loss:  2.6756582260131836  | saving best model ...\n",
      "Current loss:  2.674898386001587  | , previous best loss:  2.6752781867980957  | saving best model ...\n",
      "Current loss:  2.674518585205078  | , previous best loss:  2.674898386001587  | saving best model ...\n",
      "Current loss:  2.6741387844085693  | , previous best loss:  2.674518585205078  | saving best model ...\n",
      "Current loss:  2.6737594604492188  | , previous best loss:  2.6741387844085693  | saving best model ...\n",
      "Current loss:  2.673380136489868  | , previous best loss:  2.6737594604492188  | saving best model ...\n",
      "Current loss:  2.6730010509490967  | , previous best loss:  2.673380136489868  | saving best model ...\n",
      "Current loss:  2.6726222038269043  | , previous best loss:  2.6730010509490967  | saving best model ...\n",
      "Current loss:  2.672243595123291  | , previous best loss:  2.6726222038269043  | saving best model ...\n",
      "Current loss:  2.6718647480010986  | , previous best loss:  2.672243595123291  | saving best model ...\n",
      "Current loss:  2.6714868545532227  | , previous best loss:  2.6718647480010986  | saving best model ...\n",
      "Current loss:  2.6711084842681885  | , previous best loss:  2.6714868545532227  | saving best model ...\n",
      "Current loss:  2.6707303524017334  | , previous best loss:  2.6711084842681885  | saving best model ...\n",
      "Current loss:  2.6703524589538574  | , previous best loss:  2.6707303524017334  | saving best model ...\n",
      "Current loss:  2.6699750423431396  | , previous best loss:  2.6703524589538574  | saving best model ...\n",
      "Current loss:  2.6695973873138428  | , previous best loss:  2.6699750423431396  | saving best model ...\n",
      "Current loss:  2.669220209121704  | , previous best loss:  2.6695973873138428  | saving best model ...\n",
      "Current loss:  2.6688432693481445  | , previous best loss:  2.669220209121704  | saving best model ...\n",
      "Current loss:  2.668466329574585  | , previous best loss:  2.6688432693481445  | saving best model ...\n",
      "Current loss:  2.6680893898010254  | , previous best loss:  2.668466329574585  | saving best model ...\n",
      "Current loss:  2.667712926864624  | , previous best loss:  2.6680893898010254  | saving best model ...\n",
      "Current loss:  2.6673364639282227  | , previous best loss:  2.667712926864624  | saving best model ...\n",
      "Current loss:  2.6669600009918213  | , previous best loss:  2.6673364639282227  | saving best model ...\n",
      "Current loss:  2.666584014892578  | , previous best loss:  2.6669600009918213  | saving best model ...\n",
      "Current loss:  2.666208028793335  | , previous best loss:  2.666584014892578  | saving best model ...\n",
      "Current loss:  2.66583251953125  | , previous best loss:  2.666208028793335  | saving best model ...\n",
      "Current loss:  2.665456533432007  | , previous best loss:  2.66583251953125  | saving best model ...\n",
      "Current loss:  2.66508150100708  | , previous best loss:  2.665456533432007  | saving best model ...\n",
      "Current loss:  2.664705991744995  | , previous best loss:  2.66508150100708  | saving best model ...\n",
      "Current loss:  2.6643309593200684  | , previous best loss:  2.664705991744995  | saving best model ...\n",
      "Current loss:  2.6639561653137207  | , previous best loss:  2.6643309593200684  | saving best model ...\n",
      "Current loss:  2.663581371307373  | , previous best loss:  2.6639561653137207  | saving best model ...\n",
      "Current loss:  2.6632068157196045  | , previous best loss:  2.663581371307373  | saving best model ...\n",
      "Current loss:  2.662832498550415  | , previous best loss:  2.6632068157196045  | saving best model ...\n",
      "Current loss:  2.6624579429626465  | , previous best loss:  2.662832498550415  | saving best model ...\n",
      "Current loss:  2.6620843410491943  | , previous best loss:  2.6624579429626465  | saving best model ...\n",
      "Current loss:  2.661710500717163  | , previous best loss:  2.6620843410491943  | saving best model ...\n",
      "Current loss:  2.6613364219665527  | , previous best loss:  2.661710500717163  | saving best model ...\n",
      "Current loss:  2.6609628200531006  | , previous best loss:  2.6613364219665527  | saving best model ...\n",
      "Current loss:  2.6605896949768066  | , previous best loss:  2.6609628200531006  | saving best model ...\n",
      "Current loss:  2.6602160930633545  | , previous best loss:  2.6605896949768066  | saving best model ...\n",
      "Current loss:  2.6598432064056396  | , previous best loss:  2.6602160930633545  | saving best model ...\n",
      "Current loss:  2.659470319747925  | , previous best loss:  2.6598432064056396  | saving best model ...\n",
      "Current loss:  2.65909743309021  | , previous best loss:  2.659470319747925  | saving best model ...\n",
      "Current loss:  2.6587250232696533  | , previous best loss:  2.65909743309021  | saving best model ...\n",
      "Current loss:  2.6583523750305176  | , previous best loss:  2.6587250232696533  | saving best model ...\n",
      "Current loss:  2.657979965209961  | , previous best loss:  2.6583523750305176  | saving best model ...\n",
      "Current loss:  2.6576082706451416  | , previous best loss:  2.657979965209961  | saving best model ...\n",
      "Current loss:  2.657235860824585  | , previous best loss:  2.6576082706451416  | saving best model ...\n",
      "Current loss:  2.6568641662597656  | , previous best loss:  2.657235860824585  | saving best model ...\n",
      "Current loss:  2.6564927101135254  | , previous best loss:  2.6568641662597656  | saving best model ...\n",
      "Current loss:  2.656120777130127  | , previous best loss:  2.6564927101135254  | saving best model ...\n",
      "Current loss:  2.655749559402466  | , previous best loss:  2.656120777130127  | saving best model ...\n",
      "Current loss:  2.6553783416748047  | , previous best loss:  2.655749559402466  | saving best model ...\n",
      "Current loss:  2.6550071239471436  | , previous best loss:  2.6553783416748047  | saving best model ...\n",
      "Current loss:  2.6546363830566406  | , previous best loss:  2.6550071239471436  | saving best model ...\n",
      "Current loss:  2.654266119003296  | , previous best loss:  2.6546363830566406  | saving best model ...\n",
      "Current loss:  2.653895378112793  | , previous best loss:  2.654266119003296  | saving best model ...\n",
      "Current loss:  2.653524398803711  | , previous best loss:  2.653895378112793  | saving best model ...\n",
      "Current loss:  2.6531546115875244  | , previous best loss:  2.653524398803711  | saving best model ...\n",
      "Current loss:  2.6527843475341797  | , previous best loss:  2.6531546115875244  | saving best model ...\n",
      "Current loss:  2.652414321899414  | , previous best loss:  2.6527843475341797  | saving best model ...\n",
      "Current loss:  2.6520450115203857  | , previous best loss:  2.652414321899414  | saving best model ...\n",
      "Current loss:  2.651674747467041  | , previous best loss:  2.6520450115203857  | saving best model ...\n",
      "Current loss:  2.6513054370880127  | , previous best loss:  2.651674747467041  | saving best model ...\n",
      "Current loss:  2.6509358882904053  | , previous best loss:  2.6513054370880127  | saving best model ...\n",
      "Current loss:  2.650566816329956  | , previous best loss:  2.6509358882904053  | saving best model ...\n",
      "Current loss:  2.650197982788086  | , previous best loss:  2.650566816329956  | saving best model ...\n",
      "Current loss:  2.6498289108276367  | , previous best loss:  2.650197982788086  | saving best model ...\n",
      "Current loss:  2.6494603157043457  | , previous best loss:  2.6498289108276367  | saving best model ...\n",
      "Current loss:  2.6490912437438965  | , previous best loss:  2.6494603157043457  | saving best model ...\n",
      "Current loss:  2.6487226486206055  | , previous best loss:  2.6490912437438965  | saving best model ...\n",
      "Current loss:  2.6483547687530518  | , previous best loss:  2.6487226486206055  | saving best model ...\n",
      "Current loss:  2.64798641204834  | , previous best loss:  2.6483547687530518  | saving best model ...\n",
      "Current loss:  2.647618293762207  | , previous best loss:  2.64798641204834  | saving best model ...\n",
      "Current loss:  2.6472504138946533  | , previous best loss:  2.647618293762207  | saving best model ...\n",
      "Current loss:  2.6468825340270996  | , previous best loss:  2.6472504138946533  | saving best model ...\n",
      "Current loss:  2.646515130996704  | , previous best loss:  2.6468825340270996  | saving best model ...\n",
      "Current loss:  2.6461474895477295  | , previous best loss:  2.646515130996704  | saving best model ...\n",
      "Current loss:  2.645780324935913  | , previous best loss:  2.6461474895477295  | saving best model ...\n",
      "Current loss:  2.6454131603240967  | , previous best loss:  2.645780324935913  | saving best model ...\n",
      "Current loss:  2.6450462341308594  | , previous best loss:  2.6454131603240967  | saving best model ...\n",
      "Current loss:  2.644679307937622  | , previous best loss:  2.6450462341308594  | saving best model ...\n",
      "Current loss:  2.6443123817443848  | , previous best loss:  2.644679307937622  | saving best model ...\n",
      "Current loss:  2.6439456939697266  | , previous best loss:  2.6443123817443848  | saving best model ...\n",
      "Current loss:  2.6435792446136475  | , previous best loss:  2.6439456939697266  | saving best model ...\n",
      "Current loss:  2.6432130336761475  | , previous best loss:  2.6435792446136475  | saving best model ...\n",
      "Current loss:  2.6428468227386475  | , previous best loss:  2.6432130336761475  | saving best model ...\n",
      "Current loss:  2.6424806118011475  | , previous best loss:  2.6428468227386475  | saving best model ...\n",
      "Current loss:  2.6421146392822266  | , previous best loss:  2.6424806118011475  | saving best model ...\n",
      "Current loss:  2.6417489051818848  | , previous best loss:  2.6421146392822266  | saving best model ...\n",
      "Current loss:  2.641383409500122  | , previous best loss:  2.6417489051818848  | saving best model ...\n",
      "Current loss:  2.6410181522369385  | , previous best loss:  2.641383409500122  | saving best model ...\n",
      "Current loss:  2.640652656555176  | , previous best loss:  2.6410181522369385  | saving best model ...\n",
      "Current loss:  2.640287399291992  | , previous best loss:  2.640652656555176  | saving best model ...\n",
      "Current loss:  2.6399219036102295  | , previous best loss:  2.640287399291992  | saving best model ...\n",
      "Current loss:  2.639557123184204  | , previous best loss:  2.6399219036102295  | saving best model ...\n",
      "Current loss:  2.6391923427581787  | , previous best loss:  2.639557123184204  | saving best model ...\n",
      "Current loss:  2.6388275623321533  | , previous best loss:  2.6391923427581787  | saving best model ...\n",
      "Current loss:  2.638462781906128  | , previous best loss:  2.6388275623321533  | saving best model ...\n",
      "Current loss:  2.63809871673584  | , previous best loss:  2.638462781906128  | saving best model ...\n",
      "Current loss:  2.6377341747283936  | , previous best loss:  2.63809871673584  | saving best model ...\n",
      "Current loss:  2.6373703479766846  | , previous best loss:  2.6377341747283936  | saving best model ...\n",
      "Current loss:  2.6370058059692383  | , previous best loss:  2.6373703479766846  | saving best model ...\n",
      "Current loss:  2.6366419792175293  | , previous best loss:  2.6370058059692383  | saving best model ...\n",
      "Current loss:  2.636276960372925  | , previous best loss:  2.6366419792175293  | saving best model ...\n",
      "Current loss:  2.635911226272583  | , previous best loss:  2.636276960372925  | saving best model ...\n",
      "Current loss:  2.635545492172241  | , previous best loss:  2.635911226272583  | saving best model ...\n",
      "Current loss:  2.6351802349090576  | , previous best loss:  2.635545492172241  | saving best model ...\n",
      "Current loss:  2.634814500808716  | , previous best loss:  2.6351802349090576  | saving best model ...\n",
      "Current loss:  2.634449005126953  | , previous best loss:  2.634814500808716  | saving best model ...\n",
      "Current loss:  2.6340839862823486  | , previous best loss:  2.634449005126953  | saving best model ...\n",
      "Current loss:  2.633718252182007  | , previous best loss:  2.6340839862823486  | saving best model ...\n",
      "Current loss:  2.6333532333374023  | , previous best loss:  2.633718252182007  | saving best model ...\n",
      "Current loss:  2.6329877376556396  | , previous best loss:  2.6333532333374023  | saving best model ...\n",
      "Current loss:  2.632622718811035  | , previous best loss:  2.6329877376556396  | saving best model ...\n",
      "Current loss:  2.6322576999664307  | , previous best loss:  2.632622718811035  | saving best model ...\n",
      "Current loss:  2.631892681121826  | , previous best loss:  2.6322576999664307  | saving best model ...\n",
      "Current loss:  2.6315276622772217  | , previous best loss:  2.631892681121826  | saving best model ...\n",
      "Current loss:  2.631162405014038  | , previous best loss:  2.6315276622772217  | saving best model ...\n",
      "Current loss:  2.6307973861694336  | , previous best loss:  2.631162405014038  | saving best model ...\n",
      "Current loss:  2.6304328441619873  | , previous best loss:  2.6307973861694336  | saving best model ...\n",
      "Current loss:  2.630068302154541  | , previous best loss:  2.6304328441619873  | saving best model ...\n",
      "Current loss:  2.6297037601470947  | , previous best loss:  2.630068302154541  | saving best model ...\n",
      "Current loss:  2.6293396949768066  | , previous best loss:  2.6297037601470947  | saving best model ...\n",
      "Current loss:  2.6289751529693604  | , previous best loss:  2.6293396949768066  | saving best model ...\n",
      "Current loss:  2.6286110877990723  | , previous best loss:  2.6289751529693604  | saving best model ...\n",
      "Current loss:  2.628246784210205  | , previous best loss:  2.6286110877990723  | saving best model ...\n",
      "Current loss:  2.627882719039917  | , previous best loss:  2.628246784210205  | saving best model ...\n",
      "Current loss:  2.627519130706787  | , previous best loss:  2.627882719039917  | saving best model ...\n",
      "Current loss:  2.627155303955078  | , previous best loss:  2.627519130706787  | saving best model ...\n",
      "Current loss:  2.62679123878479  | , previous best loss:  2.627155303955078  | saving best model ...\n",
      "Current loss:  2.6264278888702393  | , previous best loss:  2.62679123878479  | saving best model ...\n",
      "Current loss:  2.6260645389556885  | , previous best loss:  2.6264278888702393  | saving best model ...\n",
      "Current loss:  2.6257011890411377  | , previous best loss:  2.6260645389556885  | saving best model ...\n",
      "Current loss:  2.625337839126587  | , previous best loss:  2.6257011890411377  | saving best model ...\n",
      "Current loss:  2.6249749660491943  | , previous best loss:  2.625337839126587  | saving best model ...\n",
      "Current loss:  2.6246118545532227  | , previous best loss:  2.6249749660491943  | saving best model ...\n",
      "Current loss:  2.624248743057251  | , previous best loss:  2.6246118545532227  | saving best model ...\n",
      "Current loss:  2.6238858699798584  | , previous best loss:  2.624248743057251  | saving best model ...\n",
      "Current loss:  2.623523473739624  | , previous best loss:  2.6238858699798584  | saving best model ...\n",
      "Current loss:  2.6231606006622314  | , previous best loss:  2.623523473739624  | saving best model ...\n",
      "Current loss:  2.622798442840576  | , previous best loss:  2.6231606006622314  | saving best model ...\n",
      "Current loss:  2.622436285018921  | , previous best loss:  2.622798442840576  | saving best model ...\n",
      "Current loss:  2.6220738887786865  | , previous best loss:  2.622436285018921  | saving best model ...\n",
      "Current loss:  2.6217122077941895  | , previous best loss:  2.6220738887786865  | saving best model ...\n",
      "Current loss:  2.621350049972534  | , previous best loss:  2.6217122077941895  | saving best model ...\n",
      "Current loss:  2.620988368988037  | , previous best loss:  2.621350049972534  | saving best model ...\n",
      "Current loss:  2.620626211166382  | , previous best loss:  2.620988368988037  | saving best model ...\n",
      "Current loss:  2.620265007019043  | , previous best loss:  2.620626211166382  | saving best model ...\n",
      "Current loss:  2.619903326034546  | , previous best loss:  2.620265007019043  | saving best model ...\n",
      "Current loss:  2.619541645050049  | , previous best loss:  2.619903326034546  | saving best model ...\n",
      "Current loss:  2.61918044090271  | , previous best loss:  2.619541645050049  | saving best model ...\n",
      "Current loss:  2.618818998336792  | , previous best loss:  2.61918044090271  | saving best model ...\n",
      "Current loss:  2.6184585094451904  | , previous best loss:  2.618818998336792  | saving best model ...\n",
      "Current loss:  2.6180973052978516  | , previous best loss:  2.6184585094451904  | saving best model ...\n",
      "Current loss:  2.617736339569092  | , previous best loss:  2.6180973052978516  | saving best model ...\n",
      "Current loss:  2.617375373840332  | , previous best loss:  2.617736339569092  | saving best model ...\n",
      "Current loss:  2.6170148849487305  | , previous best loss:  2.617375373840332  | saving best model ...\n",
      "Current loss:  2.616654634475708  | , previous best loss:  2.6170148849487305  | saving best model ...\n",
      "Current loss:  2.6162941455841064  | , previous best loss:  2.616654634475708  | saving best model ...\n",
      "Current loss:  2.615933895111084  | , previous best loss:  2.6162941455841064  | saving best model ...\n",
      "Current loss:  2.6155734062194824  | , previous best loss:  2.615933895111084  | saving best model ...\n",
      "Current loss:  2.61521315574646  | , previous best loss:  2.6155734062194824  | saving best model ...\n",
      "Current loss:  2.614853620529175  | , previous best loss:  2.61521315574646  | saving best model ...\n",
      "Current loss:  2.614492416381836  | , previous best loss:  2.614853620529175  | saving best model ...\n",
      "Current loss:  2.6141297817230225  | , previous best loss:  2.614492416381836  | saving best model ...\n",
      "Current loss:  2.61376690864563  | , previous best loss:  2.6141297817230225  | saving best model ...\n",
      "Current loss:  2.6134040355682373  | , previous best loss:  2.61376690864563  | saving best model ...\n",
      "Current loss:  2.6130406856536865  | , previous best loss:  2.6134040355682373  | saving best model ...\n",
      "Current loss:  2.612677812576294  | , previous best loss:  2.6130406856536865  | saving best model ...\n",
      "Current loss:  2.6123149394989014  | , previous best loss:  2.612677812576294  | saving best model ...\n",
      "Current loss:  2.611952066421509  | , previous best loss:  2.6123149394989014  | saving best model ...\n",
      "Current loss:  2.6115894317626953  | , previous best loss:  2.611952066421509  | saving best model ...\n",
      "Current loss:  2.6112265586853027  | , previous best loss:  2.6115894317626953  | saving best model ...\n",
      "Current loss:  2.610863447189331  | , previous best loss:  2.6112265586853027  | saving best model ...\n",
      "Current loss:  2.6105005741119385  | , previous best loss:  2.610863447189331  | saving best model ...\n",
      "Current loss:  2.6101372241973877  | , previous best loss:  2.6105005741119385  | saving best model ...\n",
      "Current loss:  2.609773874282837  | , previous best loss:  2.6101372241973877  | saving best model ...\n",
      "Current loss:  2.6094107627868652  | , previous best loss:  2.609773874282837  | saving best model ...\n",
      "Current loss:  2.6090476512908936  | , previous best loss:  2.6094107627868652  | saving best model ...\n",
      "Current loss:  2.608684778213501  | , previous best loss:  2.6090476512908936  | saving best model ...\n",
      "Current loss:  2.6083219051361084  | , previous best loss:  2.608684778213501  | saving best model ...\n",
      "Current loss:  2.607959270477295  | , previous best loss:  2.6083219051361084  | saving best model ...\n",
      "Current loss:  2.6075966358184814  | , previous best loss:  2.607959270477295  | saving best model ...\n",
      "Current loss:  2.607234001159668  | , previous best loss:  2.6075966358184814  | saving best model ...\n",
      "Current loss:  2.6068718433380127  | , previous best loss:  2.607234001159668  | saving best model ...\n",
      "Current loss:  2.60650897026062  | , previous best loss:  2.6068718433380127  | saving best model ...\n",
      "Current loss:  2.606146812438965  | , previous best loss:  2.60650897026062  | saving best model ...\n",
      "Current loss:  2.6057844161987305  | , previous best loss:  2.606146812438965  | saving best model ...\n",
      "Current loss:  2.6054224967956543  | , previous best loss:  2.6057844161987305  | saving best model ...\n",
      "Current loss:  2.605060577392578  | , previous best loss:  2.6054224967956543  | saving best model ...\n",
      "Current loss:  2.604698419570923  | , previous best loss:  2.605060577392578  | saving best model ...\n",
      "Current loss:  2.604336977005005  | , previous best loss:  2.604698419570923  | saving best model ...\n",
      "Current loss:  2.6039750576019287  | , previous best loss:  2.604336977005005  | saving best model ...\n",
      "Current loss:  2.6036136150360107  | , previous best loss:  2.6039750576019287  | saving best model ...\n",
      "Current loss:  2.6032519340515137  | , previous best loss:  2.6036136150360107  | saving best model ...\n",
      "Current loss:  2.602890968322754  | , previous best loss:  2.6032519340515137  | saving best model ...\n",
      "Current loss:  2.602529525756836  | , previous best loss:  2.602890968322754  | saving best model ...\n",
      "Current loss:  2.602168560028076  | , previous best loss:  2.602529525756836  | saving best model ...\n",
      "Current loss:  2.6018075942993164  | , previous best loss:  2.602168560028076  | saving best model ...\n",
      "Current loss:  2.6014463901519775  | , previous best loss:  2.6018075942993164  | saving best model ...\n",
      "Current loss:  2.601085901260376  | , previous best loss:  2.6014463901519775  | saving best model ...\n",
      "Current loss:  2.6007251739501953  | , previous best loss:  2.601085901260376  | saving best model ...\n",
      "Current loss:  2.6003644466400146  | , previous best loss:  2.6007251739501953  | saving best model ...\n",
      "Current loss:  2.600003957748413  | , previous best loss:  2.6003644466400146  | saving best model ...\n",
      "Current loss:  2.5996434688568115  | , previous best loss:  2.600003957748413  | saving best model ...\n",
      "Current loss:  2.599283456802368  | , previous best loss:  2.5996434688568115  | saving best model ...\n",
      "Current loss:  2.5989232063293457  | , previous best loss:  2.599283456802368  | saving best model ...\n",
      "Current loss:  2.5985631942749023  | , previous best loss:  2.5989232063293457  | saving best model ...\n",
      "Current loss:  2.598203182220459  | , previous best loss:  2.5985631942749023  | saving best model ...\n",
      "Current loss:  2.5978431701660156  | , previous best loss:  2.598203182220459  | saving best model ...\n",
      "Current loss:  2.5974833965301514  | , previous best loss:  2.5978431701660156  | saving best model ...\n",
      "Current loss:  2.597123861312866  | , previous best loss:  2.5974833965301514  | saving best model ...\n",
      "Current loss:  2.596764326095581  | , previous best loss:  2.597123861312866  | saving best model ...\n",
      "Current loss:  2.596404552459717  | , previous best loss:  2.596764326095581  | saving best model ...\n",
      "Current loss:  2.59604549407959  | , previous best loss:  2.596404552459717  | saving best model ...\n",
      "Current loss:  2.595686197280884  | , previous best loss:  2.59604549407959  | saving best model ...\n",
      "Current loss:  2.5953269004821777  | , previous best loss:  2.595686197280884  | saving best model ...\n",
      "Current loss:  2.59496808052063  | , previous best loss:  2.5953269004821777  | saving best model ...\n",
      "Current loss:  2.594608783721924  | , previous best loss:  2.59496808052063  | saving best model ...\n",
      "Current loss:  2.594250202178955  | , previous best loss:  2.594608783721924  | saving best model ...\n",
      "Current loss:  2.5938913822174072  | , previous best loss:  2.594250202178955  | saving best model ...\n",
      "Current loss:  2.5935325622558594  | , previous best loss:  2.5938913822174072  | saving best model ...\n",
      "Current loss:  2.5931739807128906  | , previous best loss:  2.5935325622558594  | saving best model ...\n",
      "Current loss:  2.59281587600708  | , previous best loss:  2.5931739807128906  | saving best model ...\n",
      "Current loss:  2.5924570560455322  | , previous best loss:  2.59281587600708  | saving best model ...\n",
      "Current loss:  2.5920989513397217  | , previous best loss:  2.5924570560455322  | saving best model ...\n",
      "Current loss:  2.591740369796753  | , previous best loss:  2.5920989513397217  | saving best model ...\n",
      "Current loss:  2.5913822650909424  | , previous best loss:  2.591740369796753  | saving best model ...\n",
      "Current loss:  2.591024398803711  | , previous best loss:  2.5913822650909424  | saving best model ...\n",
      "Current loss:  2.5906662940979004  | , previous best loss:  2.591024398803711  | saving best model ...\n",
      "Current loss:  2.59030818939209  | , previous best loss:  2.5906662940979004  | saving best model ...\n",
      "Current loss:  2.5899500846862793  | , previous best loss:  2.59030818939209  | saving best model ...\n",
      "Current loss:  2.589592695236206  | , previous best loss:  2.5899500846862793  | saving best model ...\n",
      "Current loss:  2.5892350673675537  | , previous best loss:  2.589592695236206  | saving best model ...\n",
      "Current loss:  2.5888776779174805  | , previous best loss:  2.5892350673675537  | saving best model ...\n",
      "Current loss:  2.5885202884674072  | , previous best loss:  2.5888776779174805  | saving best model ...\n",
      "Current loss:  2.588162899017334  | , previous best loss:  2.5885202884674072  | saving best model ...\n",
      "Current loss:  2.58780574798584  | , previous best loss:  2.588162899017334  | saving best model ...\n",
      "Current loss:  2.5874485969543457  | , previous best loss:  2.58780574798584  | saving best model ...\n",
      "Current loss:  2.5870909690856934  | , previous best loss:  2.5874485969543457  | saving best model ...\n",
      "Current loss:  2.5867340564727783  | , previous best loss:  2.5870909690856934  | saving best model ...\n",
      "Current loss:  2.5863771438598633  | , previous best loss:  2.5867340564727783  | saving best model ...\n",
      "Current loss:  2.586019992828369  | , previous best loss:  2.5863771438598633  | saving best model ...\n",
      "Current loss:  2.585663318634033  | , previous best loss:  2.586019992828369  | saving best model ...\n",
      "Current loss:  2.585306406021118  | , previous best loss:  2.585663318634033  | saving best model ...\n",
      "Current loss:  2.5849497318267822  | , previous best loss:  2.585306406021118  | saving best model ...\n",
      "Current loss:  2.5845932960510254  | , previous best loss:  2.5849497318267822  | saving best model ...\n",
      "Current loss:  2.5842368602752686  | , previous best loss:  2.5845932960510254  | saving best model ...\n",
      "Current loss:  2.5838804244995117  | , previous best loss:  2.5842368602752686  | saving best model ...\n",
      "Current loss:  2.583523988723755  | , previous best loss:  2.5838804244995117  | saving best model ...\n",
      "Current loss:  2.583167791366577  | , previous best loss:  2.583523988723755  | saving best model ...\n",
      "Current loss:  2.5828115940093994  | , previous best loss:  2.583167791366577  | saving best model ...\n",
      "Current loss:  2.5824553966522217  | , previous best loss:  2.5828115940093994  | saving best model ...\n",
      "Current loss:  2.582099199295044  | , previous best loss:  2.5824553966522217  | saving best model ...\n",
      "Current loss:  2.5817434787750244  | , previous best loss:  2.582099199295044  | saving best model ...\n",
      "Current loss:  2.581387519836426  | , previous best loss:  2.5817434787750244  | saving best model ...\n",
      "Current loss:  2.5810317993164062  | , previous best loss:  2.581387519836426  | saving best model ...\n",
      "Current loss:  2.5806758403778076  | , previous best loss:  2.5810317993164062  | saving best model ...\n",
      "Current loss:  2.580320358276367  | , previous best loss:  2.5806758403778076  | saving best model ...\n",
      "Current loss:  2.5799646377563477  | , previous best loss:  2.580320358276367  | saving best model ...\n",
      "Current loss:  2.579608917236328  | , previous best loss:  2.5799646377563477  | saving best model ...\n",
      "Current loss:  2.579253673553467  | , previous best loss:  2.579608917236328  | saving best model ...\n",
      "Current loss:  2.5788979530334473  | , previous best loss:  2.579253673553467  | saving best model ...\n",
      "Current loss:  2.578542470932007  | , previous best loss:  2.5788979530334473  | saving best model ...\n",
      "Current loss:  2.5781872272491455  | , previous best loss:  2.578542470932007  | saving best model ...\n",
      "Current loss:  2.5778322219848633  | , previous best loss:  2.5781872272491455  | saving best model ...\n",
      "Current loss:  2.577476978302002  | , previous best loss:  2.5778322219848633  | saving best model ...\n",
      "Current loss:  2.5771217346191406  | , previous best loss:  2.577476978302002  | saving best model ...\n",
      "Current loss:  2.5767667293548584  | , previous best loss:  2.5771217346191406  | saving best model ...\n",
      "Current loss:  2.5764119625091553  | , previous best loss:  2.5767667293548584  | saving best model ...\n",
      "Current loss:  2.576056957244873  | , previous best loss:  2.5764119625091553  | saving best model ...\n",
      "Current loss:  2.57570219039917  | , previous best loss:  2.576056957244873  | saving best model ...\n",
      "Current loss:  2.575347423553467  | , previous best loss:  2.57570219039917  | saving best model ...\n",
      "Current loss:  2.5749926567077637  | , previous best loss:  2.575347423553467  | saving best model ...\n",
      "Current loss:  2.5746378898620605  | , previous best loss:  2.5749926567077637  | saving best model ...\n",
      "Current loss:  2.5742831230163574  | , previous best loss:  2.5746378898620605  | saving best model ...\n",
      "Current loss:  2.5739285945892334  | , previous best loss:  2.5742831230163574  | saving best model ...\n",
      "Current loss:  2.5735745429992676  | , previous best loss:  2.5739285945892334  | saving best model ...\n",
      "Current loss:  2.5732202529907227  | , previous best loss:  2.5735745429992676  | saving best model ...\n",
      "Current loss:  2.5728657245635986  | , previous best loss:  2.5732202529907227  | saving best model ...\n",
      "Current loss:  2.5725114345550537  | , previous best loss:  2.5728657245635986  | saving best model ...\n",
      "Current loss:  2.572157144546509  | , previous best loss:  2.5725114345550537  | saving best model ...\n",
      "Current loss:  2.571803092956543  | , previous best loss:  2.572157144546509  | saving best model ...\n",
      "Current loss:  2.5714492797851562  | , previous best loss:  2.571803092956543  | saving best model ...\n",
      "Current loss:  2.5710949897766113  | , previous best loss:  2.5714492797851562  | saving best model ...\n",
      "Current loss:  2.5707406997680664  | , previous best loss:  2.5710949897766113  | saving best model ...\n",
      "Current loss:  2.5703868865966797  | , previous best loss:  2.5707406997680664  | saving best model ...\n",
      "Current loss:  2.570033073425293  | , previous best loss:  2.5703868865966797  | saving best model ...\n",
      "Current loss:  2.5696792602539062  | , previous best loss:  2.570033073425293  | saving best model ...\n",
      "Current loss:  2.5693254470825195  | , previous best loss:  2.5696792602539062  | saving best model ...\n",
      "Current loss:  2.568971633911133  | , previous best loss:  2.5693254470825195  | saving best model ...\n",
      "Current loss:  2.568618059158325  | , previous best loss:  2.568971633911133  | saving best model ...\n",
      "Current loss:  2.5682647228240967  | , previous best loss:  2.568618059158325  | saving best model ...\n",
      "Current loss:  2.567911148071289  | , previous best loss:  2.5682647228240967  | saving best model ...\n",
      "Current loss:  2.5675575733184814  | , previous best loss:  2.567911148071289  | saving best model ...\n",
      "Current loss:  2.567203998565674  | , previous best loss:  2.5675575733184814  | saving best model ...\n",
      "Current loss:  2.5668509006500244  | , previous best loss:  2.567203998565674  | saving best model ...\n",
      "Current loss:  2.566497564315796  | , previous best loss:  2.5668509006500244  | saving best model ...\n",
      "Current loss:  2.5661442279815674  | , previous best loss:  2.566497564315796  | saving best model ...\n",
      "Current loss:  2.5657906532287598  | , previous best loss:  2.5661442279815674  | saving best model ...\n",
      "Current loss:  2.5654380321502686  | , previous best loss:  2.5657906532287598  | saving best model ...\n",
      "Current loss:  2.56508469581604  | , previous best loss:  2.5654380321502686  | saving best model ...\n",
      "Current loss:  2.5647318363189697  | , previous best loss:  2.56508469581604  | saving best model ...\n",
      "Current loss:  2.5643787384033203  | , previous best loss:  2.5647318363189697  | saving best model ...\n",
      "Current loss:  2.564025640487671  | , previous best loss:  2.5643787384033203  | saving best model ...\n",
      "Current loss:  2.5636725425720215  | , previous best loss:  2.564025640487671  | saving best model ...\n",
      "Current loss:  2.5633199214935303  | , previous best loss:  2.5636725425720215  | saving best model ...\n",
      "Current loss:  2.56296706199646  | , previous best loss:  2.5633199214935303  | saving best model ...\n",
      "Current loss:  2.5626142024993896  | , previous best loss:  2.56296706199646  | saving best model ...\n",
      "Current loss:  2.5622615814208984  | , previous best loss:  2.5626142024993896  | saving best model ...\n",
      "Current loss:  2.561908721923828  | , previous best loss:  2.5622615814208984  | saving best model ...\n",
      "Current loss:  2.561556100845337  | , previous best loss:  2.561908721923828  | saving best model ...\n",
      "Current loss:  2.561203718185425  | , previous best loss:  2.561556100845337  | saving best model ...\n",
      "Current loss:  2.5608508586883545  | , previous best loss:  2.561203718185425  | saving best model ...\n",
      "Current loss:  2.5604984760284424  | , previous best loss:  2.5608508586883545  | saving best model ...\n",
      "Current loss:  2.5601460933685303  | , previous best loss:  2.5604984760284424  | saving best model ...\n",
      "Current loss:  2.559793710708618  | , previous best loss:  2.5601460933685303  | saving best model ...\n",
      "Current loss:  2.559441089630127  | , previous best loss:  2.559793710708618  | saving best model ...\n",
      "Current loss:  2.559088706970215  | , previous best loss:  2.559441089630127  | saving best model ...\n",
      "Current loss:  2.558736562728882  | , previous best loss:  2.559088706970215  | saving best model ...\n",
      "Current loss:  2.5583841800689697  | , previous best loss:  2.558736562728882  | saving best model ...\n",
      "Current loss:  2.5580320358276367  | , previous best loss:  2.5583841800689697  | saving best model ...\n",
      "Current loss:  2.5576798915863037  | , previous best loss:  2.5580320358276367  | saving best model ...\n",
      "Current loss:  2.5573275089263916  | , previous best loss:  2.5576798915863037  | saving best model ...\n",
      "Current loss:  2.556975841522217  | , previous best loss:  2.5573275089263916  | saving best model ...\n",
      "Current loss:  2.5566234588623047  | , previous best loss:  2.556975841522217  | saving best model ...\n",
      "Current loss:  2.55627179145813  | , previous best loss:  2.5566234588623047  | saving best model ...\n",
      "Current loss:  2.5559194087982178  | , previous best loss:  2.55627179145813  | saving best model ...\n",
      "Current loss:  2.555567741394043  | , previous best loss:  2.5559194087982178  | saving best model ...\n",
      "Current loss:  2.555216073989868  | , previous best loss:  2.555567741394043  | saving best model ...\n",
      "Current loss:  2.554863691329956  | , previous best loss:  2.555216073989868  | saving best model ...\n",
      "Current loss:  2.5545120239257812  | , previous best loss:  2.554863691329956  | saving best model ...\n",
      "Current loss:  2.5541601181030273  | , previous best loss:  2.5545120239257812  | saving best model ...\n",
      "Current loss:  2.5538086891174316  | , previous best loss:  2.5541601181030273  | saving best model ...\n",
      "Current loss:  2.553457021713257  | , previous best loss:  2.5538086891174316  | saving best model ...\n",
      "Current loss:  2.553105115890503  | , previous best loss:  2.553457021713257  | saving best model ...\n",
      "Current loss:  2.5527536869049072  | , previous best loss:  2.553105115890503  | saving best model ...\n",
      "Current loss:  2.5524020195007324  | , previous best loss:  2.5527536869049072  | saving best model ...\n",
      "Current loss:  2.5520505905151367  | , previous best loss:  2.5524020195007324  | saving best model ...\n",
      "Current loss:  2.551698923110962  | , previous best loss:  2.5520505905151367  | saving best model ...\n",
      "Current loss:  2.551347494125366  | , previous best loss:  2.551698923110962  | saving best model ...\n",
      "Current loss:  2.5509960651397705  | , previous best loss:  2.551347494125366  | saving best model ...\n",
      "Current loss:  2.5506443977355957  | , previous best loss:  2.5509960651397705  | saving best model ...\n",
      "Current loss:  2.55029296875  | , previous best loss:  2.5506443977355957  | saving best model ...\n",
      "Current loss:  2.5499417781829834  | , previous best loss:  2.55029296875  | saving best model ...\n",
      "Current loss:  2.549590587615967  | , previous best loss:  2.5499417781829834  | saving best model ...\n",
      "Current loss:  2.549239158630371  | , previous best loss:  2.549590587615967  | saving best model ...\n",
      "Current loss:  2.5488879680633545  | , previous best loss:  2.549239158630371  | saving best model ...\n",
      "Current loss:  2.548536539077759  | , previous best loss:  2.5488879680633545  | saving best model ...\n",
      "Current loss:  2.548185110092163  | , previous best loss:  2.548536539077759  | saving best model ...\n",
      "Current loss:  2.5478341579437256  | , previous best loss:  2.548185110092163  | saving best model ...\n",
      "Current loss:  2.547482967376709  | , previous best loss:  2.5478341579437256  | saving best model ...\n",
      "Current loss:  2.5471320152282715  | , previous best loss:  2.547482967376709  | saving best model ...\n",
      "Current loss:  2.546781063079834  | , previous best loss:  2.5471320152282715  | saving best model ...\n",
      "Current loss:  2.5464296340942383  | , previous best loss:  2.546781063079834  | saving best model ...\n",
      "Current loss:  2.54607892036438  | , previous best loss:  2.5464296340942383  | saving best model ...\n",
      "Current loss:  2.5457277297973633  | , previous best loss:  2.54607892036438  | saving best model ...\n",
      "Current loss:  2.5453765392303467  | , previous best loss:  2.5457277297973633  | saving best model ...\n",
      "Current loss:  2.5450258255004883  | , previous best loss:  2.5453765392303467  | saving best model ...\n",
      "Current loss:  2.54467511177063  | , previous best loss:  2.5450258255004883  | saving best model ...\n",
      "Current loss:  2.5443241596221924  | , previous best loss:  2.54467511177063  | saving best model ...\n",
      "Current loss:  2.543973207473755  | , previous best loss:  2.5443241596221924  | saving best model ...\n",
      "Current loss:  2.5436220169067383  | , previous best loss:  2.543973207473755  | saving best model ...\n",
      "Current loss:  2.54327130317688  | , previous best loss:  2.5436220169067383  | saving best model ...\n",
      "Current loss:  2.5429205894470215  | , previous best loss:  2.54327130317688  | saving best model ...\n",
      "Current loss:  2.542569875717163  | , previous best loss:  2.5429205894470215  | saving best model ...\n",
      "Current loss:  2.5422189235687256  | , previous best loss:  2.542569875717163  | saving best model ...\n",
      "Current loss:  2.541868209838867  | , previous best loss:  2.5422189235687256  | saving best model ...\n",
      "Current loss:  2.541517734527588  | , previous best loss:  2.541868209838867  | saving best model ...\n",
      "Current loss:  2.5411672592163086  | , previous best loss:  2.541517734527588  | saving best model ...\n",
      "Current loss:  2.540816307067871  | , previous best loss:  2.5411672592163086  | saving best model ...\n",
      "Current loss:  2.5404655933380127  | , previous best loss:  2.540816307067871  | saving best model ...\n",
      "Current loss:  2.5401151180267334  | , previous best loss:  2.5404655933380127  | saving best model ...\n",
      "Current loss:  2.539764404296875  | , previous best loss:  2.5401151180267334  | saving best model ...\n",
      "Current loss:  2.5394139289855957  | , previous best loss:  2.539764404296875  | saving best model ...\n",
      "Current loss:  2.5390634536743164  | , previous best loss:  2.5394139289855957  | saving best model ...\n",
      "Current loss:  2.538712978363037  | , previous best loss:  2.5390634536743164  | saving best model ...\n",
      "Current loss:  2.5383622646331787  | , previous best loss:  2.538712978363037  | saving best model ...\n",
      "Current loss:  2.5380120277404785  | , previous best loss:  2.5383622646331787  | saving best model ...\n",
      "Current loss:  2.53766131401062  | , previous best loss:  2.5380120277404785  | saving best model ...\n",
      "Current loss:  2.537310838699341  | , previous best loss:  2.53766131401062  | saving best model ...\n",
      "Current loss:  2.5369601249694824  | , previous best loss:  2.537310838699341  | saving best model ...\n",
      "Current loss:  2.5366103649139404  | , previous best loss:  2.5369601249694824  | saving best model ...\n",
      "Current loss:  2.536259651184082  | , previous best loss:  2.5366103649139404  | saving best model ...\n",
      "Current loss:  2.5359091758728027  | , previous best loss:  2.536259651184082  | saving best model ...\n",
      "Current loss:  2.5355591773986816  | , previous best loss:  2.5359091758728027  | saving best model ...\n",
      "Current loss:  2.5352084636688232  | , previous best loss:  2.5355591773986816  | saving best model ...\n",
      "Current loss:  2.534858226776123  | , previous best loss:  2.5352084636688232  | saving best model ...\n",
      "Current loss:  2.5345077514648438  | , previous best loss:  2.534858226776123  | saving best model ...\n",
      "Current loss:  2.5341577529907227  | , previous best loss:  2.5345077514648438  | saving best model ...\n",
      "Current loss:  2.5338070392608643  | , previous best loss:  2.5341577529907227  | saving best model ...\n",
      "Current loss:  2.533457040786743  | , previous best loss:  2.5338070392608643  | saving best model ...\n",
      "Current loss:  2.533107042312622  | , previous best loss:  2.533457040786743  | saving best model ...\n",
      "Current loss:  2.5327565670013428  | , previous best loss:  2.533107042312622  | saving best model ...\n",
      "Current loss:  2.5324063301086426  | , previous best loss:  2.5327565670013428  | saving best model ...\n",
      "Current loss:  2.5320558547973633  | , previous best loss:  2.5324063301086426  | saving best model ...\n",
      "Current loss:  2.5317060947418213  | , previous best loss:  2.5320558547973633  | saving best model ...\n",
      "Current loss:  2.531355619430542  | , previous best loss:  2.5317060947418213  | saving best model ...\n",
      "Current loss:  2.531005859375  | , previous best loss:  2.531355619430542  | saving best model ...\n",
      "Current loss:  2.5306556224823  | , previous best loss:  2.531005859375  | saving best model ...\n",
      "Current loss:  2.5303053855895996  | , previous best loss:  2.5306556224823  | saving best model ...\n",
      "Current loss:  2.5299551486968994  | , previous best loss:  2.5303053855895996  | saving best model ...\n",
      "Current loss:  2.529604911804199  | , previous best loss:  2.5299551486968994  | saving best model ...\n",
      "Current loss:  2.529254913330078  | , previous best loss:  2.529604911804199  | saving best model ...\n",
      "Current loss:  2.528904676437378  | , previous best loss:  2.529254913330078  | saving best model ...\n",
      "Current loss:  2.528554916381836  | , previous best loss:  2.528904676437378  | saving best model ...\n",
      "Current loss:  2.5282046794891357  | , previous best loss:  2.528554916381836  | saving best model ...\n",
      "Current loss:  2.5278546810150146  | , previous best loss:  2.5282046794891357  | saving best model ...\n",
      "Current loss:  2.5275049209594727  | , previous best loss:  2.5278546810150146  | saving best model ...\n",
      "Current loss:  2.5271544456481934  | , previous best loss:  2.5275049209594727  | saving best model ...\n",
      "Current loss:  2.5268044471740723  | , previous best loss:  2.5271544456481934  | saving best model ...\n",
      "Current loss:  2.5264546871185303  | , previous best loss:  2.5268044471740723  | saving best model ...\n",
      "Current loss:  2.526104688644409  | , previous best loss:  2.5264546871185303  | saving best model ...\n",
      "Current loss:  2.525754451751709  | , previous best loss:  2.526104688644409  | saving best model ...\n",
      "Current loss:  2.525404453277588  | , previous best loss:  2.525754451751709  | saving best model ...\n",
      "Current loss:  2.5250542163848877  | , previous best loss:  2.525404453277588  | saving best model ...\n",
      "Current loss:  2.5247044563293457  | , previous best loss:  2.5250542163848877  | saving best model ...\n",
      "Current loss:  2.5243544578552246  | , previous best loss:  2.5247044563293457  | saving best model ...\n",
      "Current loss:  2.5240042209625244  | , previous best loss:  2.5243544578552246  | saving best model ...\n",
      "Current loss:  2.5236544609069824  | , previous best loss:  2.5240042209625244  | saving best model ...\n",
      "Current loss:  2.5233044624328613  | , previous best loss:  2.5236544609069824  | saving best model ...\n",
      "Current loss:  2.5229547023773193  | , previous best loss:  2.5233044624328613  | saving best model ...\n",
      "Current loss:  2.522604465484619  | , previous best loss:  2.5229547023773193  | saving best model ...\n",
      "Current loss:  2.522254467010498  | , previous best loss:  2.522604465484619  | saving best model ...\n",
      "Current loss:  2.521904468536377  | , previous best loss:  2.522254467010498  | saving best model ...\n",
      "Current loss:  2.521554708480835  | , previous best loss:  2.521904468536377  | saving best model ...\n",
      "Current loss:  2.521205186843872  | , previous best loss:  2.521554708480835  | saving best model ...\n",
      "Current loss:  2.520854949951172  | , previous best loss:  2.521205186843872  | saving best model ...\n",
      "Current loss:  2.520504951477051  | , previous best loss:  2.520854949951172  | saving best model ...\n",
      "Current loss:  2.5201549530029297  | , previous best loss:  2.520504951477051  | saving best model ...\n",
      "Current loss:  2.5198049545288086  | , previous best loss:  2.5201549530029297  | saving best model ...\n",
      "Current loss:  2.5194547176361084  | , previous best loss:  2.5198049545288086  | saving best model ...\n",
      "Current loss:  2.5191049575805664  | , previous best loss:  2.5194547176361084  | saving best model ...\n",
      "Current loss:  2.5187554359436035  | , previous best loss:  2.5191049575805664  | saving best model ...\n",
      "Current loss:  2.5184054374694824  | , previous best loss:  2.5187554359436035  | saving best model ...\n",
      "Current loss:  2.5180554389953613  | , previous best loss:  2.5184054374694824  | saving best model ...\n",
      "Current loss:  2.5177056789398193  | , previous best loss:  2.5180554389953613  | saving best model ...\n",
      "Current loss:  2.517355442047119  | , previous best loss:  2.5177056789398193  | saving best model ...\n",
      "Current loss:  2.517005443572998  | , previous best loss:  2.517355442047119  | saving best model ...\n",
      "Current loss:  2.516655683517456  | , previous best loss:  2.517005443572998  | saving best model ...\n",
      "Current loss:  2.516305685043335  | , previous best loss:  2.516655683517456  | saving best model ...\n",
      "Current loss:  2.515955924987793  | , previous best loss:  2.516305685043335  | saving best model ...\n",
      "Current loss:  2.515605926513672  | , previous best loss:  2.515955924987793  | saving best model ...\n",
      "Current loss:  2.515256404876709  | , previous best loss:  2.515605926513672  | saving best model ...\n",
      "Current loss:  2.514906167984009  | , previous best loss:  2.515256404876709  | saving best model ...\n",
      "Current loss:  2.5145561695098877  | , previous best loss:  2.514906167984009  | saving best model ...\n",
      "Current loss:  2.514206647872925  | , previous best loss:  2.5145561695098877  | saving best model ...\n",
      "Current loss:  2.5138564109802246  | , previous best loss:  2.514206647872925  | saving best model ...\n",
      "Current loss:  2.5135066509246826  | , previous best loss:  2.5138564109802246  | saving best model ...\n",
      "Current loss:  2.5131566524505615  | , previous best loss:  2.5135066509246826  | saving best model ...\n",
      "Current loss:  2.5128068923950195  | , previous best loss:  2.5131566524505615  | saving best model ...\n",
      "Current loss:  2.5124566555023193  | , previous best loss:  2.5128068923950195  | saving best model ...\n",
      "Current loss:  2.5121068954467773  | , previous best loss:  2.5124566555023193  | saving best model ...\n",
      "Current loss:  2.5117568969726562  | , previous best loss:  2.5121068954467773  | saving best model ...\n",
      "Current loss:  2.511406898498535  | , previous best loss:  2.5117568969726562  | saving best model ...\n",
      "Current loss:  2.511056661605835  | , previous best loss:  2.511406898498535  | saving best model ...\n",
      "Current loss:  2.510707378387451  | , previous best loss:  2.511056661605835  | saving best model ...\n",
      "Current loss:  2.510356903076172  | , previous best loss:  2.510707378387451  | saving best model ...\n",
      "Current loss:  2.51000714302063  | , previous best loss:  2.510356903076172  | saving best model ...\n",
      "Current loss:  2.509657382965088  | , previous best loss:  2.51000714302063  | saving best model ...\n",
      "Current loss:  2.5093071460723877  | , previous best loss:  2.509657382965088  | saving best model ...\n",
      "Current loss:  2.5089573860168457  | , previous best loss:  2.5093071460723877  | saving best model ...\n",
      "Current loss:  2.5086071491241455  | , previous best loss:  2.5089573860168457  | saving best model ...\n",
      "Current loss:  2.5082573890686035  | , previous best loss:  2.5086071491241455  | saving best model ...\n",
      "Current loss:  2.5079071521759033  | , previous best loss:  2.5082573890686035  | saving best model ...\n",
      "Current loss:  2.5075573921203613  | , previous best loss:  2.5079071521759033  | saving best model ...\n",
      "Current loss:  2.5072076320648193  | , previous best loss:  2.5075573921203613  | saving best model ...\n",
      "Current loss:  2.506857395172119  | , previous best loss:  2.5072076320648193  | saving best model ...\n",
      "Current loss:  2.506507396697998  | , previous best loss:  2.506857395172119  | saving best model ...\n",
      "Current loss:  2.506157159805298  | , previous best loss:  2.506507396697998  | saving best model ...\n",
      "Current loss:  2.505807399749756  | , previous best loss:  2.506157159805298  | saving best model ...\n",
      "Current loss:  2.5054569244384766  | , previous best loss:  2.505807399749756  | saving best model ...\n",
      "Current loss:  2.5051074028015137  | , previous best loss:  2.5054569244384766  | saving best model ...\n",
      "Current loss:  2.5047571659088135  | , previous best loss:  2.5051074028015137  | saving best model ...\n",
      "Current loss:  2.5044069290161133  | , previous best loss:  2.5047571659088135  | saving best model ...\n",
      "Current loss:  2.504056930541992  | , previous best loss:  2.5044069290161133  | saving best model ...\n",
      "Current loss:  2.503706932067871  | , previous best loss:  2.504056930541992  | saving best model ...\n",
      "Current loss:  2.503356695175171  | , previous best loss:  2.503706932067871  | saving best model ...\n",
      "Current loss:  2.50300669670105  | , previous best loss:  2.503356695175171  | saving best model ...\n",
      "Current loss:  2.5026564598083496  | , previous best loss:  2.50300669670105  | saving best model ...\n",
      "Current loss:  2.5023066997528076  | , previous best loss:  2.5026564598083496  | saving best model ...\n",
      "Current loss:  2.501955986022949  | , previous best loss:  2.5023066997528076  | saving best model ...\n",
      "Current loss:  2.5016062259674072  | , previous best loss:  2.501955986022949  | saving best model ...\n",
      "Current loss:  2.501255989074707  | , previous best loss:  2.5016062259674072  | saving best model ...\n",
      "Current loss:  2.500905752182007  | , previous best loss:  2.501255989074707  | saving best model ...\n",
      "Current loss:  2.5005557537078857  | , previous best loss:  2.500905752182007  | saving best model ...\n",
      "Current loss:  2.5002055168151855  | , previous best loss:  2.5005557537078857  | saving best model ...\n",
      "Current loss:  2.4998552799224854  | , previous best loss:  2.5002055168151855  | saving best model ...\n",
      "Current loss:  2.499504804611206  | , previous best loss:  2.4998552799224854  | saving best model ...\n",
      "Current loss:  2.499154806137085  | , previous best loss:  2.499504804611206  | saving best model ...\n",
      "Current loss:  2.498805046081543  | , previous best loss:  2.499154806137085  | saving best model ...\n",
      "Current loss:  2.4984543323516846  | , previous best loss:  2.498805046081543  | saving best model ...\n",
      "Current loss:  2.4981040954589844  | , previous best loss:  2.4984543323516846  | saving best model ...\n",
      "Current loss:  2.497753858566284  | , previous best loss:  2.4981040954589844  | saving best model ...\n",
      "Current loss:  2.497403383255005  | , previous best loss:  2.497753858566284  | saving best model ...\n",
      "Current loss:  2.4970526695251465  | , previous best loss:  2.497403383255005  | saving best model ...\n",
      "Current loss:  2.496702194213867  | , previous best loss:  2.4970526695251465  | saving best model ...\n",
      "Current loss:  2.4963512420654297  | , previous best loss:  2.496702194213867  | saving best model ...\n",
      "Current loss:  2.4960005283355713  | , previous best loss:  2.4963512420654297  | saving best model ...\n",
      "Current loss:  2.495649814605713  | , previous best loss:  2.4960005283355713  | saving best model ...\n",
      "Current loss:  2.4952988624572754  | , previous best loss:  2.495649814605713  | saving best model ...\n",
      "Current loss:  2.494947910308838  | , previous best loss:  2.4952988624572754  | saving best model ...\n",
      "Current loss:  2.4945971965789795  | , previous best loss:  2.494947910308838  | saving best model ...\n",
      "Current loss:  2.494246244430542  | , previous best loss:  2.4945971965789795  | saving best model ...\n",
      "Current loss:  2.4938955307006836  | , previous best loss:  2.494246244430542  | saving best model ...\n",
      "Current loss:  2.493544101715088  | , previous best loss:  2.4938955307006836  | saving best model ...\n",
      "Current loss:  2.4931933879852295  | , previous best loss:  2.493544101715088  | saving best model ...\n",
      "Current loss:  2.492842435836792  | , previous best loss:  2.4931933879852295  | saving best model ...\n",
      "Current loss:  2.4924912452697754  | , previous best loss:  2.492842435836792  | saving best model ...\n",
      "Current loss:  2.492140054702759  | , previous best loss:  2.4924912452697754  | saving best model ...\n",
      "Current loss:  2.4917891025543213  | , previous best loss:  2.492140054702759  | saving best model ...\n",
      "Current loss:  2.491438150405884  | , previous best loss:  2.4917891025543213  | saving best model ...\n",
      "Current loss:  2.491086959838867  | , previous best loss:  2.491438150405884  | saving best model ...\n",
      "Current loss:  2.4907357692718506  | , previous best loss:  2.491086959838867  | saving best model ...\n",
      "Current loss:  2.490384578704834  | , previous best loss:  2.4907357692718506  | saving best model ...\n",
      "Current loss:  2.4900333881378174  | , previous best loss:  2.490384578704834  | saving best model ...\n",
      "Current loss:  2.489682197570801  | , previous best loss:  2.4900333881378174  | saving best model ...\n",
      "Current loss:  2.489331007003784  | , previous best loss:  2.489682197570801  | saving best model ...\n",
      "Current loss:  2.4889795780181885  | , previous best loss:  2.489331007003784  | saving best model ...\n",
      "Current loss:  2.488628387451172  | , previous best loss:  2.4889795780181885  | saving best model ...\n",
      "Current loss:  2.4882774353027344  | , previous best loss:  2.488628387451172  | saving best model ...\n",
      "Current loss:  2.4879260063171387  | , previous best loss:  2.4882774353027344  | saving best model ...\n",
      "Current loss:  2.487574815750122  | , previous best loss:  2.4879260063171387  | saving best model ...\n",
      "Current loss:  2.4872233867645264  | , previous best loss:  2.487574815750122  | saving best model ...\n",
      "Current loss:  2.4868719577789307  | , previous best loss:  2.4872233867645264  | saving best model ...\n",
      "Current loss:  2.486520528793335  | , previous best loss:  2.4868719577789307  | saving best model ...\n",
      "Current loss:  2.4861693382263184  | , previous best loss:  2.486520528793335  | saving best model ...\n",
      "Current loss:  2.4858179092407227  | , previous best loss:  2.4861693382263184  | saving best model ...\n",
      "Current loss:  2.485466480255127  | , previous best loss:  2.4858179092407227  | saving best model ...\n",
      "Current loss:  2.4851152896881104  | , previous best loss:  2.485466480255127  | saving best model ...\n",
      "Current loss:  2.4847636222839355  | , previous best loss:  2.4851152896881104  | saving best model ...\n",
      "Current loss:  2.484412670135498  | , previous best loss:  2.4847636222839355  | saving best model ...\n",
      "Current loss:  2.484060764312744  | , previous best loss:  2.484412670135498  | saving best model ...\n",
      "Current loss:  2.4837093353271484  | , previous best loss:  2.484060764312744  | saving best model ...\n",
      "Current loss:  2.4833576679229736  | , previous best loss:  2.4837093353271484  | saving best model ...\n",
      "Current loss:  2.483006238937378  | , previous best loss:  2.4833576679229736  | saving best model ...\n",
      "Current loss:  2.4826548099517822  | , previous best loss:  2.483006238937378  | saving best model ...\n",
      "Current loss:  2.4823033809661865  | , previous best loss:  2.4826548099517822  | saving best model ...\n",
      "Current loss:  2.481951951980591  | , previous best loss:  2.4823033809661865  | saving best model ...\n",
      "Current loss:  2.481600046157837  | , previous best loss:  2.481951951980591  | saving best model ...\n",
      "Current loss:  2.4812488555908203  | , previous best loss:  2.481600046157837  | saving best model ...\n",
      "Current loss:  2.4808971881866455  | , previous best loss:  2.4812488555908203  | saving best model ...\n",
      "Current loss:  2.48054575920105  | , previous best loss:  2.4808971881866455  | saving best model ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Iteration):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Forward pass: Compute predicted y by passing x to the modelsp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     pyb_af \u001b[38;5;241m=\u001b[39m MBS(X_train)\n\u001b[1;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_train, pyb_af); bloss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m ((bloss_list[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39mbloss_list[t])\u001b[38;5;241m<\u001b[39mtor):        \n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 177\u001b[0m, in \u001b[0;36mMPSv3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m batch_size, _ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# # # # # # # # # # # # # #\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m#         SPLINE 1        #\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# # # # # # # # # # # # # #\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m sp1out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp1(ln1out)\n\u001b[1;32m    178\u001b[0m bslist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp1\u001b[38;5;241m.\u001b[39minter[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasic\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minter[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mebasic\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m bslist\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 123\u001b[0m, in \u001b[0;36mPRODBSplineLayerMultiFeature.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    118\u001b[0m basises \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_features):\n\u001b[1;32m    122\u001b[0m \t\u001b[38;5;66;03m# Calculate B-spline basis functions for this feature\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \tbasis \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis_function(x[:, feature], i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree, knots) \n\u001b[1;32m    124\u001b[0m \t\t\t\t\t\t \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_knots)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    125\u001b[0m \tbasises\u001b[38;5;241m.\u001b[39mappend(basis)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_features \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[0;32mIn[4], line 98\u001b[0m, in \u001b[0;36mPRODBSplineLayerMultiFeature.basis_function\u001b[0;34m(self, x, i, k, t)\u001b[0m\n\u001b[1;32m     96\u001b[0m term1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m denom1 \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 98\u001b[0m \tterm1 \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m t[i]) \u001b[38;5;241m/\u001b[39m denom1 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis_function(x, i, k \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, t)\n\u001b[1;32m    100\u001b[0m term2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m denom2 \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[4], line 102\u001b[0m, in \u001b[0;36mPRODBSplineLayerMultiFeature.basis_function\u001b[0;34m(self, x, i, k, t)\u001b[0m\n\u001b[1;32m    100\u001b[0m term2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m denom2 \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m \tterm2 \u001b[38;5;241m=\u001b[39m (t[i \u001b[38;5;241m+\u001b[39m k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m x) \u001b[38;5;241m/\u001b[39m denom2 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis_function(x, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, k \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, t)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m term1 \u001b[38;5;241m+\u001b[39m term2\n",
      "Cell \u001b[0;32mIn[4], line 98\u001b[0m, in \u001b[0;36mPRODBSplineLayerMultiFeature.basis_function\u001b[0;34m(self, x, i, k, t)\u001b[0m\n\u001b[1;32m     96\u001b[0m term1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m denom1 \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 98\u001b[0m \tterm1 \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m t[i]) \u001b[38;5;241m/\u001b[39m denom1 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis_function(x, i, k \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, t)\n\u001b[1;32m    100\u001b[0m term2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m denom2 \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[4], line 86\u001b[0m, in \u001b[0;36mPRODBSplineLayerMultiFeature.basis_function\u001b[0;34m(self, x, i, k, t)\u001b[0m\n\u001b[1;32m     82\u001b[0m \t\t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     84\u001b[0m \t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbasis_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, i, k, t):\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \t\u001b[38;5;66;03m# Base case: degree 0 spline\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     90\u001b[0m \t\t\u001b[38;5;28;01mreturn\u001b[39;00m ((t[i] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m x) \u001b[38;5;241m&\u001b[39m (x \u001b[38;5;241m<\u001b[39m t[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(Iteration):\n",
    "    # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "    pyb_af = MBS(X_train)\n",
    "    loss = criterion(y_train, pyb_af); bloss_list.append(loss.item())\n",
    "    \n",
    "    if (t > 0) and ((bloss_list[t-1]-bloss_list[t])<tor):        \n",
    "        if (tpat != 0) and (tpat % patientr) == 0:\n",
    "            learning_r *= 0.2 \n",
    "            tpat += 1\n",
    "            #print('Learning rate reduce to ', learning_r)\n",
    "            optimizer = torch.optim.Adam(MBS.parameters(), lr=learning_r)\n",
    "            if learning_r <= lr_tor:\n",
    "                if t < patientc + 1:\n",
    "                    conv = False\n",
    "                else:\n",
    "                    conv = True\n",
    "                print('Convergence!')\n",
    "                break\n",
    "        elif tpat < patientc:\n",
    "            tpat += 1\n",
    "            pass\n",
    "        else:\n",
    "            if t < patientc + 1:\n",
    "                conv = False\n",
    "            else:\n",
    "                conv = True\n",
    "            print('Convergence!')\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        if loss < bloss:\n",
    "            print('Current loss: ', loss.item(), ' | , previous best loss: ', bloss, ' | saving best model ...')\n",
    "            torch.save(MBS.state_dict(), './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1))\n",
    "            bloss = loss.item()\n",
    "            tpat = 0\n",
    "        else:\n",
    "            tpat += 1\n",
    "    \n",
    "    if tpat == patientc:\n",
    "        if t < patientc + 1:\n",
    "            conv = False\n",
    "        else:\n",
    "            conv = True\n",
    "            print('Convergence!')\n",
    "        break\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca48c62-508a-4fc7-a54d-ae5fd88b671d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabec794-0371-428a-b207-045dd5623b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f8ab738-3e56-4e71-bae9-b68792a2d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    eval_model = MPSv3(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, output_dim = Fout, bias = True).to(device)\n",
    "    eval_model.load_state_dict(torch.load( './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1), weights_only = True))\n",
    "    MPSy = eval_model(X_train)\n",
    "    LambdaB = ECM(model = eval_model, num_neurons = nm, num_knots = nk)\n",
    "    Lambdalist[str(d+1)] = LambdaB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d902f267-cdaa-49ff-a1ff-cf668f90adcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4805)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(MPSy, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbfeb0-6fd8-45a6-95f7-ecfd487e887b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e937f0f-b150-4d0b-b4cc-b37c1f8e2604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTORCH",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
