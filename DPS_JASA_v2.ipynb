{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "16d1745a-f822-44f1-af6d-7cf0b4f382ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sim_data(n, dim, Type):\n",
    "\tif Type == 'A':\n",
    "\t\tX = torch.rand((n,dim))\n",
    "\t\ty = torch.exp(2*torch.sin(X[:,0]*0.5*torch.pi)+ 0.5*torch.cos(X[:,1]*2.5*torch.pi))\n",
    "\t\ty = y.reshape(-1,1)\n",
    "\t\ty = y.float()\n",
    "\t\t\n",
    "\telif Type == 'B':\n",
    "\t\tX = torch.rand((n, dim))\n",
    "\t\ty = 1\n",
    "\t\tfor d in range(dim):\n",
    "\t\t\ta = (d+1)/2\n",
    "\t\t\ty *= ((torch.abs(4*X[:,d]-2)+a)/(1+a))\n",
    "\t\ty = y.reshape(-1,1)\n",
    "\t\ty = y.float()\n",
    "\telse:\n",
    "\t\tpass\n",
    "\t\n",
    "\treturn X, y\n",
    "\t\n",
    "def norm(x):\n",
    "\treturn (x-torch.min(x))/(torch.max(x)-torch.min(x))\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def diag_mat_weights(dimp, type = 'first'):\n",
    "\tif type == 'first':\n",
    "\t\tdg = np.zeros((dimp-1, dimp))\n",
    "\t\tfor i in range(dimp-1):\n",
    "\t\t\tdg[i,i] = -1\n",
    "\t\t\tdg[i,i+1]= 1\n",
    "\telif type == 'second':\n",
    "\t\tdg = np.zeros((dimp-2, dimp))\n",
    "\t\tfor i in range(dimp-2):\n",
    "\t\t\tdg[i,i] = -1\n",
    "\t\t\tdg[i,i+1]= 2\n",
    "\t\t\tdg[i,i+2]= -1\n",
    "\telse:\n",
    "\t\tpass\n",
    "\treturn torch.Tensor(dg)\n",
    "\n",
    "def num_para(model):\n",
    "\ttp = 0\n",
    "\tfor param in model.parameters():\n",
    "\t\ttp += param.numel()\n",
    "\treturn tp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba56b5bf-a574-4b6b-94e4-5f69d6d294f7",
   "metadata": {},
   "source": [
    "## ECM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "cac109e3-c01b-4a96-a6dc-7dc2427c54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ECM(par, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4):\n",
    "    lambdab = initial_lambda\n",
    "    sigma = initial_sigma\n",
    "    xi = initial_xi\n",
    "    \n",
    "    n_block, num_knots, num_neurons = par['wbasic'].size()\n",
    "    ls_lambda = torch.empty(n_block)\n",
    "    \n",
    "    for l in range(n_block):\n",
    "        B = par['ebasic'][l]\n",
    "        By = par['basic'][l]\n",
    "        WB = par['wbasic'][l]\n",
    "        \n",
    "        DB = diag_mat_weights(WB.size()[0]).to(device)\n",
    "        size = B.size()[1]\n",
    "        S = DB.T @ DB\n",
    "        Cov_a = (xi**2)* torch.linalg.pinv(S)\n",
    "        Cov_e = torch.eye(size*num_neurons)* sigma\n",
    "        \n",
    "        block_y = torch.reshape(By, (-1,1))\n",
    "        flatB = B.view(num_neurons, num_knots, size)\n",
    "            \n",
    "        sqr_xi= 0\n",
    "        sqr_sig = 0\n",
    "        \n",
    "        for i in range(num_neurons):\n",
    "            Ncov = (Cov_a -(Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)]) @ flatB[i].T @ Cov_a))\n",
    "            Nmu = (Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)])) @ By[:,i].reshape(-1,1)\n",
    "            \n",
    "            first_xi = S @ Ncov\n",
    "            second_xi = (Nmu.T @ S @ Nmu)\n",
    "            sqr_xi += torch.trace(first_xi) + second_xi\n",
    "                \n",
    "            first_sig = torch.norm(By[:,i])\n",
    "            second_sig = 2 * (By[:,i] @ flatB[i].T) @ Nmu \n",
    "            third_sig = torch.trace((flatB[i] @ flatB[i].T) @ Ncov)\n",
    "            four_sig = (Nmu.T @ flatB[i] @ flatB[i].T @ Nmu)\n",
    "            \n",
    "            sqr_sig += (first_sig + second_sig + third_sig + four_sig)\n",
    "        \n",
    "        sqr_xi /= num_neurons\n",
    "        sqr_sig /= (num_neurons*size)\n",
    "        \n",
    "        ls_lambda[l] = (sqr_sig/sqr_xi).item()\n",
    "    \n",
    "    return ls_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb6ac5-a0e0-460c-8819-abd99bf34073",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "2d5b9ef9-aced-4426-b106-08585f6cce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSL(nn.Module):\n",
    "    def __init__(self, degree, num_knots, num_neurons, bias = True):\n",
    "        super(BSL, self).__init__()\n",
    "        self.degree = degree\n",
    "        self.num_knots = num_knots\n",
    "        self.num_neurons = num_neurons\n",
    "        self.control_p = nn.Parameter(torch.randn(self.num_knots, self.num_neurons))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(self.num_neurons))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.inter = {}\n",
    "    \n",
    "    def basis_function(self, x, i, k, t):\n",
    "    \n",
    "        # Base case: degree 0 spline\n",
    "        if k == 0:\n",
    "            return ((t[i] <= x) & (x < t[i + 1])).float()\n",
    "    \n",
    "        # Recursive case\n",
    "        denom1 = t[i + k] - t[i]\n",
    "        denom2 = t[i + k + 1] - t[i + 1]\n",
    "    \n",
    "        term1 = 0\n",
    "        if denom1 != 0:\n",
    "            term1 = (x - t[i]) / denom1 * self.basis_function(x, i, k - 1, t)\n",
    "    \n",
    "        term2 = 0\n",
    "        if denom2 != 0:\n",
    "            term2 = (t[i + k + 1] - x) / denom2 * self.basis_function(x, i + 1, k - 1, t)\n",
    "    \n",
    "        return term1 + term2\n",
    "\n",
    "    def knots_distribution(self, dg, nk):\n",
    "\n",
    "        knots = torch.cat([torch.linspace(-0.002, -0.001, steps=dg),            # Add repeated values at the start for clamping\n",
    "            torch.linspace(0, 1, nk-2*dg-2),  # Uniform knot spacing in the middle\n",
    "            torch.linspace(1.001, 1.002, steps=dg)           # Add repeated values at the end for clamping\n",
    "            ]).view(-1,1)\n",
    "     \n",
    "        return knots\n",
    "    \n",
    "    def basis_function(self, x, spl):\n",
    "        basis_output = spl.fit_transform(x)\n",
    "        return basis_output\n",
    "            \n",
    "    def forward(self, x):\n",
    "        batch_size, num_features = x.size()\n",
    "        device = x.device\n",
    "        \n",
    "        # Create knot vector and apply B-spline basis functions for each feature\n",
    "        basises = []\n",
    "        knots = self.knots_distribution(self.degree, self.num_knots)\n",
    "        knots = knots.to(device)\n",
    "        spl = SplineTransformer(n_knots=self.num_knots, degree=self.degree, knots = knots)\n",
    "\n",
    "        \n",
    "        for feature in range(num_features):\n",
    "            # Calculate B-spline basis functions for this feature\n",
    "            \n",
    "            basis = self.basis_function(x[:, feature].reshape(-1,1), spl)\n",
    "            basis = torch.Tensor(basis)\n",
    "            basises.append(basis)\n",
    "\n",
    "        if num_features == 1:\n",
    "            tout = basises[0] @ self.control_p\n",
    "            self.inter['basic'] = basises[0].T\n",
    "        else:\n",
    "            self.inter['basic'] = torch.reshape(torch.stack(basises, dim = 1), (batch_size, self.num_knots * self.num_neurons)).T\n",
    "            basises = torch.stack(basises)\n",
    "            tout = basises.permute(1,2,0) * self.control_p\n",
    "            tout = tout.sum(dim =1)\n",
    "                \n",
    "        if self.bias is not None:\n",
    "            tout += self.bias        \n",
    "            \n",
    "        return tout\n",
    "\n",
    "\n",
    "class NormLayer(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(NormLayer, self).__init__()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tmin_val = torch.min(x, axis = 1).values.reshape(-1,1)\n",
    "\t\tmax_val = torch.max(x, axis = 1).values.reshape(-1,1)\n",
    "\n",
    "\t\tx = (x - min_val)/(max_val - min_val)  # Rescale to [0, 1]\n",
    "\t\treturn x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "5ba6d1c7-50bb-4c92-9a2f-5ab66703d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSpline_block(nn.Module):\n",
    "    def __init__(self, degree, num_knots, num_neurons, dropout = 0.0, bias = True):\n",
    "        super(BSpline_block, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(OrderedDict([\n",
    "            ('norm', NormLayer()),\n",
    "            ('BSL', BSL(degree = degree, num_knots = num_knots, num_neurons = num_neurons, bias = bias)),\n",
    "            ('drop', nn.Dropout(dropout)),\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "        \n",
    "class StackBS_block(nn.Module):\n",
    "    def __init__(self, block, degree, num_knots, num_neurons, num_blocks, dropout = 0.0, bias = True):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleDict({\n",
    "            f'block_{i}': block(degree = degree, num_knots = num_knots, num_neurons = num_neurons)\n",
    "            for i in range(num_blocks)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        for name, block in self.model.items():\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "d56f6179-804b-461f-bf1e-8423ab517582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DPS(nn.Module):\n",
    "    def __init__(self, input_dim, degree, num_knots, num_neurons, num_bsl, output_dim, bias):\n",
    "        super(DPS, self).__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_knots = num_knots\n",
    "        self.ln1 = nn.Linear(input_dim, num_neurons)\n",
    "        #self.nm1 = NormLayer() \n",
    "        #self.sp1 = BSL(degree = degree, num_knots = num_knots, num_neurons = num_neurons, bias = True)\n",
    "        self.Spline_block = StackBS_block(BSpline_block, degree = degree, num_knots = num_knots, num_neurons = num_neurons, num_blocks = num_bsl)\n",
    "        self.ln2 = nn.Linear(num_neurons, output_dim)\n",
    "        #self.inter = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.ln1(x)\n",
    "\n",
    "        # # # # # # # # # # # # # #\n",
    "        #          SPLINE         #\n",
    "        # # # # # # # # # # # # # #\n",
    "        \n",
    "        spout = self.Spline_block(x)\n",
    "\n",
    "        '''  \n",
    "        ln1out = self.nm1(ln1out)\n",
    "        device = ln1out.device\n",
    "        batch_size, _ = x.size()\n",
    "        \n",
    "        # # # # # # # # # # # # # #\n",
    "        #          SPLINE         #\n",
    "        # # # # # # # # # # # # # #\n",
    "        \n",
    "        sp1out = self.sp1(ln1out)\n",
    "        bslist = self.sp1.inter['basic']\n",
    "        \n",
    "        self.inter['ebasic'] = bslist\n",
    "        self.inter['basic'] = sp1out\n",
    "        '''\n",
    "        \n",
    "        output = self.ln2(spout)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def get_para_ecm(self, x):\n",
    "\n",
    "        '''\n",
    "        ecm_para: A dictionary that collects the parameter we need to the following ECM algorithm.\n",
    "        ecm_para.basic: Store the output of each B-Spline block; Dimension = [n_sample, n_neurons]\n",
    "        ecm_para.ebasic Store the weight matrix of each B-Spline expansion; Dimension = [n_knots * n_neurons, n_sample]\n",
    "\n",
    "        '''\n",
    "        ecm_para = {}\n",
    "        bs_block_out = {}\n",
    "        bs_spline_weight = {}\n",
    "        bs_spline_value = {}\n",
    "        bs_spline_bias = {}\n",
    "\n",
    "        _ = self(x)\n",
    "        \n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                bs_block_out[name] = output.detach()\n",
    "            return hook\n",
    "\n",
    "        handles = []\n",
    "        for name, layer in self.named_modules():\n",
    "            if 'block.drop' in name:\n",
    "                handles.append(layer.register_forward_hook(get_activation(name)))\n",
    "            elif 'block.BSL' in name:\n",
    "                bs_spline_value[name] = layer.inter['basic'].detach()\n",
    "                bs_spline_weight[name] = layer.control_p.detach()\n",
    "                bs_spline_bias[name] = layer.bias.detach()\n",
    "        # Run forward pass (triggers hooks)\n",
    "        _ = self(x)\n",
    "\n",
    "        # Clean up hooks\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "            \n",
    "        ecm_para['basic'] = torch.stack(list(bs_block_out.values()), dim=0)\n",
    "        ecm_para['ebasic'] = torch.stack(list(bs_spline_value.values()), dim=0)\n",
    "        ecm_para['wbasic'] = torch.stack(list(bs_spline_weight.values()), dim=0)\n",
    "        ecm_para['bbasic'] = torch.stack(list(bs_spline_bias.values()), dim=0)\n",
    "        del bs_block_out, bs_spline_weight, bs_spline_value, bs_spline_bias\n",
    "        \n",
    "        return ecm_para\n",
    "\n",
    "    def fit(self, x):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c23cb-4c93-4fbb-876f-91530c18b96f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "id": "64e443ff-3729-4a1c-85ad-c500872d5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 800; ntest = 2500; ndim = 2; ndf = 1; nk = 15; nm = 50; Fout = 1\n",
    "Dtype = 'A'\n",
    "data = {}\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for d in range(ndf):\n",
    "    X_train, y_train = sim_data(ntrain, ndim, Dtype)\n",
    "    X_test, y_test = sim_data(ntest, ndim, Dtype)\n",
    "    epstrain = torch.normal(0, torch.var(y_train)*0.05, size=y_train.size())\n",
    "    epstest = torch.normal(0,  torch.var(y_train)*0.05, size=y_test.size())\n",
    "    \n",
    "    y_train, y_test = y_train + epstrain, y_test + epstest\n",
    "    data[str(d+1)] = {'TrainX': X_train, 'Trainy': y_train, 'TestX': X_test, 'Testy': y_test}\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328c265-731b-4044-b65b-b6b39792fdae",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "id": "7098de70-4ba8-419a-9dbd-3165171a16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepBS = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = 2, output_dim = Fout, bias = True).to(device)\n",
    "learning_r = 1e-1\n",
    "optimizer = torch.optim.Adam(DeepBS.parameters(), lr=learning_r)\n",
    "Iteration = 10000; bloss_list = []; tor = 1e-5; lr_tor = 1e-6\n",
    "patientc = 30; patientr = 10; tpat = 0; bloss = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "d63a7ea3-6516-4209-8777-64b4d6eab545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss:  39.23202896118164  | , previous best loss:  9999  | saving best model ...\n",
      "Current loss:  10.675946235656738  | , previous best loss:  39.23202896118164  | saving best model ...\n",
      "Current loss:  8.817166328430176  | , previous best loss:  10.675946235656738  | saving best model ...\n",
      "Current loss:  7.756436824798584  | , previous best loss:  8.817166328430176  | saving best model ...\n",
      "Current loss:  5.437385082244873  | , previous best loss:  7.756436824798584  | saving best model ...\n",
      "Current loss:  5.335794448852539  | , previous best loss:  5.437385082244873  | saving best model ...\n",
      "Current loss:  4.226558208465576  | , previous best loss:  5.335794448852539  | saving best model ...\n",
      "Current loss:  3.3025193214416504  | , previous best loss:  4.226558208465576  | saving best model ...\n",
      "Current loss:  2.9383482933044434  | , previous best loss:  3.3025193214416504  | saving best model ...\n",
      "Current loss:  2.4671576023101807  | , previous best loss:  2.9383482933044434  | saving best model ...\n",
      "Current loss:  2.1243348121643066  | , previous best loss:  2.4671576023101807  | saving best model ...\n",
      "Current loss:  1.9939106702804565  | , previous best loss:  2.1243348121643066  | saving best model ...\n",
      "Current loss:  1.6831742525100708  | , previous best loss:  1.9939106702804565  | saving best model ...\n",
      "Current loss:  1.5087904930114746  | , previous best loss:  1.6831742525100708  | saving best model ...\n",
      "Current loss:  1.365594506263733  | , previous best loss:  1.5087904930114746  | saving best model ...\n",
      "Current loss:  1.1748155355453491  | , previous best loss:  1.365594506263733  | saving best model ...\n",
      "Current loss:  1.155928611755371  | , previous best loss:  1.1748155355453491  | saving best model ...\n",
      "Current loss:  1.028439998626709  | , previous best loss:  1.155928611755371  | saving best model ...\n",
      "Current loss:  0.966352105140686  | , previous best loss:  1.028439998626709  | saving best model ...\n",
      "Current loss:  0.9430983662605286  | , previous best loss:  0.966352105140686  | saving best model ...\n",
      "Current loss:  0.8767402768135071  | , previous best loss:  0.9430983662605286  | saving best model ...\n",
      "Current loss:  0.8701077103614807  | , previous best loss:  0.8767402768135071  | saving best model ...\n",
      "Current loss:  0.813210666179657  | , previous best loss:  0.8701077103614807  | saving best model ...\n",
      "Current loss:  0.8086758255958557  | , previous best loss:  0.813210666179657  | saving best model ...\n",
      "Current loss:  0.7674508690834045  | , previous best loss:  0.8086758255958557  | saving best model ...\n",
      "Current loss:  0.7586538195610046  | , previous best loss:  0.7674508690834045  | saving best model ...\n",
      "Current loss:  0.7281907796859741  | , previous best loss:  0.7586538195610046  | saving best model ...\n",
      "Current loss:  0.7150617837905884  | , previous best loss:  0.7281907796859741  | saving best model ...\n",
      "Current loss:  0.6954692602157593  | , previous best loss:  0.7150617837905884  | saving best model ...\n",
      "Current loss:  0.6809917688369751  | , previous best loss:  0.6954692602157593  | saving best model ...\n",
      "Current loss:  0.6687429547309875  | , previous best loss:  0.6809917688369751  | saving best model ...\n",
      "Current loss:  0.6657562255859375  | , previous best loss:  0.6687429547309875  | saving best model ...\n",
      "Current loss:  0.6512895226478577  | , previous best loss:  0.6657562255859375  | saving best model ...\n",
      "Current loss:  0.645802915096283  | , previous best loss:  0.6512895226478577  | saving best model ...\n",
      "Current loss:  0.640347421169281  | , previous best loss:  0.645802915096283  | saving best model ...\n",
      "Current loss:  0.6309558749198914  | , previous best loss:  0.640347421169281  | saving best model ...\n",
      "Current loss:  0.6297949552536011  | , previous best loss:  0.6309558749198914  | saving best model ...\n",
      "Current loss:  0.6286391019821167  | , previous best loss:  0.6297949552536011  | saving best model ...\n",
      "Current loss:  0.6211065053939819  | , previous best loss:  0.6286391019821167  | saving best model ...\n",
      "Current loss:  0.6161478757858276  | , previous best loss:  0.6211065053939819  | saving best model ...\n",
      "Current loss:  0.6159980893135071  | , previous best loss:  0.6161478757858276  | saving best model ...\n",
      "Current loss:  0.6122751832008362  | , previous best loss:  0.6159980893135071  | saving best model ...\n",
      "Current loss:  0.6061785817146301  | , previous best loss:  0.6122751832008362  | saving best model ...\n",
      "Current loss:  0.6040945649147034  | , previous best loss:  0.6061785817146301  | saving best model ...\n",
      "Current loss:  0.6023461222648621  | , previous best loss:  0.6040945649147034  | saving best model ...\n",
      "Current loss:  0.5971941947937012  | , previous best loss:  0.6023461222648621  | saving best model ...\n",
      "Current loss:  0.5934076905250549  | , previous best loss:  0.5971941947937012  | saving best model ...\n",
      "Current loss:  0.592075526714325  | , previous best loss:  0.5934076905250549  | saving best model ...\n",
      "Current loss:  0.5887861847877502  | , previous best loss:  0.592075526714325  | saving best model ...\n",
      "Current loss:  0.5847703814506531  | , previous best loss:  0.5887861847877502  | saving best model ...\n",
      "Current loss:  0.5830265283584595  | , previous best loss:  0.5847703814506531  | saving best model ...\n",
      "Current loss:  0.580920398235321  | , previous best loss:  0.5830265283584595  | saving best model ...\n",
      "Current loss:  0.5772717595100403  | , previous best loss:  0.580920398235321  | saving best model ...\n",
      "Current loss:  0.574868381023407  | , previous best loss:  0.5772717595100403  | saving best model ...\n",
      "Current loss:  0.5731746554374695  | , previous best loss:  0.574868381023407  | saving best model ...\n",
      "Current loss:  0.570203959941864  | , previous best loss:  0.5731746554374695  | saving best model ...\n",
      "Current loss:  0.56756192445755  | , previous best loss:  0.570203959941864  | saving best model ...\n",
      "Current loss:  0.5659406781196594  | , previous best loss:  0.56756192445755  | saving best model ...\n",
      "Current loss:  0.5635737776756287  | , previous best loss:  0.5659406781196594  | saving best model ...\n",
      "Current loss:  0.5609439611434937  | , previous best loss:  0.5635737776756287  | saving best model ...\n",
      "Current loss:  0.5592030882835388  | , previous best loss:  0.5609439611434937  | saving best model ...\n",
      "Current loss:  0.55722576379776  | , previous best loss:  0.5592030882835388  | saving best model ...\n",
      "Current loss:  0.5548062324523926  | , previous best loss:  0.55722576379776  | saving best model ...\n",
      "Current loss:  0.5530221462249756  | , previous best loss:  0.5548062324523926  | saving best model ...\n",
      "Current loss:  0.5513107776641846  | , previous best loss:  0.5530221462249756  | saving best model ...\n",
      "Current loss:  0.5491510629653931  | , previous best loss:  0.5513107776641846  | saving best model ...\n",
      "Current loss:  0.5473545789718628  | , previous best loss:  0.5491510629653931  | saving best model ...\n",
      "Current loss:  0.5457649230957031  | , previous best loss:  0.5473545789718628  | saving best model ...\n",
      "Current loss:  0.5438277721405029  | , previous best loss:  0.5457649230957031  | saving best model ...\n",
      "Current loss:  0.5420939922332764  | , previous best loss:  0.5438277721405029  | saving best model ...\n",
      "Current loss:  0.5406189560890198  | , previous best loss:  0.5420939922332764  | saving best model ...\n",
      "Current loss:  0.5389088988304138  | , previous best loss:  0.5406189560890198  | saving best model ...\n",
      "Current loss:  0.5372959971427917  | , previous best loss:  0.5389088988304138  | saving best model ...\n",
      "Current loss:  0.5359166860580444  | , previous best loss:  0.5372959971427917  | saving best model ...\n",
      "Current loss:  0.5343793630599976  | , previous best loss:  0.5359166860580444  | saving best model ...\n",
      "Current loss:  0.5328856706619263  | , previous best loss:  0.5343793630599976  | saving best model ...\n",
      "Current loss:  0.5315916538238525  | , previous best loss:  0.5328856706619263  | saving best model ...\n",
      "Current loss:  0.5301996469497681  | , previous best loss:  0.5315916538238525  | saving best model ...\n",
      "Current loss:  0.5288292169570923  | , previous best loss:  0.5301996469497681  | saving best model ...\n",
      "Current loss:  0.527616024017334  | , previous best loss:  0.5288292169570923  | saving best model ...\n",
      "Current loss:  0.5263359546661377  | , previous best loss:  0.527616024017334  | saving best model ...\n",
      "Current loss:  0.5250691175460815  | , previous best loss:  0.5263359546661377  | saving best model ...\n",
      "Current loss:  0.5239266753196716  | , previous best loss:  0.5250691175460815  | saving best model ...\n",
      "Current loss:  0.5227423310279846  | , previous best loss:  0.5239266753196716  | saving best model ...\n",
      "Current loss:  0.5215764045715332  | , previous best loss:  0.5227423310279846  | saving best model ...\n",
      "Current loss:  0.5205067992210388  | , previous best loss:  0.5215764045715332  | saving best model ...\n",
      "Current loss:  0.5194053053855896  | , previous best loss:  0.5205067992210388  | saving best model ...\n",
      "Current loss:  0.518321692943573  | , previous best loss:  0.5194053053855896  | saving best model ...\n",
      "Current loss:  0.5173073410987854  | , previous best loss:  0.518321692943573  | saving best model ...\n",
      "Current loss:  0.5162672400474548  | , previous best loss:  0.5173073410987854  | saving best model ...\n",
      "Current loss:  0.5152497291564941  | , previous best loss:  0.5162672400474548  | saving best model ...\n",
      "Current loss:  0.5142838954925537  | , previous best loss:  0.5152497291564941  | saving best model ...\n",
      "Current loss:  0.5132991671562195  | , previous best loss:  0.5142838954925537  | saving best model ...\n",
      "Current loss:  0.5123403668403625  | , previous best loss:  0.5132991671562195  | saving best model ...\n",
      "Current loss:  0.5114172101020813  | , previous best loss:  0.5123403668403625  | saving best model ...\n",
      "Current loss:  0.5104806423187256  | , previous best loss:  0.5114172101020813  | saving best model ...\n",
      "Current loss:  0.5095722079277039  | , previous best loss:  0.5104806423187256  | saving best model ...\n",
      "Current loss:  0.5086870789527893  | , previous best loss:  0.5095722079277039  | saving best model ...\n",
      "Current loss:  0.5077945590019226  | , previous best loss:  0.5086870789527893  | saving best model ...\n",
      "Current loss:  0.5069288611412048  | , previous best loss:  0.5077945590019226  | saving best model ...\n",
      "Current loss:  0.5060756206512451  | , previous best loss:  0.5069288611412048  | saving best model ...\n",
      "Current loss:  0.505219578742981  | , previous best loss:  0.5060756206512451  | saving best model ...\n",
      "Current loss:  0.5043874382972717  | , previous best loss:  0.505219578742981  | saving best model ...\n",
      "Current loss:  0.5035610795021057  | , previous best loss:  0.5043874382972717  | saving best model ...\n",
      "Current loss:  0.5027379393577576  | , previous best loss:  0.5035610795021057  | saving best model ...\n",
      "Current loss:  0.5019344091415405  | , previous best loss:  0.5027379393577576  | saving best model ...\n",
      "Current loss:  0.5011324882507324  | , previous best loss:  0.5019344091415405  | saving best model ...\n",
      "Current loss:  0.500337541103363  | , previous best loss:  0.5011324882507324  | saving best model ...\n",
      "Current loss:  0.4995567202568054  | , previous best loss:  0.500337541103363  | saving best model ...\n",
      "Current loss:  0.4987764358520508  | , previous best loss:  0.4995567202568054  | saving best model ...\n",
      "Current loss:  0.49800586700439453  | , previous best loss:  0.4987764358520508  | saving best model ...\n",
      "Current loss:  0.4972444772720337  | , previous best loss:  0.49800586700439453  | saving best model ...\n",
      "Current loss:  0.4964846074581146  | , previous best loss:  0.4972444772720337  | saving best model ...\n",
      "Current loss:  0.4957348704338074  | , previous best loss:  0.4964846074581146  | saving best model ...\n",
      "Current loss:  0.49499043822288513  | , previous best loss:  0.4957348704338074  | saving best model ...\n",
      "Current loss:  0.49424949288368225  | , previous best loss:  0.49499043822288513  | saving best model ...\n",
      "Current loss:  0.49351778626441956  | , previous best loss:  0.49424949288368225  | saving best model ...\n",
      "Current loss:  0.4927893579006195  | , previous best loss:  0.49351778626441956  | saving best model ...\n",
      "Current loss:  0.492066353559494  | , previous best loss:  0.4927893579006195  | saving best model ...\n",
      "Current loss:  0.49135059118270874  | , previous best loss:  0.492066353559494  | saving best model ...\n",
      "Current loss:  0.4906373918056488  | , previous best loss:  0.49135059118270874  | saving best model ...\n",
      "Current loss:  0.48993080854415894  | , previous best loss:  0.4906373918056488  | saving best model ...\n",
      "Current loss:  0.48922911286354065  | , previous best loss:  0.48993080854415894  | saving best model ...\n",
      "Current loss:  0.4885311424732208  | , previous best loss:  0.48922911286354065  | saving best model ...\n",
      "Current loss:  0.48783960938453674  | , previous best loss:  0.4885311424732208  | saving best model ...\n",
      "Current loss:  0.48715174198150635  | , previous best loss:  0.48783960938453674  | saving best model ...\n",
      "Current loss:  0.4864686131477356  | , previous best loss:  0.48715174198150635  | saving best model ...\n",
      "Current loss:  0.48579084873199463  | , previous best loss:  0.4864686131477356  | saving best model ...\n",
      "Current loss:  0.48511651158332825  | , previous best loss:  0.48579084873199463  | saving best model ...\n",
      "Current loss:  0.48444730043411255  | , previous best loss:  0.48511651158332825  | saving best model ...\n",
      "Current loss:  0.48378250002861023  | , previous best loss:  0.48444730043411255  | saving best model ...\n",
      "Current loss:  0.48312151432037354  | , previous best loss:  0.48378250002861023  | saving best model ...\n",
      "Current loss:  0.4824655055999756  | , previous best loss:  0.48312151432037354  | saving best model ...\n",
      "Current loss:  0.4818131625652313  | , previous best loss:  0.4824655055999756  | saving best model ...\n",
      "Current loss:  0.48116517066955566  | , previous best loss:  0.4818131625652313  | saving best model ...\n",
      "Current loss:  0.48052144050598145  | , previous best loss:  0.48116517066955566  | saving best model ...\n",
      "Current loss:  0.4798814058303833  | , previous best loss:  0.48052144050598145  | saving best model ...\n",
      "Current loss:  0.47924578189849854  | , previous best loss:  0.4798814058303833  | saving best model ...\n",
      "Current loss:  0.47861388325691223  | , previous best loss:  0.47924578189849854  | saving best model ...\n",
      "Current loss:  0.4779859185218811  | , previous best loss:  0.47861388325691223  | saving best model ...\n",
      "Current loss:  0.47736185789108276  | , previous best loss:  0.4779859185218811  | saving best model ...\n",
      "Current loss:  0.47674137353897095  | , previous best loss:  0.47736185789108276  | saving best model ...\n",
      "Current loss:  0.4761248826980591  | , previous best loss:  0.47674137353897095  | saving best model ...\n",
      "Current loss:  0.47551190853118896  | , previous best loss:  0.4761248826980591  | saving best model ...\n",
      "Current loss:  0.47490251064300537  | , previous best loss:  0.47551190853118896  | saving best model ...\n",
      "Current loss:  0.4742968678474426  | , previous best loss:  0.47490251064300537  | saving best model ...\n",
      "Current loss:  0.4736945331096649  | , previous best loss:  0.4742968678474426  | saving best model ...\n",
      "Current loss:  0.4730958640575409  | , previous best loss:  0.4736945331096649  | saving best model ...\n",
      "Current loss:  0.4725005626678467  | , previous best loss:  0.4730958640575409  | saving best model ...\n",
      "Current loss:  0.47190868854522705  | , previous best loss:  0.4725005626678467  | saving best model ...\n",
      "Current loss:  0.4713202714920044  | , previous best loss:  0.47190868854522705  | saving best model ...\n",
      "Current loss:  0.47073501348495483  | , previous best loss:  0.4713202714920044  | saving best model ...\n",
      "Current loss:  0.4701531231403351  | , previous best loss:  0.47073501348495483  | saving best model ...\n",
      "Current loss:  0.46957436203956604  | , previous best loss:  0.4701531231403351  | saving best model ...\n",
      "Current loss:  0.46899881958961487  | , previous best loss:  0.46957436203956604  | saving best model ...\n",
      "Current loss:  0.46842658519744873  | , previous best loss:  0.46899881958961487  | saving best model ...\n",
      "Current loss:  0.46785733103752136  | , previous best loss:  0.46842658519744873  | saving best model ...\n",
      "Current loss:  0.4672912657260895  | , previous best loss:  0.46785733103752136  | saving best model ...\n",
      "Current loss:  0.46672821044921875  | , previous best loss:  0.4672912657260895  | saving best model ...\n",
      "Current loss:  0.46616822481155396  | , previous best loss:  0.46672821044921875  | saving best model ...\n",
      "Current loss:  0.46561118960380554  | , previous best loss:  0.46616822481155396  | saving best model ...\n",
      "Current loss:  0.46505722403526306  | , previous best loss:  0.46561118960380554  | saving best model ...\n",
      "Current loss:  0.464506059885025  | , previous best loss:  0.46505722403526306  | saving best model ...\n",
      "Current loss:  0.463957816362381  | , previous best loss:  0.464506059885025  | saving best model ...\n",
      "Current loss:  0.4634125232696533  | , previous best loss:  0.463957816362381  | saving best model ...\n",
      "Current loss:  0.4628700315952301  | , previous best loss:  0.4634125232696533  | saving best model ...\n",
      "Current loss:  0.46233031153678894  | , previous best loss:  0.4628700315952301  | saving best model ...\n",
      "Current loss:  0.4617934823036194  | , previous best loss:  0.46233031153678894  | saving best model ...\n",
      "Current loss:  0.4612593948841095  | , previous best loss:  0.4617934823036194  | saving best model ...\n",
      "Current loss:  0.4607279598712921  | , previous best loss:  0.4612593948841095  | saving best model ...\n",
      "Current loss:  0.4601992666721344  | , previous best loss:  0.4607279598712921  | saving best model ...\n",
      "Current loss:  0.45967328548431396  | , previous best loss:  0.4601992666721344  | saving best model ...\n",
      "Current loss:  0.45914995670318604  | , previous best loss:  0.45967328548431396  | saving best model ...\n",
      "Current loss:  0.4586293399333954  | , previous best loss:  0.45914995670318604  | saving best model ...\n",
      "Current loss:  0.4581112563610077  | , previous best loss:  0.4586293399333954  | saving best model ...\n",
      "Current loss:  0.4575957953929901  | , previous best loss:  0.4581112563610077  | saving best model ...\n",
      "Current loss:  0.4570828676223755  | , previous best loss:  0.4575957953929901  | saving best model ...\n",
      "Current loss:  0.4565725028514862  | , previous best loss:  0.4570828676223755  | saving best model ...\n",
      "Current loss:  0.4560646712779999  | , previous best loss:  0.4565725028514862  | saving best model ...\n",
      "Current loss:  0.45555922389030457  | , previous best loss:  0.4560646712779999  | saving best model ...\n",
      "Current loss:  0.45505645871162415  | , previous best loss:  0.45555922389030457  | saving best model ...\n",
      "Current loss:  0.45455601811408997  | , previous best loss:  0.45505645871162415  | saving best model ...\n",
      "Current loss:  0.4540579915046692  | , previous best loss:  0.45455601811408997  | saving best model ...\n",
      "Current loss:  0.45356234908103943  | , previous best loss:  0.4540579915046692  | saving best model ...\n",
      "Current loss:  0.45306912064552307  | , previous best loss:  0.45356234908103943  | saving best model ...\n",
      "Current loss:  0.4525783061981201  | , previous best loss:  0.45306912064552307  | saving best model ...\n",
      "Current loss:  0.4520898163318634  | , previous best loss:  0.4525783061981201  | saving best model ...\n",
      "Current loss:  0.45160359144210815  | , previous best loss:  0.4520898163318634  | saving best model ...\n",
      "Current loss:  0.45111969113349915  | , previous best loss:  0.45160359144210815  | saving best model ...\n",
      "Current loss:  0.4506380558013916  | , previous best loss:  0.45111969113349915  | saving best model ...\n",
      "Current loss:  0.4501586854457855  | , previous best loss:  0.4506380558013916  | saving best model ...\n",
      "Current loss:  0.4496815502643585  | , previous best loss:  0.4501586854457855  | saving best model ...\n",
      "Current loss:  0.4492066204547882  | , previous best loss:  0.4496815502643585  | saving best model ...\n",
      "Current loss:  0.4487338662147522  | , previous best loss:  0.4492066204547882  | saving best model ...\n",
      "Current loss:  0.44826334714889526  | , previous best loss:  0.4487338662147522  | saving best model ...\n",
      "Current loss:  0.44779491424560547  | , previous best loss:  0.44826334714889526  | saving best model ...\n",
      "Current loss:  0.44732868671417236  | , previous best loss:  0.44779491424560547  | saving best model ...\n",
      "Current loss:  0.446864515542984  | , previous best loss:  0.44732868671417236  | saving best model ...\n",
      "Current loss:  0.4464024603366852  | , previous best loss:  0.446864515542984  | saving best model ...\n",
      "Current loss:  0.4459424912929535  | , previous best loss:  0.4464024603366852  | saving best model ...\n",
      "Current loss:  0.4454845190048218  | , previous best loss:  0.4459424912929535  | saving best model ...\n",
      "Current loss:  0.4450286626815796  | , previous best loss:  0.4454845190048218  | saving best model ...\n",
      "Current loss:  0.4445747435092926  | , previous best loss:  0.4450286626815796  | saving best model ...\n",
      "Current loss:  0.444122850894928  | , previous best loss:  0.4445747435092926  | saving best model ...\n",
      "Current loss:  0.44367289543151855  | , previous best loss:  0.444122850894928  | saving best model ...\n",
      "Current loss:  0.44322505593299866  | , previous best loss:  0.44367289543151855  | saving best model ...\n",
      "Current loss:  0.44277897477149963  | , previous best loss:  0.44322505593299866  | saving best model ...\n",
      "Current loss:  0.4423348903656006  | , previous best loss:  0.44277897477149963  | saving best model ...\n",
      "Current loss:  0.44189271330833435  | , previous best loss:  0.4423348903656006  | saving best model ...\n",
      "Current loss:  0.44145241379737854  | , previous best loss:  0.44189271330833435  | saving best model ...\n",
      "Current loss:  0.44101399183273315  | , previous best loss:  0.44145241379737854  | saving best model ...\n",
      "Current loss:  0.4405773878097534  | , previous best loss:  0.44101399183273315  | saving best model ...\n",
      "Current loss:  0.4401426613330841  | , previous best loss:  0.4405773878097534  | saving best model ...\n",
      "Current loss:  0.43970969319343567  | , previous best loss:  0.4401426613330841  | saving best model ...\n",
      "Current loss:  0.4392785131931305  | , previous best loss:  0.43970969319343567  | saving best model ...\n",
      "Current loss:  0.43884915113449097  | , previous best loss:  0.4392785131931305  | saving best model ...\n",
      "Current loss:  0.4384215474128723  | , previous best loss:  0.43884915113449097  | saving best model ...\n",
      "Current loss:  0.4379957318305969  | , previous best loss:  0.4384215474128723  | saving best model ...\n",
      "Current loss:  0.43757155537605286  | , previous best loss:  0.4379957318305969  | saving best model ...\n",
      "Current loss:  0.43714919686317444  | , previous best loss:  0.43757155537605286  | saving best model ...\n",
      "Current loss:  0.4367283880710602  | , previous best loss:  0.43714919686317444  | saving best model ...\n",
      "Current loss:  0.4363093674182892  | , previous best loss:  0.4367283880710602  | saving best model ...\n",
      "Current loss:  0.4358919858932495  | , previous best loss:  0.4363093674182892  | saving best model ...\n",
      "Current loss:  0.4354761838912964  | , previous best loss:  0.4358919858932495  | saving best model ...\n",
      "Current loss:  0.435062050819397  | , previous best loss:  0.4354761838912964  | saving best model ...\n",
      "Current loss:  0.43464964628219604  | , previous best loss:  0.435062050819397  | saving best model ...\n",
      "Current loss:  0.4342386722564697  | , previous best loss:  0.43464964628219604  | saving best model ...\n",
      "Current loss:  0.43382933735847473  | , previous best loss:  0.4342386722564697  | saving best model ...\n",
      "Current loss:  0.4334215521812439  | , previous best loss:  0.43382933735847473  | saving best model ...\n",
      "Current loss:  0.43301528692245483  | , previous best loss:  0.4334215521812439  | saving best model ...\n",
      "Current loss:  0.4326106011867523  | , previous best loss:  0.43301528692245483  | saving best model ...\n",
      "Current loss:  0.43220746517181396  | , previous best loss:  0.4326106011867523  | saving best model ...\n",
      "Current loss:  0.43180564045906067  | , previous best loss:  0.43220746517181396  | saving best model ...\n",
      "Current loss:  0.4314054846763611  | , previous best loss:  0.43180564045906067  | saving best model ...\n",
      "Current loss:  0.4310067892074585  | , previous best loss:  0.4314054846763611  | saving best model ...\n",
      "Current loss:  0.4306095242500305  | , previous best loss:  0.4310067892074585  | saving best model ...\n",
      "Current loss:  0.43021368980407715  | , previous best loss:  0.4306095242500305  | saving best model ...\n",
      "Current loss:  0.4298192858695984  | , previous best loss:  0.43021368980407715  | saving best model ...\n",
      "Current loss:  0.4294262230396271  | , previous best loss:  0.4298192858695984  | saving best model ...\n",
      "Current loss:  0.4290347397327423  | , previous best loss:  0.4294262230396271  | saving best model ...\n",
      "Current loss:  0.42864447832107544  | , previous best loss:  0.4290347397327423  | saving best model ...\n",
      "Current loss:  0.42825567722320557  | , previous best loss:  0.42864447832107544  | saving best model ...\n",
      "Current loss:  0.42786818742752075  | , previous best loss:  0.42825567722320557  | saving best model ...\n",
      "Current loss:  0.42748209834098816  | , previous best loss:  0.42786818742752075  | saving best model ...\n",
      "Current loss:  0.42709723114967346  | , previous best loss:  0.42748209834098816  | saving best model ...\n",
      "Current loss:  0.4267137944698334  | , previous best loss:  0.42709723114967346  | saving best model ...\n",
      "Current loss:  0.42633166909217834  | , previous best loss:  0.4267137944698334  | saving best model ...\n",
      "Current loss:  0.425950825214386  | , previous best loss:  0.42633166909217834  | saving best model ...\n",
      "Current loss:  0.42557117342948914  | , previous best loss:  0.425950825214386  | saving best model ...\n",
      "Current loss:  0.42519286274909973  | , previous best loss:  0.42557117342948914  | saving best model ...\n",
      "Current loss:  0.4248157739639282  | , previous best loss:  0.42519286274909973  | saving best model ...\n",
      "Current loss:  0.4244399666786194  | , previous best loss:  0.4248157739639282  | saving best model ...\n",
      "Current loss:  0.42406532168388367  | , previous best loss:  0.4244399666786194  | saving best model ...\n",
      "Current loss:  0.423691987991333  | , previous best loss:  0.42406532168388367  | saving best model ...\n",
      "Current loss:  0.42331981658935547  | , previous best loss:  0.423691987991333  | saving best model ...\n",
      "Current loss:  0.42294883728027344  | , previous best loss:  0.42331981658935547  | saving best model ...\n",
      "Current loss:  0.4225790500640869  | , previous best loss:  0.42294883728027344  | saving best model ...\n",
      "Current loss:  0.4222103953361511  | , previous best loss:  0.4225790500640869  | saving best model ...\n",
      "Current loss:  0.4218429923057556  | , previous best loss:  0.4222103953361511  | saving best model ...\n",
      "Current loss:  0.4214766025543213  | , previous best loss:  0.4218429923057556  | saving best model ...\n",
      "Current loss:  0.42111146450042725  | , previous best loss:  0.4214766025543213  | saving best model ...\n",
      "Current loss:  0.4207473695278168  | , previous best loss:  0.42111146450042725  | saving best model ...\n",
      "Current loss:  0.4203844368457794  | , previous best loss:  0.4207473695278168  | saving best model ...\n",
      "Current loss:  0.4200226068496704  | , previous best loss:  0.4203844368457794  | saving best model ...\n",
      "Current loss:  0.41966187953948975  | , previous best loss:  0.4200226068496704  | saving best model ...\n",
      "Current loss:  0.4193021357059479  | , previous best loss:  0.41966187953948975  | saving best model ...\n",
      "Current loss:  0.4189435541629791  | , previous best loss:  0.4193021357059479  | saving best model ...\n",
      "Current loss:  0.41858595609664917  | , previous best loss:  0.4189435541629791  | saving best model ...\n",
      "Current loss:  0.41822946071624756  | , previous best loss:  0.41858595609664917  | saving best model ...\n",
      "Current loss:  0.41787397861480713  | , previous best loss:  0.41822946071624756  | saving best model ...\n",
      "Current loss:  0.41751953959465027  | , previous best loss:  0.41787397861480713  | saving best model ...\n",
      "Current loss:  0.417166143655777  | , previous best loss:  0.41751953959465027  | saving best model ...\n",
      "Current loss:  0.4168137013912201  | , previous best loss:  0.417166143655777  | saving best model ...\n",
      "Current loss:  0.416462242603302  | , previous best loss:  0.4168137013912201  | saving best model ...\n",
      "Current loss:  0.4161118268966675  | , previous best loss:  0.416462242603302  | saving best model ...\n",
      "Current loss:  0.4157622456550598  | , previous best loss:  0.4161118268966675  | saving best model ...\n",
      "Current loss:  0.41541382670402527  | , previous best loss:  0.4157622456550598  | saving best model ...\n",
      "Current loss:  0.41506630182266235  | , previous best loss:  0.41541382670402527  | saving best model ...\n",
      "Current loss:  0.41471967101097107  | , previous best loss:  0.41506630182266235  | saving best model ...\n",
      "Current loss:  0.4143739342689514  | , previous best loss:  0.41471967101097107  | saving best model ...\n",
      "Current loss:  0.41402915120124817  | , previous best loss:  0.4143739342689514  | saving best model ...\n",
      "Current loss:  0.41368529200553894  | , previous best loss:  0.41402915120124817  | saving best model ...\n",
      "Current loss:  0.41334235668182373  | , previous best loss:  0.41368529200553894  | saving best model ...\n",
      "Current loss:  0.41300034523010254  | , previous best loss:  0.41334235668182373  | saving best model ...\n",
      "Current loss:  0.4126591980457306  | , previous best loss:  0.41300034523010254  | saving best model ...\n",
      "Current loss:  0.4123188257217407  | , previous best loss:  0.4126591980457306  | saving best model ...\n",
      "Current loss:  0.41197940707206726  | , previous best loss:  0.4123188257217407  | saving best model ...\n",
      "Current loss:  0.41164085268974304  | , previous best loss:  0.41197940707206726  | saving best model ...\n",
      "Current loss:  0.4113031029701233  | , previous best loss:  0.41164085268974304  | saving best model ...\n",
      "Current loss:  0.4109661877155304  | , previous best loss:  0.4113031029701233  | saving best model ...\n",
      "Current loss:  0.41063010692596436  | , previous best loss:  0.4109661877155304  | saving best model ...\n",
      "Current loss:  0.4102948307991028  | , previous best loss:  0.41063010692596436  | saving best model ...\n",
      "Current loss:  0.40996038913726807  | , previous best loss:  0.4102948307991028  | saving best model ...\n",
      "Current loss:  0.4096267819404602  | , previous best loss:  0.40996038913726807  | saving best model ...\n",
      "Current loss:  0.40929388999938965  | , previous best loss:  0.4096267819404602  | saving best model ...\n",
      "Current loss:  0.40896180272102356  | , previous best loss:  0.40929388999938965  | saving best model ...\n",
      "Current loss:  0.40863049030303955  | , previous best loss:  0.40896180272102356  | saving best model ...\n",
      "Current loss:  0.40829986333847046  | , previous best loss:  0.40863049030303955  | saving best model ...\n",
      "Current loss:  0.40797004103660583  | , previous best loss:  0.40829986333847046  | saving best model ...\n",
      "Current loss:  0.4076410233974457  | , previous best loss:  0.40797004103660583  | saving best model ...\n",
      "Current loss:  0.40731266140937805  | , previous best loss:  0.4076410233974457  | saving best model ...\n",
      "Current loss:  0.4069850444793701  | , previous best loss:  0.40731266140937805  | saving best model ...\n",
      "Current loss:  0.4066581726074219  | , previous best loss:  0.4069850444793701  | saving best model ...\n",
      "Current loss:  0.4063319265842438  | , previous best loss:  0.4066581726074219  | saving best model ...\n",
      "Current loss:  0.40600645542144775  | , previous best loss:  0.4063319265842438  | saving best model ...\n",
      "Current loss:  0.4056816101074219  | , previous best loss:  0.40600645542144775  | saving best model ...\n",
      "Current loss:  0.4053575098514557  | , previous best loss:  0.4056816101074219  | saving best model ...\n",
      "Current loss:  0.40503403544425964  | , previous best loss:  0.4053575098514557  | saving best model ...\n",
      "Current loss:  0.4047113060951233  | , previous best loss:  0.40503403544425964  | saving best model ...\n",
      "Current loss:  0.40438908338546753  | , previous best loss:  0.4047113060951233  | saving best model ...\n",
      "Current loss:  0.40406760573387146  | , previous best loss:  0.40438908338546753  | saving best model ...\n",
      "Current loss:  0.4037468731403351  | , previous best loss:  0.40406760573387146  | saving best model ...\n",
      "Current loss:  0.4034265875816345  | , previous best loss:  0.4037468731403351  | saving best model ...\n",
      "Current loss:  0.4031069576740265  | , previous best loss:  0.4034265875816345  | saving best model ...\n",
      "Current loss:  0.4027880132198334  | , previous best loss:  0.4031069576740265  | saving best model ...\n",
      "Current loss:  0.402469664812088  | , previous best loss:  0.4027880132198334  | saving best model ...\n",
      "Current loss:  0.4021519124507904  | , previous best loss:  0.402469664812088  | saving best model ...\n",
      "Current loss:  0.4018346667289734  | , previous best loss:  0.4021519124507904  | saving best model ...\n",
      "Current loss:  0.4015181362628937  | , previous best loss:  0.4018346667289734  | saving best model ...\n",
      "Current loss:  0.40120208263397217  | , previous best loss:  0.4015181362628937  | saving best model ...\n",
      "Current loss:  0.4008866548538208  | , previous best loss:  0.40120208263397217  | saving best model ...\n",
      "Current loss:  0.40057173371315  | , previous best loss:  0.4008866548538208  | saving best model ...\n",
      "Current loss:  0.400257408618927  | , previous best loss:  0.40057173371315  | saving best model ...\n",
      "Current loss:  0.39994364976882935  | , previous best loss:  0.400257408618927  | saving best model ...\n",
      "Current loss:  0.39963042736053467  | , previous best loss:  0.39994364976882935  | saving best model ...\n",
      "Current loss:  0.3993177115917206  | , previous best loss:  0.39963042736053467  | saving best model ...\n",
      "Current loss:  0.399005651473999  | , previous best loss:  0.3993177115917206  | saving best model ...\n",
      "Current loss:  0.39869388937950134  | , previous best loss:  0.399005651473999  | saving best model ...\n",
      "Current loss:  0.3983827531337738  | , previous best loss:  0.39869388937950134  | saving best model ...\n",
      "Current loss:  0.39807215332984924  | , previous best loss:  0.3983827531337738  | saving best model ...\n",
      "Current loss:  0.39776211977005005  | , previous best loss:  0.39807215332984924  | saving best model ...\n",
      "Current loss:  0.39745238423347473  | , previous best loss:  0.39776211977005005  | saving best model ...\n",
      "Current loss:  0.39714333415031433  | , previous best loss:  0.39745238423347473  | saving best model ...\n",
      "Current loss:  0.3968346416950226  | , previous best loss:  0.39714333415031433  | saving best model ...\n",
      "Current loss:  0.396526575088501  | , previous best loss:  0.3968346416950226  | saving best model ...\n",
      "Current loss:  0.39621883630752563  | , previous best loss:  0.396526575088501  | saving best model ...\n",
      "Current loss:  0.39591166377067566  | , previous best loss:  0.39621883630752563  | saving best model ...\n",
      "Current loss:  0.3956049382686615  | , previous best loss:  0.39591166377067566  | saving best model ...\n",
      "Current loss:  0.39529865980148315  | , previous best loss:  0.3956049382686615  | saving best model ...\n",
      "Current loss:  0.39499273896217346  | , previous best loss:  0.39529865980148315  | saving best model ...\n",
      "Current loss:  0.3946875035762787  | , previous best loss:  0.39499273896217346  | saving best model ...\n",
      "Current loss:  0.3943824768066406  | , previous best loss:  0.3946875035762787  | saving best model ...\n",
      "Current loss:  0.3940780758857727  | , previous best loss:  0.3943824768066406  | saving best model ...\n",
      "Current loss:  0.3937740623950958  | , previous best loss:  0.3940780758857727  | saving best model ...\n",
      "Current loss:  0.3934703469276428  | , previous best loss:  0.3937740623950958  | saving best model ...\n",
      "Current loss:  0.3931671977043152  | , previous best loss:  0.3934703469276428  | saving best model ...\n",
      "Current loss:  0.3928644061088562  | , previous best loss:  0.3931671977043152  | saving best model ...\n",
      "Current loss:  0.3925620913505554  | , previous best loss:  0.3928644061088562  | saving best model ...\n",
      "Current loss:  0.39226019382476807  | , previous best loss:  0.3925620913505554  | saving best model ...\n",
      "Current loss:  0.39195868372917175  | , previous best loss:  0.39226019382476807  | saving best model ...\n",
      "Current loss:  0.39165765047073364  | , previous best loss:  0.39195868372917175  | saving best model ...\n",
      "Current loss:  0.39135703444480896  | , previous best loss:  0.39165765047073364  | saving best model ...\n",
      "Current loss:  0.3910568356513977  | , previous best loss:  0.39135703444480896  | saving best model ...\n",
      "Current loss:  0.39075687527656555  | , previous best loss:  0.3910568356513977  | saving best model ...\n",
      "Current loss:  0.39045751094818115  | , previous best loss:  0.39075687527656555  | saving best model ...\n",
      "Current loss:  0.390158474445343  | , previous best loss:  0.39045751094818115  | saving best model ...\n",
      "Current loss:  0.38985979557037354  | , previous best loss:  0.390158474445343  | saving best model ...\n",
      "Current loss:  0.38956156373023987  | , previous best loss:  0.38985979557037354  | saving best model ...\n",
      "Current loss:  0.38926371932029724  | , previous best loss:  0.38956156373023987  | saving best model ...\n",
      "Current loss:  0.38896632194519043  | , previous best loss:  0.38926371932029724  | saving best model ...\n",
      "Current loss:  0.38866928219795227  | , previous best loss:  0.38896632194519043  | saving best model ...\n",
      "Current loss:  0.38837260007858276  | , previous best loss:  0.38866928219795227  | saving best model ...\n",
      "Current loss:  0.3880763351917267  | , previous best loss:  0.38837260007858276  | saving best model ...\n",
      "Current loss:  0.38778042793273926  | , previous best loss:  0.3880763351917267  | saving best model ...\n",
      "Current loss:  0.38748493790626526  | , previous best loss:  0.38778042793273926  | saving best model ...\n",
      "Current loss:  0.3871898353099823  | , previous best loss:  0.38748493790626526  | saving best model ...\n",
      "Current loss:  0.38689514994621277  | , previous best loss:  0.3871898353099823  | saving best model ...\n",
      "Current loss:  0.3866007924079895  | , previous best loss:  0.38689514994621277  | saving best model ...\n",
      "Current loss:  0.38630691170692444  | , previous best loss:  0.3866007924079895  | saving best model ...\n",
      "Current loss:  0.386013388633728  | , previous best loss:  0.38630691170692444  | saving best model ...\n",
      "Current loss:  0.38572022318840027  | , previous best loss:  0.386013388633728  | saving best model ...\n",
      "Current loss:  0.3854273855686188  | , previous best loss:  0.38572022318840027  | saving best model ...\n",
      "Current loss:  0.38513505458831787  | , previous best loss:  0.3854273855686188  | saving best model ...\n",
      "Current loss:  0.38484305143356323  | , previous best loss:  0.38513505458831787  | saving best model ...\n",
      "Current loss:  0.38455143570899963  | , previous best loss:  0.38484305143356323  | saving best model ...\n",
      "Current loss:  0.38426026701927185  | , previous best loss:  0.38455143570899963  | saving best model ...\n",
      "Current loss:  0.3839694857597351  | , previous best loss:  0.38426026701927185  | saving best model ...\n",
      "Current loss:  0.38367903232574463  | , previous best loss:  0.3839694857597351  | saving best model ...\n",
      "Current loss:  0.38338905572891235  | , previous best loss:  0.38367903232574463  | saving best model ...\n",
      "Current loss:  0.38309943675994873  | , previous best loss:  0.38338905572891235  | saving best model ...\n",
      "Current loss:  0.38281020522117615  | , previous best loss:  0.38309943675994873  | saving best model ...\n",
      "Current loss:  0.38252145051956177  | , previous best loss:  0.38281020522117615  | saving best model ...\n",
      "Current loss:  0.38223302364349365  | , previous best loss:  0.38252145051956177  | saving best model ...\n",
      "Current loss:  0.38194501399993896  | , previous best loss:  0.38223302364349365  | saving best model ...\n",
      "Current loss:  0.3816574215888977  | , previous best loss:  0.38194501399993896  | saving best model ...\n",
      "Current loss:  0.3813702464103699  | , previous best loss:  0.3816574215888977  | saving best model ...\n",
      "Current loss:  0.3810834586620331  | , previous best loss:  0.3813702464103699  | saving best model ...\n",
      "Current loss:  0.38079702854156494  | , previous best loss:  0.3810834586620331  | saving best model ...\n",
      "Current loss:  0.38051119446754456  | , previous best loss:  0.38079702854156494  | saving best model ...\n",
      "Current loss:  0.38022562861442566  | , previous best loss:  0.38051119446754456  | saving best model ...\n",
      "Current loss:  0.37994059920310974  | , previous best loss:  0.38022562861442566  | saving best model ...\n",
      "Current loss:  0.3796559274196625  | , previous best loss:  0.37994059920310974  | saving best model ...\n",
      "Current loss:  0.37937167286872864  | , previous best loss:  0.3796559274196625  | saving best model ...\n",
      "Current loss:  0.379087895154953  | , previous best loss:  0.37937167286872864  | saving best model ...\n",
      "Current loss:  0.3788045644760132  | , previous best loss:  0.379087895154953  | saving best model ...\n",
      "Current loss:  0.3785216808319092  | , previous best loss:  0.3788045644760132  | saving best model ...\n",
      "Current loss:  0.37823912501335144  | , previous best loss:  0.3785216808319092  | saving best model ...\n",
      "Current loss:  0.37795719504356384  | , previous best loss:  0.37823912501335144  | saving best model ...\n",
      "Current loss:  0.3776756227016449  | , previous best loss:  0.37795719504356384  | saving best model ...\n",
      "Current loss:  0.37739455699920654  | , previous best loss:  0.3776756227016449  | saving best model ...\n",
      "Current loss:  0.3771139085292816  | , previous best loss:  0.37739455699920654  | saving best model ...\n",
      "Current loss:  0.3768336772918701  | , previous best loss:  0.3771139085292816  | saving best model ...\n",
      "Current loss:  0.37655410170555115  | , previous best loss:  0.3768336772918701  | saving best model ...\n",
      "Current loss:  0.37627479434013367  | , previous best loss:  0.37655410170555115  | saving best model ...\n",
      "Current loss:  0.37599608302116394  | , previous best loss:  0.37627479434013367  | saving best model ...\n",
      "Current loss:  0.3757178485393524  | , previous best loss:  0.37599608302116394  | saving best model ...\n",
      "Current loss:  0.3754400908946991  | , previous best loss:  0.3757178485393524  | saving best model ...\n",
      "Current loss:  0.3751627206802368  | , previous best loss:  0.3754400908946991  | saving best model ...\n",
      "Current loss:  0.37488600611686707  | , previous best loss:  0.3751627206802368  | saving best model ...\n",
      "Current loss:  0.3746097683906555  | , previous best loss:  0.37488600611686707  | saving best model ...\n",
      "Current loss:  0.3743339478969574  | , previous best loss:  0.3746097683906555  | saving best model ...\n",
      "Current loss:  0.37405872344970703  | , previous best loss:  0.3743339478969574  | saving best model ...\n",
      "Current loss:  0.3737838864326477  | , previous best loss:  0.37405872344970703  | saving best model ...\n",
      "Current loss:  0.3735097646713257  | , previous best loss:  0.3737838864326477  | saving best model ...\n",
      "Current loss:  0.3732360601425171  | , previous best loss:  0.3735097646713257  | saving best model ...\n",
      "Current loss:  0.3729628324508667  | , previous best loss:  0.3732360601425171  | saving best model ...\n",
      "Current loss:  0.37269023060798645  | , previous best loss:  0.3729628324508667  | saving best model ...\n",
      "Current loss:  0.3724181354045868  | , previous best loss:  0.37269023060798645  | saving best model ...\n",
      "Current loss:  0.3721465766429901  | , previous best loss:  0.3724181354045868  | saving best model ...\n",
      "Current loss:  0.3718755841255188  | , previous best loss:  0.3721465766429901  | saving best model ...\n",
      "Current loss:  0.3716050982475281  | , previous best loss:  0.3718755841255188  | saving best model ...\n",
      "Current loss:  0.3713351786136627  | , previous best loss:  0.3716050982475281  | saving best model ...\n",
      "Current loss:  0.3710659146308899  | , previous best loss:  0.3713351786136627  | saving best model ...\n",
      "Current loss:  0.37079715728759766  | , previous best loss:  0.3710659146308899  | saving best model ...\n",
      "Current loss:  0.3705289959907532  | , previous best loss:  0.37079715728759766  | saving best model ...\n",
      "Current loss:  0.37026146054267883  | , previous best loss:  0.3705289959907532  | saving best model ...\n",
      "Current loss:  0.3699944019317627  | , previous best loss:  0.37026146054267883  | saving best model ...\n",
      "Current loss:  0.3697279989719391  | , previous best loss:  0.3699944019317627  | saving best model ...\n",
      "Current loss:  0.36946216225624084  | , previous best loss:  0.3697279989719391  | saving best model ...\n",
      "Current loss:  0.36919692158699036  | , previous best loss:  0.36946216225624084  | saving best model ...\n",
      "Current loss:  0.3689322769641876  | , previous best loss:  0.36919692158699036  | saving best model ...\n",
      "Current loss:  0.36866825819015503  | , previous best loss:  0.3689322769641876  | saving best model ...\n",
      "Current loss:  0.3684048354625702  | , previous best loss:  0.36866825819015503  | saving best model ...\n",
      "Current loss:  0.3681420385837555  | , previous best loss:  0.3684048354625702  | saving best model ...\n",
      "Current loss:  0.36787986755371094  | , previous best loss:  0.3681420385837555  | saving best model ...\n",
      "Current loss:  0.3676183819770813  | , previous best loss:  0.36787986755371094  | saving best model ...\n",
      "Current loss:  0.3673574924468994  | , previous best loss:  0.3676183819770813  | saving best model ...\n",
      "Current loss:  0.36709707975387573  | , previous best loss:  0.3673574924468994  | saving best model ...\n",
      "Current loss:  0.3668374717235565  | , previous best loss:  0.36709707975387573  | saving best model ...\n",
      "Current loss:  0.36657848954200745  | , previous best loss:  0.3668374717235565  | saving best model ...\n",
      "Current loss:  0.36632010340690613  | , previous best loss:  0.36657848954200745  | saving best model ...\n",
      "Current loss:  0.3660624027252197  | , previous best loss:  0.36632010340690613  | saving best model ...\n",
      "Current loss:  0.36580535769462585  | , previous best loss:  0.3660624027252197  | saving best model ...\n",
      "Current loss:  0.3655489385128021  | , previous best loss:  0.36580535769462585  | saving best model ...\n",
      "Current loss:  0.3652932345867157  | , previous best loss:  0.3655489385128021  | saving best model ...\n",
      "Current loss:  0.36503806710243225  | , previous best loss:  0.3652932345867157  | saving best model ...\n",
      "Current loss:  0.36478370428085327  | , previous best loss:  0.36503806710243225  | saving best model ...\n",
      "Current loss:  0.3645300269126892  | , previous best loss:  0.36478370428085327  | saving best model ...\n",
      "Current loss:  0.3642769753932953  | , previous best loss:  0.3645300269126892  | saving best model ...\n",
      "Current loss:  0.3640245497226715  | , previous best loss:  0.3642769753932953  | saving best model ...\n",
      "Current loss:  0.3637728989124298  | , previous best loss:  0.3640245497226715  | saving best model ...\n",
      "Current loss:  0.36352187395095825  | , previous best loss:  0.3637728989124298  | saving best model ...\n",
      "Current loss:  0.363271564245224  | , previous best loss:  0.36352187395095825  | saving best model ...\n",
      "Current loss:  0.36302193999290466  | , previous best loss:  0.363271564245224  | saving best model ...\n",
      "Current loss:  0.36277297139167786  | , previous best loss:  0.36302193999290466  | saving best model ...\n",
      "Current loss:  0.36252474784851074  | , previous best loss:  0.36277297139167786  | saving best model ...\n",
      "Current loss:  0.36227717995643616  | , previous best loss:  0.36252474784851074  | saving best model ...\n",
      "Current loss:  0.36203041672706604  | , previous best loss:  0.36227717995643616  | saving best model ...\n",
      "Current loss:  0.3617842495441437  | , previous best loss:  0.36203041672706604  | saving best model ...\n",
      "Current loss:  0.3615387976169586  | , previous best loss:  0.3617842495441437  | saving best model ...\n",
      "Current loss:  0.3612940311431885  | , previous best loss:  0.3615387976169586  | saving best model ...\n",
      "Current loss:  0.3610500693321228  | , previous best loss:  0.3612940311431885  | saving best model ...\n",
      "Current loss:  0.36080673336982727  | , previous best loss:  0.3610500693321228  | saving best model ...\n",
      "Current loss:  0.3605642020702362  | , previous best loss:  0.36080673336982727  | saving best model ...\n",
      "Current loss:  0.3603222668170929  | , previous best loss:  0.3605642020702362  | saving best model ...\n",
      "Current loss:  0.36008113622665405  | , previous best loss:  0.3603222668170929  | saving best model ...\n",
      "Current loss:  0.3598406910896301  | , previous best loss:  0.36008113622665405  | saving best model ...\n",
      "Current loss:  0.3596009910106659  | , previous best loss:  0.3598406910896301  | saving best model ...\n",
      "Current loss:  0.3593619465827942  | , previous best loss:  0.3596009910106659  | saving best model ...\n",
      "Current loss:  0.35912367701530457  | , previous best loss:  0.3593619465827942  | saving best model ...\n",
      "Current loss:  0.35888612270355225  | , previous best loss:  0.35912367701530457  | saving best model ...\n",
      "Current loss:  0.35864925384521484  | , previous best loss:  0.35888612270355225  | saving best model ...\n",
      "Current loss:  0.3584131598472595  | , previous best loss:  0.35864925384521484  | saving best model ...\n",
      "Current loss:  0.35817772150039673  | , previous best loss:  0.3584131598472595  | saving best model ...\n",
      "Current loss:  0.3579431474208832  | , previous best loss:  0.35817772150039673  | saving best model ...\n",
      "Current loss:  0.35770922899246216  | , previous best loss:  0.3579431474208832  | saving best model ...\n",
      "Current loss:  0.3574759364128113  | , previous best loss:  0.35770922899246216  | saving best model ...\n",
      "Current loss:  0.35724350810050964  | , previous best loss:  0.3574759364128113  | saving best model ...\n",
      "Current loss:  0.3570116460323334  | , previous best loss:  0.35724350810050964  | saving best model ...\n",
      "Current loss:  0.35678064823150635  | , previous best loss:  0.3570116460323334  | saving best model ...\n",
      "Current loss:  0.35655033588409424  | , previous best loss:  0.35678064823150635  | saving best model ...\n",
      "Current loss:  0.3563207983970642  | , previous best loss:  0.35655033588409424  | saving best model ...\n",
      "Current loss:  0.35609206557273865  | , previous best loss:  0.3563207983970642  | saving best model ...\n",
      "Current loss:  0.3558642268180847  | , previous best loss:  0.35609206557273865  | saving best model ...\n",
      "Current loss:  0.3556373715400696  | , previous best loss:  0.3558642268180847  | saving best model ...\n",
      "Current loss:  0.3554123044013977  | , previous best loss:  0.3556373715400696  | saving best model ...\n",
      "Current loss:  0.3551904559135437  | , previous best loss:  0.3554123044013977  | saving best model ...\n",
      "Current loss:  0.35497599840164185  | , previous best loss:  0.3551904559135437  | saving best model ...\n",
      "Current loss:  0.35478031635284424  | , previous best loss:  0.35497599840164185  | saving best model ...\n",
      "Current loss:  0.35463568568229675  | , previous best loss:  0.35478031635284424  | saving best model ...\n",
      "Current loss:  0.3537031412124634  | , previous best loss:  0.35463568568229675  | saving best model ...\n",
      "Convergence!\n"
     ]
    }
   ],
   "source": [
    "for t in range(Iteration):\n",
    "    # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "    pyb_af = DeepBS(X_train)\n",
    "    loss = criterion(y_train, pyb_af); bloss_list.append(loss.item())\n",
    "    \n",
    "    if (t > 0) and ((bloss_list[t-1]-bloss_list[t])<tor):        \n",
    "        if (tpat != 0) and (tpat % patientr) == 0:\n",
    "            learning_r *= 0.2 \n",
    "            tpat += 1\n",
    "            #print('Learning rate reduce to ', learning_r)\n",
    "            optimizer = torch.optim.Adam(DeepBS.parameters(), lr=learning_r)\n",
    "            if learning_r <= lr_tor:\n",
    "                if t < patientc + 1:\n",
    "                    conv = False\n",
    "                else:\n",
    "                    conv = True\n",
    "                print('Convergence!')\n",
    "                break\n",
    "        elif tpat < patientc:\n",
    "            tpat += 1\n",
    "            pass\n",
    "        else:\n",
    "            if t < patientc + 1:\n",
    "                conv = False\n",
    "            else:\n",
    "                conv = True\n",
    "            print('Convergence!')\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        if loss < bloss:\n",
    "            print('Current loss: ', loss.item(), ' | , previous best loss: ', bloss, ' | saving best model ...')\n",
    "            torch.save(DeepBS.state_dict(), './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1))\n",
    "            bloss = loss.item()\n",
    "            tpat = 0\n",
    "        else:\n",
    "            tpat += 1\n",
    "    \n",
    "    if tpat == patientc:\n",
    "        if t < patientc + 1:\n",
    "            conv = False\n",
    "        else:\n",
    "            conv = True\n",
    "            print('Convergence!')\n",
    "        break\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c8763-5ba2-483c-bdf1-dafef5db12a0",
   "metadata": {},
   "source": [
    "## ECM-Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bd5ec9-cd18-45fb-aa4a-853660da01be",
   "metadata": {},
   "source": [
    "### Via ECM to implement layer-wise optimization for tuning the weight for B-Spline Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "01894a77-0720-416e-9745-9c47e283d904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeepPS = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = 2, output_dim = Fout, bias = True).to(device)\n",
    "DeepPS.load_state_dict(torch.load( './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1), weights_only = True))\n",
    "\n",
    "#DPSy = DeepPS(X_train)\n",
    "#ECM_para = DeepPS.get_para_ecm(X_train)\n",
    "#ECM_Lambda = ECM(ECM_para, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "dd6b8e90-9f65-4406-883c-66838525bf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x356f16ed0>"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(DeepPS(X_train).detach().numpy(), y_train.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "id": "6929e5c8-0f2e-4ad0-866b-1cc875639a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ECM(par, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4):\n",
    "    lambdab = initial_lambda\n",
    "    sigma = initial_sigma\n",
    "    xi = initial_xi\n",
    "    \n",
    "    n_block, num_knots, num_neurons = par['wbasic'].size()\n",
    "    ls_lambda = torch.empty(n_block)\n",
    "    \n",
    "    for l in range(n_block):\n",
    "        B = par['ebasic'][l]\n",
    "        By = par['basic'][l]\n",
    "        WB = par['wbasic'][l]\n",
    "        \n",
    "        DB = diag_mat_weights(WB.size()[0]).to(device)\n",
    "        size = B.size()[1]\n",
    "        S = DB.T @ DB\n",
    "        Cov_a = (xi**2)* torch.linalg.pinv(S)\n",
    "        Cov_e = torch.eye(size*num_neurons)* sigma\n",
    "        \n",
    "        block_y = torch.reshape(By, (-1,1))\n",
    "        flatB = B.view(num_neurons, num_knots, size)\n",
    "            \n",
    "        sqr_xi= 0\n",
    "        sqr_sig = 0\n",
    "        \n",
    "        for i in range(num_neurons):\n",
    "            Ncov = (Cov_a -(Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)]) @ flatB[i].T @ Cov_a))\n",
    "            Nmu = (Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)])) @ By[:,i].reshape(-1,1)\n",
    "            \n",
    "            first_xi = S @ Ncov\n",
    "            second_xi = (Nmu.T @ S @ Nmu)\n",
    "            sqr_xi += torch.trace(first_xi) + second_xi\n",
    "                \n",
    "            first_sig = torch.norm(By[:,i])\n",
    "            second_sig = 2 * (By[:,i] @ flatB[i].T) @ Nmu \n",
    "            third_sig = torch.trace((flatB[i] @ flatB[i].T) @ Ncov)\n",
    "            four_sig = (Nmu.T @ flatB[i] @ flatB[i].T @ Nmu)\n",
    "            \n",
    "            sqr_sig += (first_sig + second_sig + third_sig + four_sig)\n",
    "\n",
    "        sqr_xi /= num_neurons\n",
    "        sqr_sig /= (num_neurons*size)\n",
    "\n",
    "        print(sqr_xi, sqr_sig)\n",
    "        ls_lambda[l] = (sqr_sig/sqr_xi).item()\n",
    "    \n",
    "    return ls_lambda\n",
    "    \n",
    "def ECM_layersise_update(model, par, Lambda, x, y):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        DSy = model(x)\n",
    "        print('Training Error: ', np.round(criterion(y, DSy.detach()).item(), 5))\n",
    "              \n",
    "    B_out, B_in, B_w, B_b = par['basic'], par['ebasic'], par['wbasic'], par['bbasic']\n",
    "    n_layer, nk, nm = B_w.size()\n",
    "    DB = diag_mat_weights(B_w[0].size()[0], 'second').to(device)\n",
    "\n",
    "    Project_matrix = (torch.linalg.pinv(B_in[-1].T @ B_in[-1]) @ B_in[-1].T @ B_in[-1])\n",
    "    Size = [b.size()[1] for b in B_in]\n",
    "\n",
    "    B_in = B_in.view(n_layer, nm, nk, Size[0])\n",
    "    \n",
    "    for l in range(n_layer):    \n",
    "        NW = torch.empty((nk, nm))\n",
    "        NB = torch.empty((nm))\n",
    "        \n",
    "        for i in range(nm):\n",
    "            B1y = B_out[l][:,i] - B_b[l][i]\n",
    "            BB = B_in[l][i].T\n",
    "    \n",
    "            # Update the weights and bias\n",
    "            NW[:, i] = (torch.inverse(BB.T @ BB + (Lambda[l]/Size[l]) * (DB.T @ DB)) @ BB.T @ B1y)\n",
    "            NB[i] = torch.mean(B_out[l][:,i] - (NW[:,i] @ BB.T))\n",
    "                \n",
    "        # update the weight\n",
    "        block = getattr(model.Spline_block.model, f'block_{l}')\n",
    "        getattr(block.block.BSL, 'control_p').data = NW\n",
    "        getattr(block.block.BSL, 'bias').data = NB\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        DPSy = model(x)\n",
    "        Update_Train_Loss = np.round(criterion(y, DPSy.detach()).item(), 5)\n",
    "        GCV = np.round((torch.norm(y - DPSy)/(Size[-1]-torch.trace(Project_matrix))).item(), 5)\n",
    "    \n",
    "    return model, GCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "id": "6e263838-753b-498b-a6a2-4a9fa22d74d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24466666666666667"
      ]
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Initialization\n",
    "model = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = 2, output_dim = Fout, bias = True).to(device)\n",
    "model.load_state_dict(torch.load( './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1), weights_only = True))\n",
    "\n",
    "BestGCV = 9999\n",
    "for i in range(10):\n",
    "    _ = model(X_train)\n",
    "    ECM_para = model.get_para_ecm(X_train)\n",
    "    ECM_Lambda = ECM(ECM_para, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4)\n",
    "\n",
    "    print('Lambda: ', ECM_Lambda)\n",
    "    model, GCV = ECM_layersise_update(model, ECM_para, ECM_Lambda, X_train, y_train)\n",
    "\n",
    "    if GCV < BestGCV:\n",
    "        BestLambda = ECM_Lambda\n",
    "        BestGCV = GCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d780a8-ff73-4afe-9ec0-2720f81bc8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7500cd15-e680-4062-9469-0f142ed368a1",
   "metadata": {},
   "source": [
    "### Fast-Tuning for optimimal DPS parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb3d17b-eb10-47ac-983d-2f655fad7d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cbf8f85-c7d8-499b-9452-dd8f7aafe322",
   "metadata": {},
   "source": [
    "## Evaluation on DS and DPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef2fa0-ed52-4f74-aa33-c8eee06dd6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    eval_model = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = 1, output_dim = Fout, bias = True).to(device)\n",
    "    eval_model.load_state_dict(torch.load( './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1), weights_only = True))\n",
    "    DPSy = eval_model(X_train)\n",
    "    LambdaB = ECM(model = eval_model, num_neurons = nm, num_knots = nk)\n",
    "    Lambdalist[str(d+1)] = LambdaB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f32c54-d159-4dae-bbdf-e9df385dbd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d6cab1-b9de-41fd-8060-cd9eaa75e1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351bc8d1-8b22-4038-abe4-3d53c121f7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74568953-4b8f-4ebf-b210-903e59df02ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTORCH",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
