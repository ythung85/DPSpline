{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "16d1745a-f822-44f1-af6d-7cf0b4f382ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sim_data(n, dim, Type):\n",
    "\tif Type == 'A':\n",
    "\t\tX = torch.rand((n,dim))\n",
    "\t\ty = torch.exp(2*torch.sin(X[:,0]*0.5*torch.pi)+ 0.5*torch.cos(X[:,1]*2.5*torch.pi))\n",
    "\t\ty = y.reshape(-1,1)\n",
    "\t\ty = y.float()\n",
    "\t\t\n",
    "\telif Type == 'B':\n",
    "\t\tX = torch.rand((n, dim))\n",
    "\t\ty = 1\n",
    "\t\tfor d in range(dim):\n",
    "\t\t\ta = (d+1)/2\n",
    "\t\t\ty *= ((torch.abs(4*X[:,d]-2)+a)/(1+a))\n",
    "\t\ty = y.reshape(-1,1)\n",
    "\t\ty = y.float()\n",
    "\telse:\n",
    "\t\tpass\n",
    "\t\n",
    "\treturn X, y\n",
    "\t\n",
    "def norm(x):\n",
    "\treturn (x-torch.min(x))/(torch.max(x)-torch.min(x))\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def diag_mat_weights(dimp, type = 'first'):\n",
    "\tif type == 'first':\n",
    "\t\tdg = np.zeros((dimp-1, dimp))\n",
    "\t\tfor i in range(dimp-1):\n",
    "\t\t\tdg[i,i] = -1\n",
    "\t\t\tdg[i,i+1]= 1\n",
    "\telif type == 'second':\n",
    "\t\tdg = np.zeros((dimp-2, dimp))\n",
    "\t\tfor i in range(dimp-2):\n",
    "\t\t\tdg[i,i] = -1\n",
    "\t\t\tdg[i,i+1]= 2\n",
    "\t\t\tdg[i,i+2]= -1\n",
    "\telse:\n",
    "\t\tpass\n",
    "\treturn torch.Tensor(dg)\n",
    "\n",
    "def num_para(model):\n",
    "\ttp = 0\n",
    "\tfor param in model.parameters():\n",
    "\t\ttp += param.numel()\n",
    "\treturn tp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba56b5bf-a574-4b6b-94e4-5f69d6d294f7",
   "metadata": {},
   "source": [
    "## ECM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "cac109e3-c01b-4a96-a6dc-7dc2427c54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ECM(par, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4):\n",
    "    lambdab = initial_lambda\n",
    "    sigma = initial_sigma\n",
    "    xi = initial_xi\n",
    "    \n",
    "    n_block, num_knots, num_neurons = par['wbasic'].size()\n",
    "    ls_lambda = torch.empty(n_block)\n",
    "    \n",
    "    for l in range(n_block):\n",
    "        B = par['ebasic'][l]\n",
    "        By = par['basic'][l]\n",
    "        WB = par['wbasic'][l]\n",
    "        \n",
    "        DB = diag_mat_weights(WB.size()[0]).to(device)\n",
    "        size = B.size()[1]\n",
    "        S = DB.T @ DB\n",
    "        Cov_a = (xi**2)* torch.linalg.pinv(S)\n",
    "        Cov_e = torch.eye(size*num_neurons)* sigma\n",
    "        \n",
    "        block_y = torch.reshape(By, (-1,1))\n",
    "        flatB = B.view(num_neurons, num_knots, size)\n",
    "            \n",
    "        sqr_xi= 0\n",
    "        sqr_sig = 0\n",
    "        \n",
    "        for i in range(num_neurons):\n",
    "            Ncov = (Cov_a -(Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)]) @ flatB[i].T @ Cov_a))\n",
    "            Nmu = (Cov_a @ flatB[i]) @ (torch.linalg.pinv(flatB[i].T @ Cov_a @ flatB[i] + Cov_e[size*i:size*(i+1),size*i:size*(i+1)])) @ By[:,i].reshape(-1,1)\n",
    "            \n",
    "            first_xi = S @ Ncov\n",
    "            second_xi = (Nmu.T @ S @ Nmu)\n",
    "            sqr_xi += torch.trace(first_xi) + second_xi\n",
    "                \n",
    "            first_sig = torch.norm(By[:,i])\n",
    "            second_sig = 2 * (By[:,i] @ flatB[i].T) @ Nmu \n",
    "            third_sig = torch.trace((flatB[i] @ flatB[i].T) @ Ncov)\n",
    "            four_sig = (Nmu.T @ flatB[i] @ flatB[i].T @ Nmu)\n",
    "            \n",
    "            sqr_sig += (first_sig + second_sig + third_sig + four_sig)\n",
    "        \n",
    "        sqr_xi /= num_neurons\n",
    "        sqr_sig /= (num_neurons*size)\n",
    "        \n",
    "        ls_lambda[l] = (sqr_sig/sqr_xi).item()\n",
    "    \n",
    "    return ls_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb6ac5-a0e0-460c-8819-abd99bf34073",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2d5b9ef9-aced-4426-b106-08585f6cce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSL(nn.Module):\n",
    "    def __init__(self, degree, num_knots, num_neurons, bias = True):\n",
    "        super(BSL, self).__init__()\n",
    "        self.degree = degree\n",
    "        self.num_knots = num_knots\n",
    "        self.num_neurons = num_neurons\n",
    "        self.control_p = nn.Parameter(torch.randn(self.num_knots, self.num_neurons))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(self.num_neurons))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.inter = {}\n",
    "    \n",
    "    def basis_function(self, x, i, k, t):\n",
    "    \n",
    "        # Base case: degree 0 spline\n",
    "        if k == 0:\n",
    "            return ((t[i] <= x) & (x < t[i + 1])).float()\n",
    "    \n",
    "        # Recursive case\n",
    "        denom1 = t[i + k] - t[i]\n",
    "        denom2 = t[i + k + 1] - t[i + 1]\n",
    "    \n",
    "        term1 = 0\n",
    "        if denom1 != 0:\n",
    "            term1 = (x - t[i]) / denom1 * self.basis_function(x, i, k - 1, t)\n",
    "    \n",
    "        term2 = 0\n",
    "        if denom2 != 0:\n",
    "            term2 = (t[i + k + 1] - x) / denom2 * self.basis_function(x, i + 1, k - 1, t)\n",
    "    \n",
    "        return term1 + term2\n",
    "\n",
    "    def knots_distribution(self, dg, nk):\n",
    "\n",
    "        knots = torch.cat([torch.linspace(-0.002, -0.001, steps=dg),            # Add repeated values at the start for clamping\n",
    "            torch.linspace(0, 1, nk-2*dg-2),  # Uniform knot spacing in the middle\n",
    "            torch.linspace(1.001, 1.002, steps=dg)           # Add repeated values at the end for clamping\n",
    "            ]).view(-1,1)\n",
    "     \n",
    "        return knots\n",
    "    \n",
    "    def basis_function(self, x, spl):\n",
    "        basis_output = spl.fit_transform(x)\n",
    "        return basis_output\n",
    "            \n",
    "    def forward(self, x):\n",
    "        batch_size, num_features = x.size()\n",
    "        device = x.device\n",
    "        \n",
    "        # Create knot vector and apply B-spline basis functions for each feature\n",
    "        basises = []\n",
    "        knots = self.knots_distribution(self.degree, self.num_knots)\n",
    "        knots = knots.to(device)\n",
    "        spl = SplineTransformer(n_knots=self.num_knots, degree=self.degree, knots = knots)\n",
    "\n",
    "        \n",
    "        for feature in range(num_features):\n",
    "            # Calculate B-spline basis functions for this feature\n",
    "            \n",
    "            basis = self.basis_function(x[:, feature].reshape(-1,1), spl)\n",
    "            basis = torch.Tensor(basis)\n",
    "            basises.append(basis)\n",
    "\n",
    "        if num_features == 1:\n",
    "            tout = basises[0] @ self.control_p\n",
    "            self.inter['basic'] = basises[0].T\n",
    "        else:\n",
    "            self.inter['basic'] = torch.reshape(torch.stack(basises, dim = 1), (batch_size, self.num_knots * self.num_neurons)).T\n",
    "            basises = torch.stack(basises)\n",
    "            tout = basises.permute(1,2,0) * self.control_p\n",
    "            tout = tout.sum(dim =1)\n",
    "                \n",
    "        if self.bias is not None:\n",
    "            tout += self.bias        \n",
    "            \n",
    "        return tout\n",
    "\n",
    "\n",
    "class NormLayer(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(NormLayer, self).__init__()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tmin_val = torch.min(x, axis = 1).values.reshape(-1,1)\n",
    "\t\tmax_val = torch.max(x, axis = 1).values.reshape(-1,1)\n",
    "\n",
    "\t\tx = (x - min_val)/(max_val - min_val)  # Rescale to [0, 1]\n",
    "\t\treturn x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5ba6d1c7-50bb-4c92-9a2f-5ab66703d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSpline_block(nn.Module):\n",
    "    def __init__(self, degree, num_knots, num_neurons, dropout = 0.0, bias = True):\n",
    "        super(BSpline_block, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(OrderedDict([\n",
    "            ('norm', NormLayer()),\n",
    "            ('BSL', BSL(degree = degree, num_knots = num_knots, num_neurons = num_neurons, bias = bias)),\n",
    "            ('drop', nn.Dropout(dropout)),\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "        \n",
    "class StackBS_block(nn.Module):\n",
    "    def __init__(self, block, degree, num_knots, num_neurons, num_blocks, dropout = 0.0, bias = True):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleDict({\n",
    "            f'block_{i}': block(degree = degree, num_knots = num_knots, num_neurons = num_neurons)\n",
    "            for i in range(num_blocks)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        for name, block in self.model.items():\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "d56f6179-804b-461f-bf1e-8423ab517582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DPS(nn.Module):\n",
    "    def __init__(self, input_dim, degree, num_knots, num_neurons, num_bsl, output_dim, bias):\n",
    "        super(DPS, self).__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_knots = num_knots\n",
    "        self.ln1 = nn.Linear(input_dim, num_neurons)\n",
    "        #self.nm1 = NormLayer() \n",
    "        #self.sp1 = BSL(degree = degree, num_knots = num_knots, num_neurons = num_neurons, bias = True)\n",
    "        self.Spline_block = StackBS_block(BSpline_block, degree = degree, num_knots = num_knots, num_neurons = num_neurons, num_blocks = num_bsl)\n",
    "        self.ln2 = nn.Linear(num_neurons, output_dim)\n",
    "        #self.inter = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.ln1(x)\n",
    "\n",
    "        # # # # # # # # # # # # # #\n",
    "        #          SPLINE         #\n",
    "        # # # # # # # # # # # # # #\n",
    "        \n",
    "        spout = self.Spline_block(x)\n",
    "\n",
    "        '''  \n",
    "        ln1out = self.nm1(ln1out)\n",
    "        device = ln1out.device\n",
    "        batch_size, _ = x.size()\n",
    "        \n",
    "        # # # # # # # # # # # # # #\n",
    "        #          SPLINE         #\n",
    "        # # # # # # # # # # # # # #\n",
    "        \n",
    "        sp1out = self.sp1(ln1out)\n",
    "        bslist = self.sp1.inter['basic']\n",
    "        \n",
    "        self.inter['ebasic'] = bslist\n",
    "        self.inter['basic'] = sp1out\n",
    "        '''\n",
    "        \n",
    "        output = self.ln2(spout)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def get_para_ecm(self, x):\n",
    "\n",
    "        '''\n",
    "        ecm_para: A dictionary that collects the parameter we need to the following ECM algorithm.\n",
    "        ecm_para.basic: Store the output of each B-Spline block; Dimension = [n_sample, n_neurons]\n",
    "        ecm_para.ebasic Store the weight matrix of each B-Spline expansion; Dimension = [n_knots * n_neurons, n_sample]\n",
    "\n",
    "        '''\n",
    "        ecm_para = {}\n",
    "        bs_block_out = {}\n",
    "        bs_spline_weight = {}\n",
    "        bs_spline_value = {}\n",
    "        \n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                bs_block_out[name] = output.detach()\n",
    "            return hook\n",
    "\n",
    "        handles = []\n",
    "        for name, layer in self.named_modules():\n",
    "            if 'block.drop' in name:\n",
    "                handles.append(layer.register_forward_hook(get_activation(name)))\n",
    "            elif 'block.BSL' in name:\n",
    "                bs_spline_value[name] = layer.inter['basic'].detach()\n",
    "                bs_spline_weight[name] = layer.control_p.detach()\n",
    "        # Run forward pass (triggers hooks)\n",
    "        _ = self(x)\n",
    "\n",
    "        # Clean up hooks\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "            \n",
    "        ecm_para['basic'] = torch.stack(list(bs_block_out.values()), dim=0)\n",
    "        ecm_para['ebasic'] = torch.stack(list(bs_spline_value.values()), dim=0)\n",
    "        ecm_para['wbasic'] = torch.stack(list(bs_spline_weight.values()), dim=0)\n",
    "\n",
    "        del bs_block_out, bs_spline_weight, bs_spline_value\n",
    "        \n",
    "        return ecm_para\n",
    "\n",
    "    def fit(self, x):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c23cb-4c93-4fbb-876f-91530c18b96f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "64e443ff-3729-4a1c-85ad-c500872d5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 1000; ntest = 2500; ndim = 10; ndf = 1; nk = 15; nm = 50; Fout = 1\n",
    "Dtype = 'A'\n",
    "data = {}\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for d in range(ndf):\n",
    "    X_train, y_train = sim_data(ntrain, ndim, Dtype)\n",
    "    X_test, y_test = sim_data(ntest, ndim, Dtype)\n",
    "    epstrain = torch.normal(0, torch.var(y_train)*0.05, size=y_train.size())\n",
    "    epstest = torch.normal(0,  torch.var(y_train)*0.05, size=y_test.size())\n",
    "    \n",
    "    y_train, y_test = y_train + epstrain, y_test + epstest\n",
    "    data[str(d+1)] = {'TrainX': X_train, 'Trainy': y_train, 'TestX': X_test, 'Testy': y_test}\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328c265-731b-4044-b65b-b6b39792fdae",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "7098de70-4ba8-419a-9dbd-3165171a16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepBS = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = 2, output_dim = Fout, bias = True).to(device)\n",
    "learning_r = 1e-3\n",
    "optimizer = torch.optim.Adam(DeepBS.parameters(), lr=learning_r)\n",
    "Iteration = 10000; bloss_list = []; tor = 1e-5; lr_tor = 1e-6\n",
    "patientc = 30; patientr = 10; tpat = 0; bloss = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "adf2f65d-0b81-4d59-8ec2-a9e6f8bfddca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss:  35.35215759277344  | , previous best loss:  9999  | saving best model ...\n",
      "Current loss:  34.85819625854492  | , previous best loss:  35.35215759277344  | saving best model ...\n",
      "Current loss:  34.368900299072266  | , previous best loss:  34.85819625854492  | saving best model ...\n",
      "Current loss:  33.884334564208984  | , previous best loss:  34.368900299072266  | saving best model ...\n",
      "Current loss:  33.40459060668945  | , previous best loss:  33.884334564208984  | saving best model ...\n",
      "Current loss:  32.92973327636719  | , previous best loss:  33.40459060668945  | saving best model ...\n",
      "Current loss:  32.45982360839844  | , previous best loss:  32.92973327636719  | saving best model ...\n",
      "Current loss:  31.994932174682617  | , previous best loss:  32.45982360839844  | saving best model ...\n",
      "Current loss:  31.535112380981445  | , previous best loss:  31.994932174682617  | saving best model ...\n",
      "Current loss:  31.080415725708008  | , previous best loss:  31.535112380981445  | saving best model ...\n",
      "Current loss:  30.63089370727539  | , previous best loss:  31.080415725708008  | saving best model ...\n",
      "Current loss:  30.186586380004883  | , previous best loss:  30.63089370727539  | saving best model ...\n",
      "Current loss:  29.747528076171875  | , previous best loss:  30.186586380004883  | saving best model ...\n",
      "Current loss:  29.31374740600586  | , previous best loss:  29.747528076171875  | saving best model ...\n",
      "Current loss:  28.88528060913086  | , previous best loss:  29.31374740600586  | saving best model ...\n",
      "Current loss:  28.462139129638672  | , previous best loss:  28.88528060913086  | saving best model ...\n",
      "Current loss:  28.044343948364258  | , previous best loss:  28.462139129638672  | saving best model ...\n",
      "Current loss:  27.63190269470215  | , previous best loss:  28.044343948364258  | saving best model ...\n",
      "Current loss:  27.22483253479004  | , previous best loss:  27.63190269470215  | saving best model ...\n",
      "Current loss:  26.823135375976562  | , previous best loss:  27.22483253479004  | saving best model ...\n",
      "Current loss:  26.426816940307617  | , previous best loss:  26.823135375976562  | saving best model ...\n",
      "Current loss:  26.035873413085938  | , previous best loss:  26.426816940307617  | saving best model ...\n",
      "Current loss:  25.650306701660156  | , previous best loss:  26.035873413085938  | saving best model ...\n",
      "Current loss:  25.270112991333008  | , previous best loss:  25.650306701660156  | saving best model ...\n",
      "Current loss:  24.89528465270996  | , previous best loss:  25.270112991333008  | saving best model ...\n",
      "Current loss:  24.52581214904785  | , previous best loss:  24.89528465270996  | saving best model ...\n",
      "Current loss:  24.161685943603516  | , previous best loss:  24.52581214904785  | saving best model ...\n",
      "Current loss:  23.80289649963379  | , previous best loss:  24.161685943603516  | saving best model ...\n",
      "Current loss:  23.449426651000977  | , previous best loss:  23.80289649963379  | saving best model ...\n",
      "Current loss:  23.101253509521484  | , previous best loss:  23.449426651000977  | saving best model ...\n",
      "Current loss:  22.758371353149414  | , previous best loss:  23.101253509521484  | saving best model ...\n",
      "Current loss:  22.420753479003906  | , previous best loss:  22.758371353149414  | saving best model ...\n",
      "Current loss:  22.08837890625  | , previous best loss:  22.420753479003906  | saving best model ...\n",
      "Current loss:  21.761226654052734  | , previous best loss:  22.08837890625  | saving best model ...\n",
      "Current loss:  21.43927001953125  | , previous best loss:  21.761226654052734  | saving best model ...\n",
      "Current loss:  21.12248420715332  | , previous best loss:  21.43927001953125  | saving best model ...\n",
      "Current loss:  20.81084632873535  | , previous best loss:  21.12248420715332  | saving best model ...\n",
      "Current loss:  20.504323959350586  | , previous best loss:  20.81084632873535  | saving best model ...\n",
      "Current loss:  20.202890396118164  | , previous best loss:  20.504323959350586  | saving best model ...\n",
      "Current loss:  19.906518936157227  | , previous best loss:  20.202890396118164  | saving best model ...\n",
      "Current loss:  19.615171432495117  | , previous best loss:  19.906518936157227  | saving best model ...\n",
      "Current loss:  19.32882308959961  | , previous best loss:  19.615171432495117  | saving best model ...\n",
      "Current loss:  19.04743766784668  | , previous best loss:  19.32882308959961  | saving best model ...\n",
      "Current loss:  18.77098846435547  | , previous best loss:  19.04743766784668  | saving best model ...\n",
      "Current loss:  18.499431610107422  | , previous best loss:  18.77098846435547  | saving best model ...\n",
      "Current loss:  18.232742309570312  | , previous best loss:  18.499431610107422  | saving best model ...\n",
      "Current loss:  17.970884323120117  | , previous best loss:  18.232742309570312  | saving best model ...\n",
      "Current loss:  17.71381950378418  | , previous best loss:  17.970884323120117  | saving best model ...\n",
      "Current loss:  17.461519241333008  | , previous best loss:  17.71381950378418  | saving best model ...\n",
      "Current loss:  17.21394157409668  | , previous best loss:  17.461519241333008  | saving best model ...\n",
      "Current loss:  16.97104835510254  | , previous best loss:  17.21394157409668  | saving best model ...\n",
      "Current loss:  16.73281478881836  | , previous best loss:  16.97104835510254  | saving best model ...\n",
      "Current loss:  16.49919319152832  | , previous best loss:  16.73281478881836  | saving best model ...\n",
      "Current loss:  16.270151138305664  | , previous best loss:  16.49919319152832  | saving best model ...\n",
      "Current loss:  16.0456485748291  | , previous best loss:  16.270151138305664  | saving best model ...\n",
      "Current loss:  15.825652122497559  | , previous best loss:  16.0456485748291  | saving best model ...\n",
      "Current loss:  15.610118865966797  | , previous best loss:  15.825652122497559  | saving best model ...\n",
      "Current loss:  15.39901351928711  | , previous best loss:  15.610118865966797  | saving best model ...\n",
      "Current loss:  15.192296981811523  | , previous best loss:  15.39901351928711  | saving best model ...\n",
      "Current loss:  14.989927291870117  | , previous best loss:  15.192296981811523  | saving best model ...\n",
      "Current loss:  14.791868209838867  | , previous best loss:  14.989927291870117  | saving best model ...\n",
      "Current loss:  14.598076820373535  | , previous best loss:  14.791868209838867  | saving best model ...\n",
      "Current loss:  14.408515930175781  | , previous best loss:  14.598076820373535  | saving best model ...\n",
      "Current loss:  14.223140716552734  | , previous best loss:  14.408515930175781  | saving best model ...\n",
      "Current loss:  14.041913032531738  | , previous best loss:  14.223140716552734  | saving best model ...\n",
      "Current loss:  13.864789009094238  | , previous best loss:  14.041913032531738  | saving best model ...\n",
      "Current loss:  13.691725730895996  | , previous best loss:  13.864789009094238  | saving best model ...\n",
      "Current loss:  13.52268123626709  | , previous best loss:  13.691725730895996  | saving best model ...\n",
      "Current loss:  13.357616424560547  | , previous best loss:  13.52268123626709  | saving best model ...\n",
      "Current loss:  13.196480751037598  | , previous best loss:  13.357616424560547  | saving best model ...\n",
      "Current loss:  13.039231300354004  | , previous best loss:  13.196480751037598  | saving best model ...\n",
      "Current loss:  12.885828018188477  | , previous best loss:  13.039231300354004  | saving best model ...\n",
      "Current loss:  12.73621940612793  | , previous best loss:  12.885828018188477  | saving best model ...\n",
      "Current loss:  12.590363502502441  | , previous best loss:  12.73621940612793  | saving best model ...\n",
      "Current loss:  12.448209762573242  | , previous best loss:  12.590363502502441  | saving best model ...\n",
      "Current loss:  12.30971622467041  | , previous best loss:  12.448209762573242  | saving best model ...\n",
      "Current loss:  12.174830436706543  | , previous best loss:  12.30971622467041  | saving best model ...\n",
      "Current loss:  12.04350757598877  | , previous best loss:  12.174830436706543  | saving best model ...\n",
      "Current loss:  11.915698051452637  | , previous best loss:  12.04350757598877  | saving best model ...\n",
      "Current loss:  11.791351318359375  | , previous best loss:  11.915698051452637  | saving best model ...\n",
      "Current loss:  11.670417785644531  | , previous best loss:  11.791351318359375  | saving best model ...\n",
      "Current loss:  11.552849769592285  | , previous best loss:  11.670417785644531  | saving best model ...\n",
      "Current loss:  11.438591957092285  | , previous best loss:  11.552849769592285  | saving best model ...\n",
      "Current loss:  11.327597618103027  | , previous best loss:  11.438591957092285  | saving best model ...\n",
      "Current loss:  11.21981143951416  | , previous best loss:  11.327597618103027  | saving best model ...\n",
      "Current loss:  11.11518383026123  | , previous best loss:  11.21981143951416  | saving best model ...\n",
      "Current loss:  11.013659477233887  | , previous best loss:  11.11518383026123  | saving best model ...\n",
      "Current loss:  10.915186882019043  | , previous best loss:  11.013659477233887  | saving best model ...\n",
      "Current loss:  10.819710731506348  | , previous best loss:  10.915186882019043  | saving best model ...\n",
      "Current loss:  10.727178573608398  | , previous best loss:  10.819710731506348  | saving best model ...\n",
      "Current loss:  10.637537002563477  | , previous best loss:  10.727178573608398  | saving best model ...\n",
      "Current loss:  10.550729751586914  | , previous best loss:  10.637537002563477  | saving best model ...\n",
      "Current loss:  10.466703414916992  | , previous best loss:  10.550729751586914  | saving best model ...\n",
      "Current loss:  10.38539981842041  | , previous best loss:  10.466703414916992  | saving best model ...\n",
      "Current loss:  10.30676555633545  | , previous best loss:  10.38539981842041  | saving best model ...\n",
      "Current loss:  10.23074722290039  | , previous best loss:  10.30676555633545  | saving best model ...\n",
      "Current loss:  10.1572847366333  | , previous best loss:  10.23074722290039  | saving best model ...\n",
      "Current loss:  10.086325645446777  | , previous best loss:  10.1572847366333  | saving best model ...\n",
      "Current loss:  10.017812728881836  | , previous best loss:  10.086325645446777  | saving best model ...\n",
      "Current loss:  9.951689720153809  | , previous best loss:  10.017812728881836  | saving best model ...\n",
      "Current loss:  9.887899398803711  | , previous best loss:  9.951689720153809  | saving best model ...\n",
      "Current loss:  9.826388359069824  | , previous best loss:  9.887899398803711  | saving best model ...\n",
      "Current loss:  9.767099380493164  | , previous best loss:  9.826388359069824  | saving best model ...\n",
      "Current loss:  9.709977149963379  | , previous best loss:  9.767099380493164  | saving best model ...\n",
      "Current loss:  9.6549654006958  | , previous best loss:  9.709977149963379  | saving best model ...\n",
      "Current loss:  9.602009773254395  | , previous best loss:  9.6549654006958  | saving best model ...\n",
      "Current loss:  9.551054000854492  | , previous best loss:  9.602009773254395  | saving best model ...\n",
      "Current loss:  9.502043724060059  | , previous best loss:  9.551054000854492  | saving best model ...\n",
      "Current loss:  9.454923629760742  | , previous best loss:  9.502043724060059  | saving best model ...\n",
      "Current loss:  9.409639358520508  | , previous best loss:  9.454923629760742  | saving best model ...\n",
      "Current loss:  9.366140365600586  | , previous best loss:  9.409639358520508  | saving best model ...\n",
      "Current loss:  9.324371337890625  | , previous best loss:  9.366140365600586  | saving best model ...\n",
      "Current loss:  9.28427791595459  | , previous best loss:  9.324371337890625  | saving best model ...\n",
      "Current loss:  9.245809555053711  | , previous best loss:  9.28427791595459  | saving best model ...\n",
      "Current loss:  9.208913803100586  | , previous best loss:  9.245809555053711  | saving best model ...\n",
      "Current loss:  9.173542022705078  | , previous best loss:  9.208913803100586  | saving best model ...\n",
      "Current loss:  9.139639854431152  | , previous best loss:  9.173542022705078  | saving best model ...\n",
      "Current loss:  9.107160568237305  | , previous best loss:  9.139639854431152  | saving best model ...\n",
      "Current loss:  9.076053619384766  | , previous best loss:  9.107160568237305  | saving best model ...\n",
      "Current loss:  9.046272277832031  | , previous best loss:  9.076053619384766  | saving best model ...\n",
      "Current loss:  9.017768859863281  | , previous best loss:  9.046272277832031  | saving best model ...\n",
      "Current loss:  8.990493774414062  | , previous best loss:  9.017768859863281  | saving best model ...\n",
      "Current loss:  8.964406967163086  | , previous best loss:  8.990493774414062  | saving best model ...\n",
      "Current loss:  8.939458847045898  | , previous best loss:  8.964406967163086  | saving best model ...\n",
      "Current loss:  8.915608406066895  | , previous best loss:  8.939458847045898  | saving best model ...\n",
      "Current loss:  8.892809867858887  | , previous best loss:  8.915608406066895  | saving best model ...\n",
      "Current loss:  8.87102222442627  | , previous best loss:  8.892809867858887  | saving best model ...\n",
      "Current loss:  8.850205421447754  | , previous best loss:  8.87102222442627  | saving best model ...\n",
      "Current loss:  8.830318450927734  | , previous best loss:  8.850205421447754  | saving best model ...\n",
      "Current loss:  8.811321258544922  | , previous best loss:  8.830318450927734  | saving best model ...\n",
      "Current loss:  8.793177604675293  | , previous best loss:  8.811321258544922  | saving best model ...\n",
      "Current loss:  8.775847434997559  | , previous best loss:  8.793177604675293  | saving best model ...\n",
      "Current loss:  8.759298324584961  | , previous best loss:  8.775847434997559  | saving best model ...\n",
      "Current loss:  8.743492126464844  | , previous best loss:  8.759298324584961  | saving best model ...\n",
      "Current loss:  8.728394508361816  | , previous best loss:  8.743492126464844  | saving best model ...\n",
      "Current loss:  8.713974952697754  | , previous best loss:  8.728394508361816  | saving best model ...\n",
      "Current loss:  8.700200080871582  | , previous best loss:  8.713974952697754  | saving best model ...\n",
      "Current loss:  8.687039375305176  | , previous best loss:  8.700200080871582  | saving best model ...\n",
      "Current loss:  8.674461364746094  | , previous best loss:  8.687039375305176  | saving best model ...\n",
      "Current loss:  8.662437438964844  | , previous best loss:  8.674461364746094  | saving best model ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[368], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Iteration):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Forward pass: Compute predicted y by passing x to the modelsp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     pyb_af \u001b[38;5;241m=\u001b[39m DeepBS(X_train)\n\u001b[1;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_train, pyb_af); bloss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m ((bloss_list[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39mbloss_list[t])\u001b[38;5;241m<\u001b[39mtor):        \n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[349], line 21\u001b[0m, in \u001b[0;36mDPS.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# # # # # # # # # # # # # #\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#          SPLINE         #\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# # # # # # # # # # # # # #\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m spout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSpline_block(x)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''  \u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03mln1out = self.nm1(ln1out)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mdevice = ln1out.device\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03mself.inter['basic'] = sp1out\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     39\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(spout)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[138], line 24\u001b[0m, in \u001b[0;36mStackBS_block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 24\u001b[0m         x \u001b[38;5;241m=\u001b[39m block(x)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[138], line 12\u001b[0m, in \u001b[0;36mBSpline_block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[132], line 63\u001b[0m, in \u001b[0;36mBSL.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m spl \u001b[38;5;241m=\u001b[39m SplineTransformer(n_knots\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_knots, degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree, knots \u001b[38;5;241m=\u001b[39m knots)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_features):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Calculate B-spline basis functions for this feature\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     basis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis_function(x[:, feature]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), spl)\n\u001b[1;32m     64\u001b[0m     basis \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(basis)\n\u001b[1;32m     65\u001b[0m     basises\u001b[38;5;241m.\u001b[39mappend(basis)\n",
      "Cell \u001b[0;32mIn[132], line 46\u001b[0m, in \u001b[0;36mBSL.basis_function\u001b[0;34m(self, x, spl)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbasis_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, spl):\n\u001b[0;32m---> 46\u001b[0m     basis_output \u001b[38;5;241m=\u001b[39m spl\u001b[38;5;241m.\u001b[39mfit_transform(x)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m basis_output\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/base.py:1382\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1377\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1378\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1379\u001b[0m )\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[1;32m   1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/base.py:436\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    429\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m     validate_parameter_constraints(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_constraints,\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    439\u001b[0m         caller_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    440\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:75\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m constraints \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m constraints \u001b[38;5;241m=\u001b[39m [make_constraint(constraint) \u001b[38;5;28;01mfor\u001b[39;00m constraint \u001b[38;5;129;01min\u001b[39;00m constraints]\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m constraint \u001b[38;5;129;01min\u001b[39;00m constraints:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m constraint\u001b[38;5;241m.\u001b[39mis_satisfied_by(param_val):\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;66;03m# this constraint is satisfied, no need to check further.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:129\u001b[0m, in \u001b[0;36mmake_constraint\u001b[0;34m(constraint)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(constraint, \u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _InstancesOf(constraint)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    130\u001b[0m     constraint, (Interval, StrOptions, Options, HasMethods, MissingValues)\n\u001b[1;32m    131\u001b[0m ):\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m constraint\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(constraint, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m constraint \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboolean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/abc.py:117\u001b[0m, in \u001b[0;36mABCMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Register a virtual subclass of an ABC.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    Returns the subclass, to allow usage as a class decorator.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _abc_register(\u001b[38;5;28mcls\u001b[39m, subclass)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__instancecheck__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, instance):\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _abc_instancecheck(\u001b[38;5;28mcls\u001b[39m, instance)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(Iteration):\n",
    "    # Forward pass: Compute predicted y by passing x to the modelsp\n",
    "    pyb_af = DeepBS(X_train)\n",
    "    loss = criterion(y_train, pyb_af); bloss_list.append(loss.item())\n",
    "    \n",
    "    if (t > 0) and ((bloss_list[t-1]-bloss_list[t])<tor):        \n",
    "        if (tpat != 0) and (tpat % patientr) == 0:\n",
    "            learning_r *= 0.2 \n",
    "            tpat += 1\n",
    "            #print('Learning rate reduce to ', learning_r)\n",
    "            optimizer = torch.optim.Adam(DeepBS.parameters(), lr=learning_r)\n",
    "            if learning_r <= lr_tor:\n",
    "                if t < patientc + 1:\n",
    "                    conv = False\n",
    "                else:\n",
    "                    conv = True\n",
    "                print('Convergence!')\n",
    "                break\n",
    "        elif tpat < patientc:\n",
    "            tpat += 1\n",
    "            pass\n",
    "        else:\n",
    "            if t < patientc + 1:\n",
    "                conv = False\n",
    "            else:\n",
    "                conv = True\n",
    "            print('Convergence!')\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        if loss < bloss:\n",
    "            print('Current loss: ', loss.item(), ' | , previous best loss: ', bloss, ' | saving best model ...')\n",
    "            torch.save(DeepBS.state_dict(), './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1))\n",
    "            bloss = loss.item()\n",
    "            tpat = 0\n",
    "        else:\n",
    "            tpat += 1\n",
    "    \n",
    "    if tpat == patientc:\n",
    "        if t < patientc + 1:\n",
    "            conv = False\n",
    "        else:\n",
    "            conv = True\n",
    "            print('Convergence!')\n",
    "        break\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c8763-5ba2-483c-bdf1-dafef5db12a0",
   "metadata": {},
   "source": [
    "## ECM-Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bd5ec9-cd18-45fb-aa4a-853660da01be",
   "metadata": {},
   "source": [
    "### Via ECM to implement layer-wise optimization for tuning the weight for B-Spline Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5e637620-3f84-4074-9b53-553a120e9e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECM_para = DeepBS.get_para_ecm(X_train)\n",
    "ECM_Lambda = ECM(ECM_para, initial_xi = 1, initial_sigma = 1, initial_lambda = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "b170f31a-3741-43d6-9f8d-92e8cac88906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2288, 0.2560])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "01894a77-0720-416e-9745-9c47e283d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepPS = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = 2, output_dim = Fout, bias = True).to(device)\n",
    "DeepPS.load_state_dict(torch.load( './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1), weights_only = True))\n",
    "DPSy = DeepPS(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d780a8-ff73-4afe-9ec0-2720f81bc8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7500cd15-e680-4062-9469-0f142ed368a1",
   "metadata": {},
   "source": [
    "### Fast-Tuning for optimimal DPS parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb3d17b-eb10-47ac-983d-2f655fad7d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cbf8f85-c7d8-499b-9452-dd8f7aafe322",
   "metadata": {},
   "source": [
    "## Evaluation on DS and DPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef2fa0-ed52-4f74-aa33-c8eee06dd6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    eval_model = DPS(input_dim = ndim, degree = 3, num_knots = nk, num_neurons = nm, num_bsl = 1, output_dim = Fout, bias = True).to(device)\n",
    "    eval_model.load_state_dict(torch.load( './EXA'+str(X_train.size()[0])+'h'+str(nm)+'k'+str(nk)+'data'+str(d+1), weights_only = True))\n",
    "    DPSy = eval_model(X_train)\n",
    "    LambdaB = ECM(model = eval_model, num_neurons = nm, num_knots = nk)\n",
    "    Lambdalist[str(d+1)] = LambdaB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTORCH",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
